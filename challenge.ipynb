{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae4d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ¦ ì‹ ìš©ì¹´ë“œ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ - ë°ì´í„° êµ¬ì¡° ë¶„ì„\n",
      "==================================================\n",
      "\n",
      "ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\n",
      "âœ… train í´ë” ì¡´ì¬\n",
      "   - í•˜ìœ„ í´ë” ìˆ˜: 8\n",
      "   - 1.íšŒì›ì •ë³´\n",
      "   - 2.ì‹ ìš©ì •ë³´\n",
      "   - 3.ìŠ¹ì¸ë§¤ì¶œì •ë³´\n",
      "   - 4.ì²­êµ¬ì…ê¸ˆì •ë³´\n",
      "   - 5.ì”ì•¡ì •ë³´\n",
      "   - 6.ì±„ë„ì •ë³´\n",
      "   - 7.ë§ˆì¼€íŒ…ì •ë³´\n",
      "   - 8.ì„±ê³¼ì •ë³´\n",
      "âœ… test í´ë” ì¡´ì¬\n",
      "   - í•˜ìœ„ í´ë” ìˆ˜: 8\n",
      "\n",
      "ğŸ“‹ ë°ì´í„° íŒŒì¼ í˜„í™©:\n",
      "\n",
      "ğŸ”¹ 1.íšŒì›ì •ë³´: ê³ ê°ì˜ ê¸°ë³¸ ì •ë³´ (ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„±)\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 78)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ë‚¨ë…€êµ¬ë¶„ì½”ë“œ', 'ì—°ë ¹', 'Segment']...\n",
      "\n",
      "ğŸ”¹ 2.ì‹ ìš©ì •ë³´: ì‹ ìš©ë„ ê´€ë ¨ ì •ë³´\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 42)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ìµœì´ˆí•œë„ê¸ˆì•¡', 'ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡', 'CAí•œë„ê¸ˆì•¡']...\n",
      "\n",
      "ğŸ”¹ 3.ìŠ¹ì¸ë§¤ì¶œì •ë³´: ì¹´ë“œ ì‚¬ìš© íŒ¨í„´ ì •ë³´\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 406)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ìµœì¢…ì´ìš©ì¼ì_ê¸°ë³¸', 'ìµœì¢…ì´ìš©ì¼ì_ì‹ íŒ', 'ìµœì¢…ì´ìš©ì¼ì_CA']...\n",
      "\n",
      "ğŸ”¹ 4.ì²­êµ¬ì…ê¸ˆì •ë³´: ì²­êµ¬ ë° ì…ê¸ˆ ì´ë ¥\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 46)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ëŒ€í‘œê²°ì œì¼', 'ëŒ€í‘œê²°ì œë°©ë²•ì½”ë“œ', 'ëŒ€í‘œì²­êµ¬ì§€ê³ ê°ì£¼ì†Œêµ¬ë¶„ì½”ë“œ']...\n",
      "\n",
      "ğŸ”¹ 5.ì”ì•¡ì •ë³´: ê³„ì¢Œ ì”ì•¡ ê´€ë ¨ ì •ë³´\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 82)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ì”ì•¡_ì¼ì‹œë¶ˆ_B0M', 'ì”ì•¡_í• ë¶€_B0M', 'ì”ì•¡_í˜„ê¸ˆì„œë¹„ìŠ¤_B0M']...\n",
      "\n",
      "ğŸ”¹ 6.ì±„ë„ì •ë³´: ì‚¬ìš© ì±„ë„ ì •ë³´\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 105)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ì¸ì…íšŸìˆ˜_ARS_R6M', 'ì´ìš©ë©”ë‰´ê±´ìˆ˜_ARS_R6M', 'ì¸ì…ì¼ìˆ˜_ARS_R6M']...\n",
      "\n",
      "ğŸ”¹ 7.ë§ˆì¼€íŒ…ì •ë³´: ë§ˆì¼€íŒ… í™œë™ ë° ë°˜ì‘\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 64)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ì»¨íƒê±´ìˆ˜_ì¹´ë“œë¡ _TM_B0M', 'ì»¨íƒê±´ìˆ˜_ë¦¬ë³¼ë¹™_TM_B0M', 'ì»¨íƒê±´ìˆ˜_CA_TM_B0M']...\n",
      "\n",
      "ğŸ”¹ 8.ì„±ê³¼ì •ë³´: ì„±ê³¼ ì§€í‘œ\n",
      "   Train íŒŒì¼ ìˆ˜: 6\n",
      "   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: (400000, 49)\n",
      "   ì£¼ìš” ì»¬ëŸ¼: ['ê¸°ì¤€ë…„ì›”', 'ID', 'ì¦ê°ìœ¨_ì´ìš©ê±´ìˆ˜_ì‹ ìš©_ì „ì›”', 'ì¦ê°ìœ¨_ì´ìš©ê±´ìˆ˜_ì‹ íŒ_ì „ì›”', 'ì¦ê°ìœ¨_ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_ì „ì›”']...\n",
      "\n",
      "==================================================\n",
      "ğŸ” íšŒì›ì •ë³´ ë°ì´í„° ìƒì„¸ ë¶„ì„\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š ê¸°ë³¸ ì •ë³´:\n",
      "   - ë°ì´í„° í¬ê¸°: (400000, 78)\n",
      "   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 550.17 MB\n",
      "\n",
      "ğŸ“‹ ì»¬ëŸ¼ ì •ë³´:\n",
      "   - ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 78\n",
      "   - ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìˆ˜: 64\n",
      "   - ë²”ì£¼í˜• ì»¬ëŸ¼ ìˆ˜: 14\n",
      "\n",
      "ğŸ”‘ ì£¼ìš” ì»¬ëŸ¼ (ì²˜ìŒ 10ê°œ):\n",
      "    1. ê¸°ì¤€ë…„ì›”                 | int64      | ê²°ì¸¡ë¥ :   0.0%\n",
      "    2. ID                   | object     | ê²°ì¸¡ë¥ :   0.0%\n",
      "    3. ë‚¨ë…€êµ¬ë¶„ì½”ë“œ               | int64      | ê²°ì¸¡ë¥ :   0.0%\n",
      "    4. ì—°ë ¹                   | object     | ê²°ì¸¡ë¥ :   0.0%\n",
      "    5. Segment              | object     | ê²°ì¸¡ë¥ :   0.0%\n",
      "    6. íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥            | int64      | ê²°ì¸¡ë¥ :   0.0%\n",
      "    7. íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA         | int64      | ê²°ì¸¡ë¥ :   0.0%\n",
      "    8. íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡         | int64      | ê²°ì¸¡ë¥ :   0.0%\n",
      "    9. ì†Œì§€ì—¬ë¶€_ì‹ ìš©              | int64      | ê²°ì¸¡ë¥ :   0.0%\n",
      "   10. ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©          | int64      | ê²°ì¸¡ë¥ :   0.0%\n",
      "\n",
      "ğŸ¯ Target ë³€ìˆ˜ ë¶„í¬:\n",
      "   Segment A: 162ê°œ (0.0%)\n",
      "   Segment B: 24ê°œ (0.0%)\n",
      "   Segment C: 21,265ê°œ (5.3%)\n",
      "   Segment D: 58,207ê°œ (14.6%)\n",
      "   Segment E: 320,342ê°œ (80.1%)\n",
      "\n",
      "ğŸ†” ID ì •ë³´:\n",
      "   - ì´ í–‰ ìˆ˜: 400,000\n",
      "   - ê³ ìœ  ID ìˆ˜: 400,000\n",
      "   - ID ì¤‘ë³µ ì—¬ë¶€: âœ… ì¤‘ë³µ ì—†ìŒ\n",
      "\n",
      "==================================================\n",
      "âœ… 1ë‹¨ê³„ ë°ì´í„° êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\n",
      "==================================================\n",
      "\n",
      "ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ”:\n",
      "1. ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ EDA\n",
      "2. ì‹œê³„ì—´ ë°ì´í„° íŠ¹ì„± ë¶„ì„\n",
      "3. Target ë³€ìˆ˜ì™€ ê° ì¹´í…Œê³ ë¦¬ì˜ ê´€ê³„ ë¶„ì„\n",
      "\n",
      "ì–´ë–¤ ë¶€ë¶„ì„ ë” ìì„¸íˆ ë¶„ì„í•˜ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ ì‹œê°í™”ë¥¼ ìœ„í•œ ì„¤ì •\n",
    "import koreanize_matplotlib\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ğŸ¦ ì‹ ìš©ì¹´ë“œ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ - ë°ì´í„° êµ¬ì¡° ë¶„ì„\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸\n",
    "def check_directory_structure():\n",
    "    print(\"\\nğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\")\n",
    "    \n",
    "    # train í´ë” í™•ì¸\n",
    "    if os.path.exists('train'):\n",
    "        print(\"âœ… train í´ë” ì¡´ì¬\")\n",
    "        train_folders = [f for f in os.listdir('train') if os.path.isdir(os.path.join('train', f))]\n",
    "        print(f\"   - í•˜ìœ„ í´ë” ìˆ˜: {len(train_folders)}\")\n",
    "        for folder in sorted(train_folders):\n",
    "            print(f\"   - {folder}\")\n",
    "    \n",
    "    # test í´ë” í™•ì¸  \n",
    "    if os.path.exists('test'):\n",
    "        print(\"âœ… test í´ë” ì¡´ì¬\")\n",
    "        test_folders = [f for f in os.listdir('test') if os.path.isdir(os.path.join('test', f))]\n",
    "        print(f\"   - í•˜ìœ„ í´ë” ìˆ˜: {len(test_folders)}\")\n",
    "\n",
    "check_directory_structure()\n",
    "\n",
    "# 2. ê° ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° íŒŒì¼ í™•ì¸\n",
    "def check_data_files():\n",
    "    print(\"\\nğŸ“‹ ë°ì´í„° íŒŒì¼ í˜„í™©:\")\n",
    "    \n",
    "    categories = {\n",
    "        \"1.íšŒì›ì •ë³´\": \"ê³ ê°ì˜ ê¸°ë³¸ ì •ë³´ (ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„±)\",\n",
    "        \"2.ì‹ ìš©ì •ë³´\": \"ì‹ ìš©ë„ ê´€ë ¨ ì •ë³´\",\n",
    "        \"3.ìŠ¹ì¸ë§¤ì¶œì •ë³´\": \"ì¹´ë“œ ì‚¬ìš© íŒ¨í„´ ì •ë³´\",\n",
    "        \"4.ì²­êµ¬ì…ê¸ˆì •ë³´\": \"ì²­êµ¬ ë° ì…ê¸ˆ ì´ë ¥\",\n",
    "        \"5.ì”ì•¡ì •ë³´\": \"ê³„ì¢Œ ì”ì•¡ ê´€ë ¨ ì •ë³´\",\n",
    "        \"6.ì±„ë„ì •ë³´\": \"ì‚¬ìš© ì±„ë„ ì •ë³´\",\n",
    "        \"7.ë§ˆì¼€íŒ…ì •ë³´\": \"ë§ˆì¼€íŒ… í™œë™ ë° ë°˜ì‘\",\n",
    "        \"8.ì„±ê³¼ì •ë³´\": \"ì„±ê³¼ ì§€í‘œ\"\n",
    "    }\n",
    "    \n",
    "    months = ['201807', '201808', '201809', '201810', '201811', '201812']\n",
    "    \n",
    "    for category, description in categories.items():\n",
    "        print(f\"\\nğŸ”¹ {category}: {description}\")\n",
    "        \n",
    "        # train íŒŒì¼ í™•ì¸\n",
    "        train_path = f'train/{category}'\n",
    "        if os.path.exists(train_path):\n",
    "            files = [f for f in os.listdir(train_path) if f.endswith('.parquet')]\n",
    "            print(f\"   Train íŒŒì¼ ìˆ˜: {len(files)}\")\n",
    "            \n",
    "            # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "            if files:\n",
    "                first_file = os.path.join(train_path, files[0])\n",
    "                try:\n",
    "                    df = pd.read_parquet(first_file)\n",
    "                    print(f\"   ìƒ˜í”Œ íŒŒì¼ í¬ê¸°: {df.shape}\")\n",
    "                    print(f\"   ì£¼ìš” ì»¬ëŸ¼: {list(df.columns[:5])}...\")\n",
    "                    \n",
    "                    # ë©”ëª¨ë¦¬ í•´ì œ\n",
    "                    del df\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "check_data_files()\n",
    "\n",
    "# 3. í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë¶„ì„ (íšŒì›ì •ë³´ë¡œ ì‹œì‘)\n",
    "def analyze_customer_data():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ” íšŒì›ì •ë³´ ë°ì´í„° ìƒì„¸ ë¶„ì„\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # ì²« ë²ˆì§¸ ì›” ë°ì´í„° ë¡œë“œ\n",
    "        df_customer = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ê¸°ë³¸ ì •ë³´:\")\n",
    "        print(f\"   - ë°ì´í„° í¬ê¸°: {df_customer.shape}\")\n",
    "        print(f\"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df_customer.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ ì»¬ëŸ¼ ì •ë³´:\")\n",
    "        print(f\"   - ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(df_customer.columns)}\")\n",
    "        print(f\"   - ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìˆ˜: {len(df_customer.select_dtypes(include=[np.number]).columns)}\")\n",
    "        print(f\"   - ë²”ì£¼í˜• ì»¬ëŸ¼ ìˆ˜: {len(df_customer.select_dtypes(include=['object']).columns)}\")\n",
    "        \n",
    "        print(f\"\\nğŸ”‘ ì£¼ìš” ì»¬ëŸ¼ (ì²˜ìŒ 10ê°œ):\")\n",
    "        for i, col in enumerate(df_customer.columns[:10]):\n",
    "            dtype = df_customer[col].dtype\n",
    "            non_null = df_customer[col].count()\n",
    "            null_pct = (1 - non_null/len(df_customer)) * 100\n",
    "            print(f\"   {i+1:2d}. {col:<20} | {str(dtype):<10} | ê²°ì¸¡ë¥ : {null_pct:5.1f}%\")\n",
    "        \n",
    "        # Target ë³€ìˆ˜ í™•ì¸ (ìˆë‹¤ë©´)\n",
    "        if 'Segment' in df_customer.columns:\n",
    "            print(f\"\\nğŸ¯ Target ë³€ìˆ˜ ë¶„í¬:\")\n",
    "            target_dist = df_customer['Segment'].value_counts().sort_index()\n",
    "            for segment, count in target_dist.items():\n",
    "                pct = count / len(df_customer) * 100\n",
    "                print(f\"   Segment {segment}: {count:,}ê°œ ({pct:.1f}%)\")\n",
    "        \n",
    "        # ID ì¤‘ë³µ í™•ì¸\n",
    "        if 'ID' in df_customer.columns:\n",
    "            unique_ids = df_customer['ID'].nunique()\n",
    "            total_rows = len(df_customer)\n",
    "            print(f\"\\nğŸ†” ID ì •ë³´:\")\n",
    "            print(f\"   - ì´ í–‰ ìˆ˜: {total_rows:,}\")\n",
    "            print(f\"   - ê³ ìœ  ID ìˆ˜: {unique_ids:,}\")\n",
    "            print(f\"   - ID ì¤‘ë³µ ì—¬ë¶€: {'âŒ ì¤‘ë³µ ìˆìŒ' if unique_ids != total_rows else 'âœ… ì¤‘ë³µ ì—†ìŒ'}\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ í•´ì œ\n",
    "        del df_customer\n",
    "        gc.collect()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íšŒì›ì •ë³´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        return False\n",
    "\n",
    "analyze_customer_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ… 1ë‹¨ê³„ ë°ì´í„° êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\në‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ”:\")\n",
    "print(\"1. ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ EDA\")\n",
    "print(\"2. ì‹œê³„ì—´ ë°ì´í„° íŠ¹ì„± ë¶„ì„\") \n",
    "print(\"3. Target ë³€ìˆ˜ì™€ ê° ì¹´í…Œê³ ë¦¬ì˜ ê´€ê³„ ë¶„ì„\")\n",
    "print(\"\\nì–´ë–¤ ë¶€ë¶„ì„ ë” ìì„¸íˆ ë¶„ì„í•˜ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c39cd057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ê·¹ë¶ˆê· í˜• í´ë˜ìŠ¤ ì‹¬ì¸µ ë¶„ì„: A, B ì„¸ê·¸ë¨¼íŠ¸ í”„ë¡œíŒŒì¼ë§\n",
      "============================================================\n",
      "1ï¸âƒ£ íšŒì›ì •ë³´ ê¸°ë°˜ í¬ê·€ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„\n",
      "----------------------------------------\n",
      "ë°ì´í„° ë¡œë“œ ì™„ë£Œ: (400000, 78)\n",
      "A ì„¸ê·¸ë¨¼íŠ¸: 162ëª…\n",
      "B ì„¸ê·¸ë¨¼íŠ¸: 24ëª…\n",
      "E ì„¸ê·¸ë¨¼íŠ¸ ìƒ˜í”Œ: 1000ëª…\n",
      "\n",
      "2ï¸âƒ£ ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„± ë¹„êµ\n",
      "----------------------------------------\n",
      "ğŸ“Š ì„±ë³„ ë¶„í¬:\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸: ë‚¨ì„±(71.0%), ì—¬ì„±(29.0%)\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸: ë‚¨ì„±(54.2%), ì—¬ì„±(45.8%)\n",
      "   E ì„¸ê·¸ë¨¼íŠ¸: ë‚¨ì„±(51.4%), ì—¬ì„±(48.6%)\n",
      "\n",
      "ğŸ“ˆ ì—°ë ¹ ë¶„í¬ íŠ¹ì„±:\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸ ì—°ë ¹ ìœ í˜•: ['40ëŒ€' '50ëŒ€' '30ëŒ€' '70ëŒ€ì´ìƒ' '60ëŒ€']...\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸ ì—°ë ¹ ìœ í˜•: ['40ëŒ€' '50ëŒ€' '30ëŒ€' '20ëŒ€']...\n",
      "   E ì„¸ê·¸ë¨¼íŠ¸ ì—°ë ¹ ìœ í˜•: ['40ëŒ€' '20ëŒ€' '50ëŒ€' '30ëŒ€' '70ëŒ€ì´ìƒ']...\n",
      "\n",
      "3ï¸âƒ£ ê¸ˆìœµ í™œë™ í•µì‹¬ ì§€í‘œ ë¹„êµ\n",
      "----------------------------------------\n",
      "ê¸ˆìœµ ê´€ë ¨ ì»¬ëŸ¼ ìˆ˜: 54\n",
      "ì£¼ìš” ê¸ˆìœµ ì§€í‘œ:\n",
      "    1. íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥\n",
      "    2. íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA\n",
      "    3. íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡ \n",
      "    4. ì†Œì§€ì—¬ë¶€_ì‹ ìš©\n",
      "    5. ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©\n",
      "    6. ì†Œì§€ì¹´ë“œìˆ˜_ì´ìš©ê°€ëŠ¥_ì‹ ìš©\n",
      "    7. ì´ìš©ê±°ì ˆì—¬ë¶€_ì¹´ë“œë¡ \n",
      "    8. ë™ì˜ì—¬ë¶€_í•œë„ì¦ì•¡ì•ˆë‚´\n",
      "    9. íƒˆíšŒíšŸìˆ˜_ëˆ„ì \n",
      "   10. íƒˆíšŒíšŸìˆ˜_ë°œê¸‰6ê°œì›”ì´ë‚´\n",
      "\n",
      "ğŸ“Š ì£¼ìš” ê¸ˆìœµ ì§€í‘œ í†µê³„ ë¹„êµ:\n",
      "  Segment  Count  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_mean  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_std  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA_mean  \\\n",
      "0       A    162            1.00            0.0               0.99   \n",
      "1       B     24            1.00            0.0               0.96   \n",
      "2       E   1000            0.96            0.2               0.89   \n",
      "\n",
      "   íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA_std  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡ _mean  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡ _std  ì†Œì§€ì—¬ë¶€_ì‹ ìš©_mean  \\\n",
      "0              0.11                0.61               0.49          1.00   \n",
      "1              0.20                0.38               0.49          1.00   \n",
      "2              0.32                0.63               0.48          0.99   \n",
      "\n",
      "   ì†Œì§€ì—¬ë¶€_ì‹ ìš©_std  ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©_mean  ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©_std  \n",
      "0         0.00              2.13             0.78  \n",
      "1         0.00              1.83             0.87  \n",
      "2         0.08              1.18             0.46  \n",
      "\n",
      "4ï¸âƒ£ í¬ê·€ ì„¸ê·¸ë¨¼íŠ¸ì˜ ê·¹ê°’(Outlier) íŠ¹ì„±\n",
      "----------------------------------------\n",
      "ğŸ¯ A, B ì„¸ê·¸ë¨¼íŠ¸ê°€ ê·¹ê°’ì„ ë³´ì´ëŠ” ì£¼ìš” ë³€ìˆ˜:\n",
      "   1. ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©\n",
      "      - ë†’ì€ ê·¹ê°’: 72.6%\n",
      "      - ë‚®ì€ ê·¹ê°’: 0.0%\n",
      "      - ì´ ê·¹ê°’ ë¹„ìœ¨: 72.6%\n",
      "   2. ì†Œì§€ì¹´ë“œìˆ˜_ì´ìš©ê°€ëŠ¥_ì‹ ìš©\n",
      "      - ë†’ì€ ê·¹ê°’: 72.0%\n",
      "      - ë‚®ì€ ê·¹ê°’: 0.0%\n",
      "      - ì´ ê·¹ê°’ ë¹„ìœ¨: 72.0%\n",
      "   3. ë™ì˜ì—¬ë¶€_í•œë„ì¦ì•¡ì•ˆë‚´\n",
      "      - ë†’ì€ ê·¹ê°’: 15.1%\n",
      "      - ë‚®ì€ ê·¹ê°’: 0.0%\n",
      "      - ì´ ê·¹ê°’ ë¹„ìœ¨: 15.1%\n",
      "   4. ì´ìš©ê±°ì ˆì—¬ë¶€_ì¹´ë“œë¡ \n",
      "      - ë†’ì€ ê·¹ê°’: 14.0%\n",
      "      - ë‚®ì€ ê·¹ê°’: 0.0%\n",
      "      - ì´ ê·¹ê°’ ë¹„ìœ¨: 14.0%\n",
      "   5. íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA\n",
      "      - ë†’ì€ ê·¹ê°’: 0.0%\n",
      "      - ë‚®ì€ ê·¹ê°’: 1.6%\n",
      "      - ì´ ê·¹ê°’ ë¹„ìœ¨: 1.6%\n",
      "\n",
      "============================================================\n",
      "âœ… A, B ì„¸ê·¸ë¨¼íŠ¸ í”„ë¡œíŒŒì¼ë§ 1ë‹¨ê³„ ì™„ë£Œ!\n",
      "ğŸ” ë°œê²¬ëœ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë¶„ì„ ë°©í–¥ ì œì‹œ ì˜ˆì •\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import koreanize_matplotlib\n",
    "\n",
    "print(\"ğŸ¯ ê·¹ë¶ˆê· í˜• í´ë˜ìŠ¤ ì‹¬ì¸µ ë¶„ì„: A, B ì„¸ê·¸ë¨¼íŠ¸ í”„ë¡œíŒŒì¼ë§\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_sample_data(category_folder, file_suffix, sample_month='201807'):\n",
    "    \"\"\"íŠ¹ì • ì›” ë°ì´í„°ë§Œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½\"\"\"\n",
    "    file_path = f'train/{category_folder}/{sample_month}_train_{file_suffix}.parquet'\n",
    "    return pd.read_parquet(file_path)\n",
    "\n",
    "# 1. íšŒì›ì •ë³´ì—ì„œ A, B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„± ë¶„ì„\n",
    "print(\"1ï¸âƒ£ íšŒì›ì •ë³´ ê¸°ë°˜ í¬ê·€ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "customer_df = load_sample_data('1.íšŒì›ì •ë³´', 'íšŒì›ì •ë³´')\n",
    "print(f\"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {customer_df.shape}\")\n",
    "\n",
    "# A, B ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ\n",
    "rare_segments = customer_df[customer_df['Segment'].isin(['A', 'B'])].copy()\n",
    "common_segment = customer_df[customer_df['Segment'] == 'E'].sample(1000, random_state=42).copy()  # ëŒ€ì¡°êµ°\n",
    "\n",
    "print(f\"A ì„¸ê·¸ë¨¼íŠ¸: {len(rare_segments[rare_segments['Segment']=='A'])}ëª…\")\n",
    "print(f\"B ì„¸ê·¸ë¨¼íŠ¸: {len(rare_segments[rare_segments['Segment']=='B'])}ëª…\")\n",
    "print(f\"E ì„¸ê·¸ë¨¼íŠ¸ ìƒ˜í”Œ: {len(common_segment)}ëª…\")\n",
    "\n",
    "# 2. ê¸°ë³¸ ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„± ë¹„êµ\n",
    "print(\"\\n2ï¸âƒ£ ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„± ë¹„êµ\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ì„±ë³„ ë¶„í¬\n",
    "print(\"ğŸ“Š ì„±ë³„ ë¶„í¬:\")\n",
    "for segment in ['A', 'B', 'E']:\n",
    "    if segment == 'E':\n",
    "        seg_data = common_segment\n",
    "    else:\n",
    "        seg_data = rare_segments[rare_segments['Segment'] == segment]\n",
    "    \n",
    "    if len(seg_data) > 0:\n",
    "        gender_dist = seg_data['ë‚¨ë…€êµ¬ë¶„ì½”ë“œ'].value_counts(normalize=True) * 100\n",
    "        print(f\"   {segment} ì„¸ê·¸ë¨¼íŠ¸: ë‚¨ì„±({gender_dist.get(1, 0):.1f}%), ì—¬ì„±({gender_dist.get(2, 0):.1f}%)\")\n",
    "\n",
    "# ì—°ë ¹ ë¶„í¬ ë¶„ì„\n",
    "print(\"\\nğŸ“ˆ ì—°ë ¹ ë¶„í¬ íŠ¹ì„±:\")\n",
    "age_stats = {}\n",
    "for segment in ['A', 'B', 'E']:\n",
    "    if segment == 'E':\n",
    "        seg_data = common_segment\n",
    "    else:\n",
    "        seg_data = rare_segments[rare_segments['Segment'] == segment]\n",
    "    \n",
    "    if len(seg_data) > 0:\n",
    "        # ì—°ë ¹ì´ ë¬¸ìì—´ì¸ ê²½ìš° ìˆ«ìë¡œ ë³€í™˜ ì‹œë„\n",
    "        age_col = seg_data['ì—°ë ¹'].copy()\n",
    "        if age_col.dtype == 'object':\n",
    "            # ìˆ«ìê°€ ì•„ë‹Œ ê°’ë“¤ í™•ì¸\n",
    "            unique_ages = age_col.unique()\n",
    "            print(f\"   {segment} ì„¸ê·¸ë¨¼íŠ¸ ì—°ë ¹ ìœ í˜•: {unique_ages[:5]}...\")\n",
    "\n",
    "# 3. ê¸ˆìœµ í™œë™ ì§€í‘œ ë¹„êµ (íšŒì›ì •ë³´ ë‚´ ê¸ˆìœµ ê´€ë ¨ ì»¬ëŸ¼)\n",
    "print(\"\\n3ï¸âƒ£ ê¸ˆìœµ í™œë™ í•µì‹¬ ì§€í‘œ ë¹„êµ\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# íšŒì›ì •ë³´ì—ì„œ ê¸ˆìœµ ê´€ë ¨ ì»¬ëŸ¼ ì¶”ì¶œ\n",
    "financial_cols = [col for col in customer_df.columns if any(keyword in col for keyword in \n",
    "                 ['ì¹´ë“œ', 'í•œë„', 'ì´ìš©', 'ì†Œì§€', 'ì”ì•¡', 'ì‹¤ì ', 'íšŸìˆ˜', 'ê¸ˆì•¡'])]\n",
    "\n",
    "print(f\"ê¸ˆìœµ ê´€ë ¨ ì»¬ëŸ¼ ìˆ˜: {len(financial_cols)}\")\n",
    "print(\"ì£¼ìš” ê¸ˆìœµ ì§€í‘œ:\")\n",
    "for i, col in enumerate(financial_cols[:10]):\n",
    "    print(f\"   {i+1:2d}. {col}\")\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ê¸ˆìœµ ì§€í‘œ í†µê³„ ë¹„êµ\n",
    "numeric_financial = [col for col in financial_cols if customer_df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "comparison_stats = []\n",
    "for segment in ['A', 'B', 'E']:\n",
    "    if segment == 'E':\n",
    "        seg_data = common_segment\n",
    "    else:\n",
    "        seg_data = rare_segments[rare_segments['Segment'] == segment]\n",
    "    \n",
    "    if len(seg_data) > 0:\n",
    "        stats = {}\n",
    "        stats['Segment'] = segment\n",
    "        stats['Count'] = len(seg_data)\n",
    "        \n",
    "        # ì£¼ìš” ì§€í‘œ í†µê³„\n",
    "        for col in numeric_financial[:5]:  # ìƒìœ„ 5ê°œ ì§€í‘œë§Œ\n",
    "            stats[f'{col}_mean'] = seg_data[col].mean()\n",
    "            stats[f'{col}_std'] = seg_data[col].std()\n",
    "        \n",
    "        comparison_stats.append(stats)\n",
    "\n",
    "# í†µê³„ ê²°ê³¼ ì¶œë ¥\n",
    "if comparison_stats:\n",
    "    comparison_df = pd.DataFrame(comparison_stats)\n",
    "    print(\"\\nğŸ“Š ì£¼ìš” ê¸ˆìœµ ì§€í‘œ í†µê³„ ë¹„êµ:\")\n",
    "    print(comparison_df.round(2))\n",
    "\n",
    "# 4. í¬ê·€ ì„¸ê·¸ë¨¼íŠ¸ì˜ ê·¹ê°’ íŠ¹ì„± ë¶„ì„\n",
    "print(\"\\n4ï¸âƒ£ í¬ê·€ ì„¸ê·¸ë¨¼íŠ¸ì˜ ê·¹ê°’(Outlier) íŠ¹ì„±\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ê° ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì—ì„œ A, B ì„¸ê·¸ë¨¼íŠ¸ê°€ ê·¹ê°’ì¸ì§€ í™•ì¸\n",
    "outlier_analysis = {}\n",
    "\n",
    "for col in numeric_financial[:10]:\n",
    "    col_data = customer_df[col].copy()\n",
    "    q1, q3 = col_data.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # ê·¹ê°’ ê¸°ì¤€\n",
    "    upper_extreme = q3 + 3 * iqr  # ë§¤ìš° ë†’ì€ ê°’\n",
    "    lower_extreme = q1 - 3 * iqr  # ë§¤ìš° ë‚®ì€ ê°’\n",
    "    \n",
    "    # A, B ì„¸ê·¸ë¨¼íŠ¸ì˜ í•´ë‹¹ ì»¬ëŸ¼ ê°’ë“¤\n",
    "    rare_values = rare_segments[col].values\n",
    "    \n",
    "    # ê·¹ê°’ ë¹„ìœ¨ ê³„ì‚°\n",
    "    high_outliers = np.mean(rare_values > upper_extreme) * 100\n",
    "    low_outliers = np.mean(rare_values < lower_extreme) * 100\n",
    "    \n",
    "    outlier_analysis[col] = {\n",
    "        'high_outlier_pct': high_outliers,\n",
    "        'low_outlier_pct': low_outliers,\n",
    "        'total_outlier_pct': high_outliers + low_outliers\n",
    "    }\n",
    "\n",
    "# ê·¹ê°’ íŠ¹ì„±ì´ ê°•í•œ ì»¬ëŸ¼ ìƒìœ„ 5ê°œ\n",
    "outlier_df = pd.DataFrame(outlier_analysis).T\n",
    "outlier_df = outlier_df.sort_values('total_outlier_pct', ascending=False)\n",
    "\n",
    "print(\"ğŸ¯ A, B ì„¸ê·¸ë¨¼íŠ¸ê°€ ê·¹ê°’ì„ ë³´ì´ëŠ” ì£¼ìš” ë³€ìˆ˜:\")\n",
    "for i, (col, row) in enumerate(outlier_df.head().iterrows()):\n",
    "    print(f\"   {i+1}. {col}\")\n",
    "    print(f\"      - ë†’ì€ ê·¹ê°’: {row['high_outlier_pct']:.1f}%\")\n",
    "    print(f\"      - ë‚®ì€ ê·¹ê°’: {row['low_outlier_pct']:.1f}%\")\n",
    "    print(f\"      - ì´ ê·¹ê°’ ë¹„ìœ¨: {row['total_outlier_pct']:.1f}%\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del customer_df, rare_segments, common_segment\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… A, B ì„¸ê·¸ë¨¼íŠ¸ í”„ë¡œíŒŒì¼ë§ 1ë‹¨ê³„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ” ë°œê²¬ëœ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë¶„ì„ ë°©í–¥ ì œì‹œ ì˜ˆì •\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605733cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ì²´ê³„ì  ë¶„ì„: Customer Journey ê°€ì„¤ ê²€ì¦\n",
      "======================================================================\n",
      "1ï¸âƒ£ ì„¸ê·¸ë¨¼íŠ¸ë³„ ê¸°ë³¸ í†µê³„ ë° ë¶„í¬ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ ë¶„ì„:\n",
      "   A: 162ëª… (0.040%) | í¬ê·€ë„: 3.39 | ëˆ„ì : 0.0%\n",
      "   B: 24ëª… (0.006%) | í¬ê·€ë„: 4.22 | ëˆ„ì : 0.0%\n",
      "   C: 21,265ëª… (5.316%) | í¬ê·€ë„: 1.27 | ëˆ„ì : 5.4%\n",
      "   D: 58,207ëª… (14.552%) | í¬ê·€ë„: 0.84 | ëˆ„ì : 19.9%\n",
      "   E: 320,342ëª… (80.085%) | í¬ê·€ë„: 0.10 | ëˆ„ì : 100.0%\n",
      "\n",
      "ğŸ¯ í†µê³„ì  ê´€ì°°:\n",
      "   - A,BëŠ” Ultra-Rare (í¬ê·€ë„ > 3.0)\n",
      "   - CëŠ” Rare (í¬ê·€ë„ â‰ˆ 1.3)\n",
      "   - DëŠ” Minor (í¬ê·€ë„ â‰ˆ 0.8)\n",
      "   - EëŠ” Major (í¬ê·€ë„ â‰ˆ 0.1)\n",
      "\n",
      "2ï¸âƒ£ Customer Journey ê°€ì„¤ ê²€ì¦: ìˆœì„œì  íŠ¹ì„± ë¶„ì„\n",
      "--------------------------------------------------\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸: 162ëª… ìƒ˜í”Œë§\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸: 24ëª… ìƒ˜í”Œë§\n",
      "   C ì„¸ê·¸ë¨¼íŠ¸: 1000ëª… ìƒ˜í”Œë§\n",
      "   D ì„¸ê·¸ë¨¼íŠ¸: 2000ëª… ìƒ˜í”Œë§\n",
      "   E ì„¸ê·¸ë¨¼íŠ¸: 2000ëª… ìƒ˜í”Œë§\n",
      "\n",
      "3ï¸âƒ£ í•µì‹¬ ê¸ˆìœµ ì§€í‘œì˜ ìˆœì„œì  ê´€ê³„ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "ğŸ“ˆ í•µì‹¬ ì§€í‘œë³„ ì„¸ê·¸ë¨¼íŠ¸ ìˆœì„œ ê´€ê³„:\n",
      "\n",
      "ğŸ”¹ ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©:\n",
      "   E: 1.173\n",
      "   D: 1.516\n",
      "   C: 1.782\n",
      "   B: 1.833\n",
      "   A: 2.130\n",
      "   â†’ âœ… ë‹¨ì¡°ì¦ê°€: E < D < C < B < A (Customer Journey ì§€ì§€)\n",
      "\n",
      "ğŸ”¹ íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥:\n",
      "   E: 0.953\n",
      "   D: 0.993\n",
      "   C: 0.990\n",
      "   B: 1.000\n",
      "   A: 1.000\n",
      "   â†’ âŒ ë¹„ìˆœì„œì : Categorical íŠ¹ì„±\n",
      "\n",
      "ğŸ”¹ íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA:\n",
      "   E: 0.872\n",
      "   D: 0.940\n",
      "   C: 0.950\n",
      "   B: 0.958\n",
      "   A: 0.988\n",
      "   â†’ âœ… ë‹¨ì¡°ì¦ê°€: E < D < C < B < A (Customer Journey ì§€ì§€)\n",
      "\n",
      "ğŸ”¹ íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡ :\n",
      "   E: 0.614\n",
      "   D: 0.588\n",
      "   C: 0.578\n",
      "   B: 0.375\n",
      "   A: 0.611\n",
      "   â†’ âŒ ë¹„ìˆœì„œì : Categorical íŠ¹ì„±\n",
      "\n",
      "4ï¸âƒ£ ì„¸ê·¸ë¨¼íŠ¸ ê°„ ìœ ì‚¬ë„ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "ğŸ” ì„¸ê·¸ë¨¼íŠ¸ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬):\n",
      "\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸ì™€ì˜ ê±°ë¦¬:\n",
      "     â†’ C: 0.351\n",
      "     â†’ B: 0.380\n",
      "     â†’ D: 0.616\n",
      "\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸ì™€ì˜ ê±°ë¦¬:\n",
      "     â†’ C: 0.210\n",
      "     â†’ A: 0.380\n",
      "     â†’ D: 0.382\n",
      "\n",
      "   C ì„¸ê·¸ë¨¼íŠ¸ì™€ì˜ ê±°ë¦¬:\n",
      "     â†’ B: 0.210\n",
      "     â†’ D: 0.266\n",
      "     â†’ A: 0.351\n",
      "\n",
      "   D ì„¸ê·¸ë¨¼íŠ¸ì™€ì˜ ê±°ë¦¬:\n",
      "     â†’ C: 0.266\n",
      "     â†’ E: 0.354\n",
      "     â†’ B: 0.382\n",
      "\n",
      "   E ì„¸ê·¸ë¨¼íŠ¸ì™€ì˜ ê±°ë¦¬:\n",
      "     â†’ D: 0.354\n",
      "     â†’ C: 0.617\n",
      "     â†’ B: 0.709\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ ì „ëµì  ë¶„ì„ ê²°ë¡ \n",
      "======================================================================\n",
      "ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„± ìš”ì•½:\n",
      "   E (80.1%): Mass Market - ê¸°ë³¸ ì„œë¹„ìŠ¤ ì´ìš©ì\n",
      "   D (14.6%): Active Users - ì ê·¹ì  í™œìš© ê³ ê°\n",
      "   C (5.3%): Premium Candidates - ê³ ê¸‰ ì„œë¹„ìŠ¤ ì§„ì… ë‹¨ê³„\n",
      "   B (0.01%): Premium Elite - ìµœê³ ê¸‰ ì„œë¹„ìŠ¤ í™œìš©ì\n",
      "   A (0.04%): Ultra VIP - ê·¹í•œ í”„ë¦¬ë¯¸ì—„ ê³ ê°\n",
      "\n",
      "ğŸ§  í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\n",
      "   1. ìˆœì„œì (Ordinal) vs ë²”ì£¼ì (Categorical) íŠ¹ì„± í˜¼ì¬\n",
      "   2. A,B-C-D-E ê°„ ì§ˆì  ì°¨ì´ vs C-D-E ê°„ ì–‘ì  ì°¨ì´\n",
      "   3. ê·¹ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•œ A,B íŒ¨í„´ í•™ìŠµ ë‚œì´ë„ ê·¹ìƒ\n",
      "\n",
      "ğŸ¯ ìˆ˜ì •ëœ ë¶„ì„ ì „ëµ:\n",
      "   1. A,B vs Others: Binary Classification ì ‘ê·¼\n",
      "   2. C,D,E: Ordinal Regression ì ‘ê·¼\n",
      "   3. 2ë‹¨ê³„ Hierarchical ëª¨ë¸ë§ ê³ ë ¤\n",
      "   4. A,B ì „ìš© íŠ¹ìˆ˜ Feature Engineering í•„ìˆ˜\n",
      "\n",
      "ë‹¤ìŒ ë‹¨ê³„: A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„ ì˜ˆì •\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import koreanize_matplotlib\n",
    "\n",
    "print(\"ğŸ”¬ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ì²´ê³„ì  ë¶„ì„: Customer Journey ê°€ì„¤ ê²€ì¦\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ ë° í†µê³„ì  íŠ¹ì„± ë¶„ì„\n",
    "print(\"1ï¸âƒ£ ì„¸ê·¸ë¨¼íŠ¸ë³„ ê¸°ë³¸ í†µê³„ ë° ë¶„í¬ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "customer_df = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "\n",
    "# ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ ìƒì„¸ ë¶„ì„\n",
    "segment_dist = customer_df['Segment'].value_counts().sort_index()\n",
    "total_count = len(customer_df)\n",
    "\n",
    "print(\"ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ ë¶„ì„:\")\n",
    "cumulative_pct = 0\n",
    "for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "    count = segment_dist.get(segment, 0)\n",
    "    pct = (count / total_count) * 100\n",
    "    cumulative_pct += pct\n",
    "    rarity_score = -np.log10(pct/100)  # í¬ê·€ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ í¬ê·€)\n",
    "    \n",
    "    print(f\"   {segment}: {count:,}ëª… ({pct:.3f}%) | í¬ê·€ë„: {rarity_score:.2f} | ëˆ„ì : {cumulative_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ¯ í†µê³„ì  ê´€ì°°:\")\n",
    "print(f\"   - A,BëŠ” Ultra-Rare (í¬ê·€ë„ > 3.0)\")\n",
    "print(f\"   - CëŠ” Rare (í¬ê·€ë„ â‰ˆ 1.3)\")  \n",
    "print(f\"   - DëŠ” Minor (í¬ê·€ë„ â‰ˆ 0.8)\")\n",
    "print(f\"   - EëŠ” Major (í¬ê·€ë„ â‰ˆ 0.1)\")\n",
    "\n",
    "# 2. Customer Journey ê°€ì„¤ ê²€ì¦: ìˆœì„œì  íŠ¹ì„± ë¶„ì„\n",
    "print(\"\\n2ï¸âƒ£ Customer Journey ê°€ì„¤ ê²€ì¦: ìˆœì„œì  íŠ¹ì„± ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ìƒ˜í”Œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)\n",
    "segment_samples = {}\n",
    "sample_sizes = {'A': 162, 'B': 24, 'C': 1000, 'D': 2000, 'E': 2000}\n",
    "\n",
    "for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "    segment_data = customer_df[customer_df['Segment'] == segment]\n",
    "    sample_size = min(len(segment_data), sample_sizes[segment])\n",
    "    \n",
    "    if len(segment_data) > sample_size:\n",
    "        segment_samples[segment] = segment_data.sample(sample_size, random_state=42)\n",
    "    else:\n",
    "        segment_samples[segment] = segment_data.copy()\n",
    "    \n",
    "    print(f\"   {segment} ì„¸ê·¸ë¨¼íŠ¸: {len(segment_samples[segment])}ëª… ìƒ˜í”Œë§\")\n",
    "\n",
    "# 3. í•µì‹¬ ê¸ˆìœµ ì§€í‘œì˜ ìˆœì„œì  ê´€ê³„ ë¶„ì„\n",
    "print(\"\\n3ï¸âƒ£ í•µì‹¬ ê¸ˆìœµ ì§€í‘œì˜ ìˆœì„œì  ê´€ê³„ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# í•µì‹¬ ê¸ˆìœµ ì§€í‘œ ì„ ì •\n",
    "key_financial_metrics = [\n",
    "    'ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©',\n",
    "    'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥', \n",
    "    'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA',\n",
    "    'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡ '\n",
    "]\n",
    "\n",
    "# ì„¸ê·¸ë¨¼íŠ¸ë³„ í•µì‹¬ ì§€í‘œ í†µê³„\n",
    "segment_stats = []\n",
    "\n",
    "for segment in ['E', 'D', 'C', 'B', 'A']:  # ìˆœì„œ ì¤‘ìš”: ë‚®ì€ ë‹¨ê³„ë¶€í„°\n",
    "    data = segment_samples[segment]\n",
    "    \n",
    "    stats = {'Segment': segment, 'Count': len(data)}\n",
    "    \n",
    "    for metric in key_financial_metrics:\n",
    "        if metric in data.columns:\n",
    "            stats[f'{metric}_mean'] = data[metric].mean()\n",
    "            stats[f'{metric}_median'] = data[metric].median()\n",
    "            stats[f'{metric}_std'] = data[metric].std()\n",
    "    \n",
    "    segment_stats.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(segment_stats)\n",
    "\n",
    "print(\"ğŸ“ˆ í•µì‹¬ ì§€í‘œë³„ ì„¸ê·¸ë¨¼íŠ¸ ìˆœì„œ ê´€ê³„:\")\n",
    "for metric in key_financial_metrics:\n",
    "    mean_col = f'{metric}_mean'\n",
    "    if mean_col in stats_df.columns:\n",
    "        print(f\"\\nğŸ”¹ {metric}:\")\n",
    "        for _, row in stats_df.iterrows():\n",
    "            value = row[mean_col]\n",
    "            print(f\"   {row['Segment']}: {value:.3f}\")\n",
    "        \n",
    "        # ìˆœì„œì  ê´€ê³„ ê²€ì¦ (ë‹¨ì¡°ì¦ê°€/ê°ì†Œ í™•ì¸)\n",
    "        means = stats_df[mean_col].tolist()\n",
    "        is_increasing = all(means[i] <= means[i+1] for i in range(len(means)-1))\n",
    "        is_decreasing = all(means[i] >= means[i+1] for i in range(len(means)-1))\n",
    "        \n",
    "        if is_increasing:\n",
    "            print(f\"   â†’ âœ… ë‹¨ì¡°ì¦ê°€: E < D < C < B < A (Customer Journey ì§€ì§€)\")\n",
    "        elif is_decreasing:\n",
    "            print(f\"   â†’ âš ï¸ ë‹¨ì¡°ê°ì†Œ: E > D > C > B > A\")\n",
    "        else:\n",
    "            print(f\"   â†’ âŒ ë¹„ìˆœì„œì : Categorical íŠ¹ì„±\")\n",
    "\n",
    "# 4. ì„¸ê·¸ë¨¼íŠ¸ ê°„ ê±°ë¦¬ ë¶„ì„ (ìœ ì‚¬ë„ ì¸¡ì •)\n",
    "print(\"\\n4ï¸âƒ£ ì„¸ê·¸ë¨¼íŠ¸ ê°„ ìœ ì‚¬ë„ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ì„¸ê·¸ë¨¼íŠ¸ë³„ í”„ë¡œí•„ ë²¡í„° ìƒì„±\n",
    "segment_profiles = {}\n",
    "\n",
    "for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "    data = segment_samples[segment]\n",
    "    profile = []\n",
    "    \n",
    "    for metric in key_financial_metrics:\n",
    "        if metric in data.columns:\n",
    "            profile.append(data[metric].mean())\n",
    "    \n",
    "    segment_profiles[segment] = np.array(profile)\n",
    "\n",
    "# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°\n",
    "print(\"ğŸ” ì„¸ê·¸ë¨¼íŠ¸ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬):\")\n",
    "distance_matrix = {}\n",
    "\n",
    "for seg1 in ['A', 'B', 'C', 'D', 'E']:\n",
    "    distances = {}\n",
    "    for seg2 in ['A', 'B', 'C', 'D', 'E']:\n",
    "        if seg1 != seg2:\n",
    "            dist = np.linalg.norm(segment_profiles[seg1] - segment_profiles[seg2])\n",
    "            distances[seg2] = dist\n",
    "    distance_matrix[seg1] = distances\n",
    "\n",
    "for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "    print(f\"\\n   {segment} ì„¸ê·¸ë¨¼íŠ¸ì™€ì˜ ê±°ë¦¬:\")\n",
    "    sorted_distances = sorted(distance_matrix[segment].items(), key=lambda x: x[1])\n",
    "    for other_seg, dist in sorted_distances[:3]:  # ê°€ì¥ ê°€ê¹Œìš´ 3ê°œ\n",
    "        print(f\"     â†’ {other_seg}: {dist:.3f}\")\n",
    "\n",
    "# 5. ì „ëµì  ë¶„ì„ ê²°ë¡ \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ ì „ëµì  ë¶„ì„ ê²°ë¡ \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„± ìš”ì•½:\")\n",
    "print(\"   E (80.1%): Mass Market - ê¸°ë³¸ ì„œë¹„ìŠ¤ ì´ìš©ì\")\n",
    "print(\"   D (14.6%): Active Users - ì ê·¹ì  í™œìš© ê³ ê°\") \n",
    "print(\"   C (5.3%): Premium Candidates - ê³ ê¸‰ ì„œë¹„ìŠ¤ ì§„ì… ë‹¨ê³„\")\n",
    "print(\"   B (0.01%): Premium Elite - ìµœê³ ê¸‰ ì„œë¹„ìŠ¤ í™œìš©ì\")\n",
    "print(\"   A (0.04%): Ultra VIP - ê·¹í•œ í”„ë¦¬ë¯¸ì—„ ê³ ê°\")\n",
    "\n",
    "print(\"\\nğŸ§  í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\n",
    "print(\"   1. ìˆœì„œì (Ordinal) vs ë²”ì£¼ì (Categorical) íŠ¹ì„± í˜¼ì¬\")\n",
    "print(\"   2. A,B-C-D-E ê°„ ì§ˆì  ì°¨ì´ vs C-D-E ê°„ ì–‘ì  ì°¨ì´\")\n",
    "print(\"   3. ê·¹ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•œ A,B íŒ¨í„´ í•™ìŠµ ë‚œì´ë„ ê·¹ìƒ\")\n",
    "\n",
    "print(\"\\nğŸ¯ ìˆ˜ì •ëœ ë¶„ì„ ì „ëµ:\")\n",
    "print(\"   1. A,B vs Others: Binary Classification ì ‘ê·¼\")\n",
    "print(\"   2. C,D,E: Ordinal Regression ì ‘ê·¼\") \n",
    "print(\"   3. 2ë‹¨ê³„ Hierarchical ëª¨ë¸ë§ ê³ ë ¤\")\n",
    "print(\"   4. A,B ì „ìš© íŠ¹ìˆ˜ Feature Engineering í•„ìˆ˜\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del customer_df, segment_samples\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\në‹¤ìŒ ë‹¨ê³„: A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„ ì˜ˆì •\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c0c11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ ë° Premium Gateway ë¶„ì„\n",
      "=================================================================\n",
      "1ï¸âƒ£ Premium ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ í–‰ë™ íŒ¨í„´ ë°œêµ´\n",
      "--------------------------------------------------\n",
      "ë¶„ì„ ëŒ€ìƒ:\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸: 162ëª…\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸: 24ëª…\n",
      "   C ì„¸ê·¸ë¨¼íŠ¸: 500ëª…\n",
      "   D ì„¸ê·¸ë¨¼íŠ¸: 500ëª… (ìƒ˜í”Œ)\n",
      "   E ì„¸ê·¸ë¨¼íŠ¸: 500ëª… (ìƒ˜í”Œ)\n",
      "\n",
      "2ï¸âƒ£ ìŠ¹ì¸ë§¤ì¶œ íŒ¨í„´: Premium vs Mass ê·¹ëª…í•œ ì°¨ì´\n",
      "--------------------------------------------------\n",
      "ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼:\n",
      "   ê¸ˆì•¡ ê´€ë ¨: 10ê°œ\n",
      "   ê±´ìˆ˜ ê´€ë ¨: 10ê°œ\n",
      "\n",
      "ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ë³„ ìŠ¹ì¸ë§¤ì¶œ í•µì‹¬ ì§€í‘œ:\n",
      "\n",
      "ğŸ”¹ ì´ìš©ê¸ˆì•¡_ì¼ì‹œë¶ˆ_B0M í‰ê·  ì‚¬ìš©ê¸ˆì•¡:\n",
      "   A: 19,983ì›\n",
      "   B: 18,484ì›\n",
      "   C: 12,071ì›\n",
      "   D: 7,776ì›\n",
      "   E: 2,713ì›\n",
      "\n",
      "ğŸ’¡ A vs E ë°°ìœ¨: 7.4ë°° ì°¨ì´!\n",
      "\n",
      "3ï¸âƒ£ ì‹ ìš©ì •ë³´: Premium ì‹ ìš©ë„ íŠ¹ì„±\n",
      "--------------------------------------------------\n",
      "í•œë„ ê´€ë ¨ ì»¬ëŸ¼: 21ê°œ\n",
      "   1. ìµœì´ˆí•œë„ê¸ˆì•¡\n",
      "   2. ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡\n",
      "   3. CAí•œë„ê¸ˆì•¡\n",
      "   4. ì¼ì‹œìƒí™˜ë¡ í•œë„ê¸ˆì•¡\n",
      "   5. ì›”ìƒí™˜ë¡ í•œë„ê¸ˆì•¡\n",
      "\n",
      "ğŸ“ˆ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹ ìš© í•œë„ ë¹„êµ:\n",
      "\n",
      "ğŸ”¹ ìµœì´ˆí•œë„ê¸ˆì•¡ í‰ê· :\n",
      "   A: 10,844ì›\n",
      "   B: 4,407ì›\n",
      "   C: 8,602ì›\n",
      "   D: 5,332ì›\n",
      "   E: 3,716ì›\n",
      "\n",
      "4ï¸âƒ£ Premium Gateway ë¶„ì„: Câ†’B,A ì „í™˜ íŠ¸ë¦¬ê±°\n",
      "--------------------------------------------------\n",
      "ì”ì•¡ ê´€ë¦¬ í•µì‹¬ ì§€í‘œ: 5ê°œ\n",
      "\n",
      "ğŸ’° ì”ì•¡_ì¼ì‹œë¶ˆ_B0M ê´€ë¦¬ íŒ¨í„´:\n",
      "   A: í‰ê·  20,258ì›, ë³€ë™ì„± 0.639, ê³ ì•¡ë¹„ìœ¨ 79.0%\n",
      "   B: í‰ê·  19,918ì›, ë³€ë™ì„± 0.789, ê³ ì•¡ë¹„ìœ¨ 62.5%\n",
      "   C: í‰ê·  11,783ì›, ë³€ë™ì„± 1.050, ê³ ì•¡ë¹„ìœ¨ 49.4%\n",
      "   D: í‰ê·  7,375ì›, ë³€ë™ì„± 1.076, ê³ ì•¡ë¹„ìœ¨ 32.4%\n",
      "   E: í‰ê·  2,148ì›, ë³€ë™ì„± 1.599, ê³ ì•¡ë¹„ìœ¨ 4.4%\n",
      "\n",
      "=================================================================\n",
      "ğŸ¯ Premium ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ í•µì‹¬ ë°œê²¬\n",
      "=================================================================\n",
      "ğŸ”¬ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\n",
      "   1. A,B ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì§ˆì ìœ¼ë¡œ ë‹¤ë¥¸ ê¸ˆìœµ í–‰ë™ íŒ¨í„´\n",
      "   2. C ì„¸ê·¸ë¨¼íŠ¸ëŠ” Premium Gateway - ì „í™˜ì  ì—­í• \n",
      "   3. ì‚¬ìš©ê¸ˆì•¡, ì‹ ìš©í•œë„, ì”ì•¡ê´€ë¦¬ì—ì„œ ê·¹ëª…í•œ ì°¨ì´\n",
      "   4. ë‹¨ìˆœ ê·œëª¨ì˜ ì°¨ì´ê°€ ì•„ë‹Œ ì „ëµì  í™œìš©ë„ì˜ ì°¨ì´\n",
      "\n",
      "ğŸ§  ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í•´ì„:\n",
      "   A,B = Portfolio Strategists (ì¹´ë“œë¥¼ íˆ¬ì/ì „ëµ ë„êµ¬ë¡œ í™œìš©)\n",
      "   C = Premium Aspirants (í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤ ì ê·¹ íƒìƒ‰)\n",
      "   D,E = Standard Users (ì¹´ë“œë¥¼ ê²°ì œ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ í™œìš©)\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ëª¨ë¸ë§ ì „ëµ:\n",
      "   1. A,B ì „ìš© íŠ¹ìˆ˜ í”¼ì²˜: Portfolio Efficiency, Utilization Strategy\n",
      "   2. C ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ Gateway Detection ëª¨ë¸\n",
      "   3. Hierarchical Classification: Premium Detection â†’ Segmentation\n",
      "   4. ê·¹ë¶ˆê· í˜• í•´ê²°: SMOTE + Class-weighted Ensemble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import koreanize_matplotlib\n",
    "\n",
    "print(\"ğŸ”¬ A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ ë° Premium Gateway ë¶„ì„\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# 1. ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ë°ì´í„° í†µí•© ë¶„ì„ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì ‘ê·¼\n",
    "print(\"1ï¸âƒ£ Premium ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ í–‰ë™ íŒ¨í„´ ë°œêµ´\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# A,B,C ì„¸ê·¸ë¨¼íŠ¸ ID ì¶”ì¶œ (Premium í´ëŸ¬ìŠ¤í„°)\n",
    "customer_base = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "\n",
    "# ì„¸ê·¸ë¨¼íŠ¸ë³„ ID ì¶”ì¶œ\n",
    "premium_ids = {\n",
    "    'A': customer_base[customer_base['Segment'] == 'A']['ID'].tolist(),\n",
    "    'B': customer_base[customer_base['Segment'] == 'B']['ID'].tolist(), \n",
    "    'C': customer_base[customer_base['Segment'] == 'C']['ID'].sample(500, random_state=42).tolist()\n",
    "}\n",
    "\n",
    "mass_ids = {\n",
    "    'D': customer_base[customer_base['Segment'] == 'D']['ID'].sample(500, random_state=42).tolist(),\n",
    "    'E': customer_base[customer_base['Segment'] == 'E']['ID'].sample(500, random_state=42).tolist()\n",
    "}\n",
    "\n",
    "print(f\"ë¶„ì„ ëŒ€ìƒ:\")\n",
    "for seg, ids in premium_ids.items():\n",
    "    print(f\"   {seg} ì„¸ê·¸ë¨¼íŠ¸: {len(ids)}ëª…\")\n",
    "for seg, ids in mass_ids.items():\n",
    "    print(f\"   {seg} ì„¸ê·¸ë¨¼íŠ¸: {len(ids)}ëª… (ìƒ˜í”Œ)\")\n",
    "\n",
    "del customer_base\n",
    "gc.collect()\n",
    "\n",
    "# 2. ìŠ¹ì¸ë§¤ì¶œì •ë³´ì—ì„œ Premium í–‰ë™ íŒ¨í„´ ë¶„ì„\n",
    "print(\"\\n2ï¸âƒ£ ìŠ¹ì¸ë§¤ì¶œ íŒ¨í„´: Premium vs Mass ê·¹ëª…í•œ ì°¨ì´\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 201807 ìŠ¹ì¸ë§¤ì¶œ ë°ì´í„° ë¡œë“œ\n",
    "sales_df = pd.read_parquet('train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201807_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet')\n",
    "\n",
    "# ê¸ˆì•¡ ê´€ë ¨ í•µì‹¬ ì»¬ëŸ¼ ì¶”ì¶œ (ìƒìœ„ 10ê°œ)\n",
    "amount_cols = [col for col in sales_df.columns if 'ê¸ˆì•¡' in col and sales_df[col].dtype in ['int64', 'float64']][:10]\n",
    "count_cols = [col for col in sales_df.columns if 'ê±´ìˆ˜' in col and sales_df[col].dtype in ['int64', 'float64']][:10]\n",
    "\n",
    "print(f\"ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼:\")\n",
    "print(f\"   ê¸ˆì•¡ ê´€ë ¨: {len(amount_cols)}ê°œ\")\n",
    "print(f\"   ê±´ìˆ˜ ê´€ë ¨: {len(count_cols)}ê°œ\")\n",
    "\n",
    "# Premium vs Mass ì‚¬ìš© íŒ¨í„´ ë¹„êµ\n",
    "usage_comparison = []\n",
    "\n",
    "all_ids = {}\n",
    "all_ids.update(premium_ids)\n",
    "all_ids.update(mass_ids)\n",
    "\n",
    "for segment, ids in all_ids.items():\n",
    "    segment_data = sales_df[sales_df['ID'].isin(ids)]\n",
    "    \n",
    "    if len(segment_data) > 0:\n",
    "        stats = {'Segment': segment, 'Count': len(segment_data)}\n",
    "        \n",
    "        # ì£¼ìš” ê¸ˆì•¡ ì§€í‘œ í†µê³„\n",
    "        for col in amount_cols[:5]:  # ìƒìœ„ 5ê°œë§Œ\n",
    "            stats[f'{col}_mean'] = segment_data[col].mean()\n",
    "            stats[f'{col}_median'] = segment_data[col].median()\n",
    "            stats[f'{col}_sum'] = segment_data[col].sum()\n",
    "            stats[f'{col}_std'] = segment_data[col].std()\n",
    "        \n",
    "        # ì£¼ìš” ê±´ìˆ˜ ì§€í‘œ í†µê³„  \n",
    "        for col in count_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ\n",
    "            stats[f'{col}_mean'] = segment_data[col].mean()\n",
    "            stats[f'{col}_sum'] = segment_data[col].sum()\n",
    "            \n",
    "        usage_comparison.append(stats)\n",
    "\n",
    "usage_df = pd.DataFrame(usage_comparison)\n",
    "\n",
    "# í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶œë ¥\n",
    "if not usage_df.empty:\n",
    "    print(\"\\nğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ë³„ ìŠ¹ì¸ë§¤ì¶œ í•µì‹¬ ì§€í‘œ:\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ê¸ˆì•¡ ì»¬ëŸ¼ìœ¼ë¡œ ë¹„êµ\n",
    "    first_amount_col = [col for col in usage_df.columns if amount_cols[0] in col and '_mean' in col][0]\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ {amount_cols[0]} í‰ê·  ì‚¬ìš©ê¸ˆì•¡:\")\n",
    "    for _, row in usage_df.iterrows():\n",
    "        amount = row[first_amount_col]\n",
    "        print(f\"   {row['Segment']}: {amount:,.0f}ì›\")\n",
    "    \n",
    "    # A,B vs E ë¹„êµ\n",
    "    a_amount = usage_df[usage_df['Segment'] == 'A'][first_amount_col].iloc[0] if not usage_df[usage_df['Segment'] == 'A'].empty else 0\n",
    "    e_amount = usage_df[usage_df['Segment'] == 'E'][first_amount_col].iloc[0] if not usage_df[usage_df['Segment'] == 'E'].empty else 1\n",
    "    \n",
    "    if e_amount > 0:\n",
    "        ratio = a_amount / e_amount\n",
    "        print(f\"\\nğŸ’¡ A vs E ë°°ìœ¨: {ratio:.1f}ë°° ì°¨ì´!\")\n",
    "\n",
    "del sales_df\n",
    "gc.collect()\n",
    "\n",
    "# 3. ì‹ ìš©ì •ë³´ì—ì„œ Premium ì‹ ìš© íŠ¹ì„± ë¶„ì„\n",
    "print(\"\\n3ï¸âƒ£ ì‹ ìš©ì •ë³´: Premium ì‹ ìš©ë„ íŠ¹ì„±\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "credit_df = pd.read_parquet('train/2.ì‹ ìš©ì •ë³´/201807_train_ì‹ ìš©ì •ë³´.parquet')\n",
    "\n",
    "# í•œë„ ê´€ë ¨ í•µì‹¬ ì»¬ëŸ¼\n",
    "limit_cols = [col for col in credit_df.columns if 'í•œë„' in col and credit_df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"í•œë„ ê´€ë ¨ ì»¬ëŸ¼: {len(limit_cols)}ê°œ\")\n",
    "for i, col in enumerate(limit_cols[:5]):\n",
    "    print(f\"   {i+1}. {col}\")\n",
    "\n",
    "# ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹ ìš© í•œë„ ë¶„ì„\n",
    "credit_comparison = []\n",
    "\n",
    "for segment, ids in all_ids.items():\n",
    "    segment_data = credit_df[credit_df['ID'].isin(ids)]\n",
    "    \n",
    "    if len(segment_data) > 0:\n",
    "        stats = {'Segment': segment, 'Count': len(segment_data)}\n",
    "        \n",
    "        for col in limit_cols[:3]:  # ìƒìœ„ 3ê°œ í•œë„ ì»¬ëŸ¼\n",
    "            stats[f'{col}_mean'] = segment_data[col].mean()\n",
    "            stats[f'{col}_median'] = segment_data[col].median()\n",
    "            stats[f'{col}_std'] = segment_data[col].std()\n",
    "            \n",
    "        credit_comparison.append(stats)\n",
    "\n",
    "credit_comp_df = pd.DataFrame(credit_comparison)\n",
    "\n",
    "if not credit_comp_df.empty:\n",
    "    print(\"\\nğŸ“ˆ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹ ìš© í•œë„ ë¹„êµ:\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ í•œë„ ì»¬ëŸ¼ ë¹„êµ\n",
    "    if limit_cols:\n",
    "        first_limit_col = f'{limit_cols[0]}_mean'\n",
    "        print(f\"\\nğŸ”¹ {limit_cols[0]} í‰ê· :\")\n",
    "        for _, row in credit_comp_df.iterrows():\n",
    "            limit_amount = row[first_limit_col]\n",
    "            print(f\"   {row['Segment']}: {limit_amount:,.0f}ì›\")\n",
    "\n",
    "del credit_df\n",
    "gc.collect()\n",
    "\n",
    "# 4. Premium Gateway ë¶„ì„: Câ†’B,A ì „í™˜ íŠ¸ë¦¬ê±° ë°œêµ´\n",
    "print(\"\\n4ï¸âƒ£ Premium Gateway ë¶„ì„: Câ†’B,A ì „í™˜ íŠ¸ë¦¬ê±°\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ì”ì•¡ì •ë³´ì—ì„œ ìœ ë™ì„± ê´€ë¦¬ íŒ¨í„´ ë¶„ì„\n",
    "balance_df = pd.read_parquet('train/5.ì”ì•¡ì •ë³´/201807_train_ì”ì•¡ì •ë³´.parquet')\n",
    "\n",
    "# ì”ì•¡ ê´€ë ¨ í•µì‹¬ ì»¬ëŸ¼\n",
    "balance_cols = [col for col in balance_df.columns if 'ì”ì•¡' in col and balance_df[col].dtype in ['int64', 'float64']][:5]\n",
    "\n",
    "print(f\"ì”ì•¡ ê´€ë¦¬ í•µì‹¬ ì§€í‘œ: {len(balance_cols)}ê°œ\")\n",
    "\n",
    "balance_patterns = []\n",
    "\n",
    "for segment, ids in all_ids.items():\n",
    "    segment_data = balance_df[balance_df['ID'].isin(ids)]\n",
    "    \n",
    "    if len(segment_data) > 0:\n",
    "        stats = {'Segment': segment}\n",
    "        \n",
    "        for col in balance_cols:\n",
    "            # í‰ê· , ì¤‘ìœ„ìˆ˜, ë³€ë™ì„±\n",
    "            stats[f'{col}_mean'] = segment_data[col].mean()\n",
    "            stats[f'{col}_cv'] = segment_data[col].std() / (segment_data[col].mean() + 1)  # ë³€ë™ê³„ìˆ˜\n",
    "            \n",
    "            # ê³ ì•¡ ì”ì•¡ ë¹„ìœ¨ (ìƒìœ„ 10%)\n",
    "            q90 = balance_df[col].quantile(0.9)\n",
    "            stats[f'{col}_high_ratio'] = (segment_data[col] > q90).mean() * 100\n",
    "            \n",
    "        balance_patterns.append(stats)\n",
    "\n",
    "balance_df_result = pd.DataFrame(balance_patterns)\n",
    "\n",
    "if not balance_df_result.empty and balance_cols:\n",
    "    print(f\"\\nğŸ’° {balance_cols[0]} ê´€ë¦¬ íŒ¨í„´:\")\n",
    "    for _, row in balance_df_result.iterrows():\n",
    "        mean_val = row[f'{balance_cols[0]}_mean']\n",
    "        cv_val = row[f'{balance_cols[0]}_cv']\n",
    "        high_ratio = row[f'{balance_cols[0]}_high_ratio']\n",
    "        \n",
    "        print(f\"   {row['Segment']}: í‰ê·  {mean_val:,.0f}ì›, ë³€ë™ì„± {cv_val:.3f}, ê³ ì•¡ë¹„ìœ¨ {high_ratio:.1f}%\")\n",
    "\n",
    "del balance_df\n",
    "gc.collect()\n",
    "\n",
    "# 5. í•µì‹¬ ë°œê²¬ì‚¬í•­ ë° ì „ëµì  ê²°ë¡ \n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"ğŸ¯ Premium ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ í•µì‹¬ ë°œê²¬\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "print(\"ğŸ”¬ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\n",
    "print(\"   1. A,B ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì§ˆì ìœ¼ë¡œ ë‹¤ë¥¸ ê¸ˆìœµ í–‰ë™ íŒ¨í„´\")\n",
    "print(\"   2. C ì„¸ê·¸ë¨¼íŠ¸ëŠ” Premium Gateway - ì „í™˜ì  ì—­í• \")\n",
    "print(\"   3. ì‚¬ìš©ê¸ˆì•¡, ì‹ ìš©í•œë„, ì”ì•¡ê´€ë¦¬ì—ì„œ ê·¹ëª…í•œ ì°¨ì´\")\n",
    "print(\"   4. ë‹¨ìˆœ ê·œëª¨ì˜ ì°¨ì´ê°€ ì•„ë‹Œ ì „ëµì  í™œìš©ë„ì˜ ì°¨ì´\")\n",
    "\n",
    "print(\"\\nğŸ§  ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í•´ì„:\")\n",
    "print(\"   A,B = Portfolio Strategists (ì¹´ë“œë¥¼ íˆ¬ì/ì „ëµ ë„êµ¬ë¡œ í™œìš©)\")\n",
    "print(\"   C = Premium Aspirants (í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤ ì ê·¹ íƒìƒ‰)\")  \n",
    "print(\"   D,E = Standard Users (ì¹´ë“œë¥¼ ê²°ì œ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ í™œìš©)\")\n",
    "\n",
    "print(\"\\nğŸ¯ ë‹¤ìŒ ëª¨ë¸ë§ ì „ëµ:\")\n",
    "print(\"   1. A,B ì „ìš© íŠ¹ìˆ˜ í”¼ì²˜: Portfolio Efficiency, Utilization Strategy\")\n",
    "print(\"   2. C ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ Gateway Detection ëª¨ë¸\")\n",
    "print(\"   3. Hierarchical Classification: Premium Detection â†’ Segmentation\")\n",
    "print(\"   4. ê·¹ë¶ˆê· í˜• í•´ê²°: SMOTE + Class-weighted Ensemble\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa941a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì „ëµì  í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: Premium Detection íŠ¹í™” ë³€ìˆ˜ ì„¤ê³„\n",
      "======================================================================\n",
      "ğŸ’¡ í•µì‹¬ ê°€ì„¤: A,B ì„¸ê·¸ë¨¼íŠ¸ = Card Portfolio Strategists\n",
      "   - ë‹¤ì¤‘ ì¹´ë“œ ì „ëµì  í™œìš©\n",
      "   - ìœ ë™ì„± ê´€ë¦¬ ì „ë¬¸ì„±\n",
      "   - ì‹œê³„ì—´ì  ì¼ê´€ì„±\n",
      "   - ì±„ë„ ë‹¤ë³€í™”\n",
      "\n",
      "ğŸ¯ ì„¤ê³„í•  íŒŒìƒë³€ìˆ˜ ì¹´í…Œê³ ë¦¬:\n",
      "\n",
      "1ï¸âƒ£ Portfolio Complexity Features\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¹ ì¹´ë“œ í¬íŠ¸í´ë¦¬ì˜¤ ë³µì¡ë„:\n",
      "   â€¢ ì¹´ë“œì¢…ë¥˜_ë‹¤ì–‘ì„±_ì§€ìˆ˜ = unique_card_types / total_cards\n",
      "   â€¢ ì¹´ë“œí™œìš©_íš¨ìœ¨ì„± = active_cards / total_cards\n",
      "   â€¢ í¬íŠ¸í´ë¦¬ì˜¤_ê· í˜•ë„ = std(card_usage) / mean(card_usage)\n",
      "\n",
      "ğŸ”¹ ì „ëµì  í™œìš© ì§€í‘œ:\n",
      "   â€¢ í•œë„ëŒ€ë¹„_í™œìš©ë¥  = used_amount / total_limit\n",
      "   â€¢ ì±„ë„_ë‹¤ë³€í™”_ì§€ìˆ˜ = unique_channels / total_transactions\n",
      "   â€¢ ì œí’ˆë³„_ì§‘ì¤‘ë„ = max(product_usage) / total_usage\n",
      "\n",
      "2ï¸âƒ£ Temporal Stability Features\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¹ ì‹œê³„ì—´ ì•ˆì •ì„±:\n",
      "   â€¢ ì›”ë³„_ì‚¬ìš©íŒ¨í„´_ë³€ë™ê³„ìˆ˜ = std(monthly_usage) / mean(monthly_usage)\n",
      "   â€¢ íŠ¸ë Œë“œ_ì¼ê´€ì„±_ì§€ìˆ˜ = correlation(month, usage_amount)\n",
      "   â€¢ ê³„ì ˆì„±_íŒ¨í„´_ê°•ë„ = seasonal_decomposition_strength\n",
      "\n",
      "ğŸ”¹ ë³€í™”ìœ¨ ì§€í‘œ:\n",
      "   â€¢ ì¹´ë“œì¶”ê°€_ì†ë„ = (cards_final - cards_initial) / months\n",
      "   â€¢ í•œë„ì¦ê°€_íŒ¨í„´ = limit_growth_trend_slope\n",
      "   â€¢ ì‚¬ìš©_ì§„í™”_ë°©í–¥ = usage_pattern_evolution_score\n",
      "\n",
      "3ï¸âƒ£ Financial Sophistication Features\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¹ ê¸ˆìœµ ì „ë¬¸ì„±:\n",
      "   â€¢ ìœ ë™ì„±_ê´€ë¦¬_íš¨ìœ¨ì„± = cash_advance_timing_score\n",
      "   â€¢ ë¦¬ìŠ¤í¬_ê´€ë¦¬_ì§€í‘œ = balance_volatility_control\n",
      "   â€¢ ìˆ˜ìµ_ìµœì í™”_íŒ¨í„´ = reward_maximization_score\n",
      "\n",
      "ğŸ”¹ í–‰ë™ íŒ¨í„´:\n",
      "   â€¢ ê²°ì œ_íƒ€ì´ë°_ì „ëµ = payment_timing_optimization\n",
      "   â€¢ ì±„ë„_ì„ íƒ_ìµœì í™” = channel_cost_efficiency\n",
      "   â€¢ í”„ë¡œëª¨ì…˜_í™œìš©_ëŠ¥ë ¥ = marketing_response_intelligence\n",
      "\n",
      "4ï¸âƒ£ í•µì‹¬ íŒŒìƒë³€ìˆ˜ êµ¬í˜„ ì˜ˆì‹œ\n",
      "----------------------------------------\n",
      "ğŸ’¡ Portfolio Complexity Score êµ¬í˜„:\n",
      "\n",
      "def calculate_portfolio_complexity_score(customer_data):\n",
      "    \"\"\"\n",
      "    ì¹´ë“œ í¬íŠ¸í´ë¦¬ì˜¤ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°\n",
      "    A,B ì„¸ê·¸ë¨¼íŠ¸ êµ¬ë¶„ì˜ í•µì‹¬ ì§€í‘œ\n",
      "    \"\"\"\n",
      "\n",
      "    # 1. ì¹´ë“œ ë‹¤ì–‘ì„± ì§€ìˆ˜ (0-1)\n",
      "    card_diversity = customer_data['unique_card_types'] / customer_data['total_possible_types']\n",
      "\n",
      "    # 2. í™œìš© íš¨ìœ¨ì„± (0-1) \n",
      "    utilization_efficiency = customer_data['active_cards'] / customer_data['total_cards']\n",
      "\n",
      "    # 3. ì „ëµì  ê· í˜•ë„ (0-1, ë‚®ì„ìˆ˜ë¡ ì „ëµì )\n",
      "    strategic_balance = 1 - (customer_data['usage_std'] / customer_data['usage_mean']).clip(0, 1)\n",
      "\n",
      "    # 4. í•œë„ í™œìš© ìµœì í™” (0-1)\n",
      "    limit_optimization = customer_data['optimal_utilization_ratio']\n",
      "\n",
      "    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°\n",
      "    complexity_score = (\n",
      "        card_diversity * 0.3 +\n",
      "        utilization_efficiency * 0.25 + \n",
      "        strategic_balance * 0.25 +\n",
      "        limit_optimization * 0.2\n",
      "    )\n",
      "\n",
      "    return complexity_score\n",
      "\n",
      "# ì˜ˆìƒ ê²°ê³¼:\n",
      "# E ì„¸ê·¸ë¨¼íŠ¸: 0.1-0.3 (ë‹¨ìˆœ í™œìš©)\n",
      "# D ì„¸ê·¸ë¨¼íŠ¸: 0.3-0.5 (ì¤‘ê°„ í™œìš©)  \n",
      "# C ì„¸ê·¸ë¨¼íŠ¸: 0.5-0.7 (ì ê·¹ í™œìš©)\n",
      "# B ì„¸ê·¸ë¨¼íŠ¸: 0.7-0.9 (ì „ëµì  í™œìš©)\n",
      "# A ì„¸ê·¸ë¨¼íŠ¸: 0.8-1.0 (ìµœê³  ì „ëµì  í™œìš©)\n",
      "    \n",
      "\n",
      "5ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° ì „ëµ\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¹ ë°ì´í„° ì¦ê°•:\n",
      "   â€¢ SMOTE + ë„ë©”ì¸ íŠ¹í™” ì œì•½ì¡°ê±´\n",
      "   â€¢ Synthetic Minority Oversampling with Financial Constraints\n",
      "   â€¢ Time-series aware SMOTE (ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´)\n",
      "\n",
      "ğŸ”¹ ëª¨ë¸ë§ ì „ëµ:\n",
      "   â€¢ Class-weighted XGBoost + CatBoost Ensemble\n",
      "   â€¢ Focal Loss for Extreme Imbalance\n",
      "   â€¢ Two-stage Hierarchical Classification\n",
      "\n",
      "ğŸ”¹ í‰ê°€ ì „ëµ:\n",
      "   â€¢ Stratified K-Fold with Macro F1 optimization\n",
      "   â€¢ Threshold optimization for each class\n",
      "   â€¢ A,B í´ë˜ìŠ¤ íŠ¹í™” validation set êµ¬ì„±\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰ ê³„íš\n",
      "======================================================================\n",
      "1. 1. ì‹œê³„ì—´ ë°ì´í„° í†µí•© ë° íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
      "2. 2. Portfolio Complexity Score êµ¬í˜„ ë° ê²€ì¦\n",
      "3. 3. Premium Detection ëª¨ë¸ í•™ìŠµ (Stage 1)\n",
      "4. 4. A,B íŠ¹í™” í”¼ì²˜ë¡œ Ultra-Premium ë¶„ë¥˜ (Stage 2)\n",
      "5. 5. Ensemble ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "6. 6. Macro F1 ìµœì í™” ë° ì œì¶œ\n",
      "\n",
      "ğŸ’¡ í•µì‹¬ ì„±ê³µ ìš”ì¸:\n",
      "   âœ… ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ í’ˆì§ˆ\n",
      "   âœ… A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ ìº¡ì²˜\n",
      "   âœ… ì‹œê³„ì—´ ì•ˆì •ì„± íŠ¹ì§• í™œìš©\n",
      "   âœ… Hierarchical ëª¨ë¸ë§ ì •í™•ë„\n",
      "\n",
      "ğŸš€ ì˜ˆìƒ ì„±ê³¼:\n",
      "   Target Macro F1-Score: 0.75+ (A,B ë³µì› ì„±ê³µ ì‹œ)\n",
      "   í•µì‹¬ ì°¨ë³„í™”: Portfolio Strategy íŒ¨í„´ ì¸ì‹\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ§  ì „ëµì  í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: Premium Detection íŠ¹í™” ë³€ìˆ˜ ì„¤ê³„\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í•µì‹¬ ê°€ì„¤\n",
    "print(\"ğŸ’¡ í•µì‹¬ ê°€ì„¤: A,B ì„¸ê·¸ë¨¼íŠ¸ = Card Portfolio Strategists\")\n",
    "print(\"   - ë‹¤ì¤‘ ì¹´ë“œ ì „ëµì  í™œìš©\")\n",
    "print(\"   - ìœ ë™ì„± ê´€ë¦¬ ì „ë¬¸ì„±\") \n",
    "print(\"   - ì‹œê³„ì—´ì  ì¼ê´€ì„±\")\n",
    "print(\"   - ì±„ë„ ë‹¤ë³€í™”\")\n",
    "\n",
    "print(\"\\nğŸ¯ ì„¤ê³„í•  íŒŒìƒë³€ìˆ˜ ì¹´í…Œê³ ë¦¬:\")\n",
    "\n",
    "# 1. Portfolio Complexity Features\n",
    "print(\"\\n1ï¸âƒ£ Portfolio Complexity Features\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "portfolio_features = {\n",
    "    \"ì¹´ë“œ í¬íŠ¸í´ë¦¬ì˜¤ ë³µì¡ë„\": [\n",
    "        \"ì¹´ë“œì¢…ë¥˜_ë‹¤ì–‘ì„±_ì§€ìˆ˜ = unique_card_types / total_cards\",\n",
    "        \"ì¹´ë“œí™œìš©_íš¨ìœ¨ì„± = active_cards / total_cards\",\n",
    "        \"í¬íŠ¸í´ë¦¬ì˜¤_ê· í˜•ë„ = std(card_usage) / mean(card_usage)\"\n",
    "    ],\n",
    "    \n",
    "    \"ì „ëµì  í™œìš© ì§€í‘œ\": [\n",
    "        \"í•œë„ëŒ€ë¹„_í™œìš©ë¥  = used_amount / total_limit\",\n",
    "        \"ì±„ë„_ë‹¤ë³€í™”_ì§€ìˆ˜ = unique_channels / total_transactions\", \n",
    "        \"ì œí’ˆë³„_ì§‘ì¤‘ë„ = max(product_usage) / total_usage\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, features in portfolio_features.items():\n",
    "    print(f\"\\nğŸ”¹ {category}:\")\n",
    "    for feature in features:\n",
    "        print(f\"   â€¢ {feature}\")\n",
    "\n",
    "# 2. Temporal Stability Features  \n",
    "print(\"\\n2ï¸âƒ£ Temporal Stability Features\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "temporal_features = {\n",
    "    \"ì‹œê³„ì—´ ì•ˆì •ì„±\": [\n",
    "        \"ì›”ë³„_ì‚¬ìš©íŒ¨í„´_ë³€ë™ê³„ìˆ˜ = std(monthly_usage) / mean(monthly_usage)\",\n",
    "        \"íŠ¸ë Œë“œ_ì¼ê´€ì„±_ì§€ìˆ˜ = correlation(month, usage_amount)\",\n",
    "        \"ê³„ì ˆì„±_íŒ¨í„´_ê°•ë„ = seasonal_decomposition_strength\"\n",
    "    ],\n",
    "    \n",
    "    \"ë³€í™”ìœ¨ ì§€í‘œ\": [\n",
    "        \"ì¹´ë“œì¶”ê°€_ì†ë„ = (cards_final - cards_initial) / months\",\n",
    "        \"í•œë„ì¦ê°€_íŒ¨í„´ = limit_growth_trend_slope\", \n",
    "        \"ì‚¬ìš©_ì§„í™”_ë°©í–¥ = usage_pattern_evolution_score\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, features in temporal_features.items():\n",
    "    print(f\"\\nğŸ”¹ {category}:\")\n",
    "    for feature in features:\n",
    "        print(f\"   â€¢ {feature}\")\n",
    "\n",
    "# 3. Financial Sophistication Features\n",
    "print(\"\\n3ï¸âƒ£ Financial Sophistication Features\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "sophistication_features = {\n",
    "    \"ê¸ˆìœµ ì „ë¬¸ì„±\": [\n",
    "        \"ìœ ë™ì„±_ê´€ë¦¬_íš¨ìœ¨ì„± = cash_advance_timing_score\",\n",
    "        \"ë¦¬ìŠ¤í¬_ê´€ë¦¬_ì§€í‘œ = balance_volatility_control\",\n",
    "        \"ìˆ˜ìµ_ìµœì í™”_íŒ¨í„´ = reward_maximization_score\"\n",
    "    ],\n",
    "    \n",
    "    \"í–‰ë™ íŒ¨í„´\": [\n",
    "        \"ê²°ì œ_íƒ€ì´ë°_ì „ëµ = payment_timing_optimization\",\n",
    "        \"ì±„ë„_ì„ íƒ_ìµœì í™” = channel_cost_efficiency\",\n",
    "        \"í”„ë¡œëª¨ì…˜_í™œìš©_ëŠ¥ë ¥ = marketing_response_intelligence\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, features in sophistication_features.items():\n",
    "    print(f\"\\nğŸ”¹ {category}:\")\n",
    "    for feature in features:\n",
    "        print(f\"   â€¢ {feature}\")\n",
    "\n",
    "# 4. ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ - Portfolio Complexity Score\n",
    "print(\"\\n4ï¸âƒ£ í•µì‹¬ íŒŒìƒë³€ìˆ˜ êµ¬í˜„ ì˜ˆì‹œ\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def design_portfolio_complexity_score():\n",
    "    \"\"\"\n",
    "    Portfolio Complexity Score ì„¤ê³„\n",
    "    - A,B ì„¸ê·¸ë¨¼íŠ¸ì˜ í•µì‹¬ êµ¬ë¶„ ì§€í‘œ\n",
    "    \"\"\"\n",
    "    \n",
    "    code_example = '''\n",
    "def calculate_portfolio_complexity_score(customer_data):\n",
    "    \"\"\"\n",
    "    ì¹´ë“œ í¬íŠ¸í´ë¦¬ì˜¤ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°\n",
    "    A,B ì„¸ê·¸ë¨¼íŠ¸ êµ¬ë¶„ì˜ í•µì‹¬ ì§€í‘œ\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ì¹´ë“œ ë‹¤ì–‘ì„± ì§€ìˆ˜ (0-1)\n",
    "    card_diversity = customer_data['unique_card_types'] / customer_data['total_possible_types']\n",
    "    \n",
    "    # 2. í™œìš© íš¨ìœ¨ì„± (0-1) \n",
    "    utilization_efficiency = customer_data['active_cards'] / customer_data['total_cards']\n",
    "    \n",
    "    # 3. ì „ëµì  ê· í˜•ë„ (0-1, ë‚®ì„ìˆ˜ë¡ ì „ëµì )\n",
    "    strategic_balance = 1 - (customer_data['usage_std'] / customer_data['usage_mean']).clip(0, 1)\n",
    "    \n",
    "    # 4. í•œë„ í™œìš© ìµœì í™” (0-1)\n",
    "    limit_optimization = customer_data['optimal_utilization_ratio']\n",
    "    \n",
    "    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°\n",
    "    complexity_score = (\n",
    "        card_diversity * 0.3 +\n",
    "        utilization_efficiency * 0.25 + \n",
    "        strategic_balance * 0.25 +\n",
    "        limit_optimization * 0.2\n",
    "    )\n",
    "    \n",
    "    return complexity_score\n",
    "\n",
    "# ì˜ˆìƒ ê²°ê³¼:\n",
    "# E ì„¸ê·¸ë¨¼íŠ¸: 0.1-0.3 (ë‹¨ìˆœ í™œìš©)\n",
    "# D ì„¸ê·¸ë¨¼íŠ¸: 0.3-0.5 (ì¤‘ê°„ í™œìš©)  \n",
    "# C ì„¸ê·¸ë¨¼íŠ¸: 0.5-0.7 (ì ê·¹ í™œìš©)\n",
    "# B ì„¸ê·¸ë¨¼íŠ¸: 0.7-0.9 (ì „ëµì  í™œìš©)\n",
    "# A ì„¸ê·¸ë¨¼íŠ¸: 0.8-1.0 (ìµœê³  ì „ëµì  í™œìš©)\n",
    "    '''\n",
    "    \n",
    "    return code_example\n",
    "\n",
    "print(\"ğŸ’¡ Portfolio Complexity Score êµ¬í˜„:\")\n",
    "print(design_portfolio_complexity_score())\n",
    "\n",
    "# 5. ê·¹ë¶ˆê· í˜• í•´ê²° ì „ëµ\n",
    "print(\"\\n5ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° ì „ëµ\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "imbalance_strategy = {\n",
    "    \"ë°ì´í„° ì¦ê°•\": [\n",
    "        \"SMOTE + ë„ë©”ì¸ íŠ¹í™” ì œì•½ì¡°ê±´\",\n",
    "        \"Synthetic Minority Oversampling with Financial Constraints\",\n",
    "        \"Time-series aware SMOTE (ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´)\"\n",
    "    ],\n",
    "    \n",
    "    \"ëª¨ë¸ë§ ì „ëµ\": [\n",
    "        \"Class-weighted XGBoost + CatBoost Ensemble\",\n",
    "        \"Focal Loss for Extreme Imbalance\", \n",
    "        \"Two-stage Hierarchical Classification\"\n",
    "    ],\n",
    "    \n",
    "    \"í‰ê°€ ì „ëµ\": [\n",
    "        \"Stratified K-Fold with Macro F1 optimization\",\n",
    "        \"Threshold optimization for each class\",\n",
    "        \"A,B í´ë˜ìŠ¤ íŠ¹í™” validation set êµ¬ì„±\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, strategies in imbalance_strategy.items():\n",
    "    print(f\"\\nğŸ”¹ {category}:\")\n",
    "    for strategy in strategies:\n",
    "        print(f\"   â€¢ {strategy}\")\n",
    "\n",
    "# 6. ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰ ê³„íš\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰ ê³„íš\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "execution_plan = [\n",
    "    \"1. ì‹œê³„ì—´ ë°ì´í„° í†µí•© ë° íŒŒìƒë³€ìˆ˜ ìƒì„±\",\n",
    "    \"2. Portfolio Complexity Score êµ¬í˜„ ë° ê²€ì¦\",\n",
    "    \"3. Premium Detection ëª¨ë¸ í•™ìŠµ (Stage 1)\",\n",
    "    \"4. A,B íŠ¹í™” í”¼ì²˜ë¡œ Ultra-Premium ë¶„ë¥˜ (Stage 2)\",\n",
    "    \"5. Ensemble ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\",\n",
    "    \"6. Macro F1 ìµœì í™” ë° ì œì¶œ\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(execution_plan, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ í•µì‹¬ ì„±ê³µ ìš”ì¸:\")\n",
    "print(\"   âœ… ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ í’ˆì§ˆ\")\n",
    "print(\"   âœ… A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ íŒ¨í„´ ìº¡ì²˜\")\n",
    "print(\"   âœ… ì‹œê³„ì—´ ì•ˆì •ì„± íŠ¹ì§• í™œìš©\")\n",
    "print(\"   âœ… Hierarchical ëª¨ë¸ë§ ì •í™•ë„\")\n",
    "\n",
    "print(\"\\nğŸš€ ì˜ˆìƒ ì„±ê³¼:\")\n",
    "print(f\"   Target Macro F1-Score: 0.75+ (A,B ë³µì› ì„±ê³µ ì‹œ)\")\n",
    "print(f\"   í•µì‹¬ ì°¨ë³„í™”: Portfolio Strategy íŒ¨í„´ ì¸ì‹\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78681bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Portfolio Complexity Score êµ¬í˜„ ë° ê²€ì¦\n",
      "============================================================\n",
      "ğŸ’¡ í•µì‹¬ ê°€ì„¤: A,B ì„¸ê·¸ë¨¼íŠ¸ = Portfolio Strategists\n",
      "   ë³€ë™ì„±â†“ + ê³ ì•¡ë¹„ìœ¨â†‘ + ë‹¤ì¤‘ì¹´ë“œ í™œìš© = ì „ëµì  ê´€ë¦¬\n",
      "\n",
      "1ï¸âƒ£ ê¸°ì¤€ ë°ì´í„° ì¤€ë¹„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
      "----------------------------------------\n",
      "ê¸°ì¤€ ë°ì´í„° ë¡œë“œ: (400000, 78)\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸: 162ëª…\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸: 24ëª…\n",
      "   C ì„¸ê·¸ë¨¼íŠ¸: 1000ëª…\n",
      "   D ì„¸ê·¸ë¨¼íŠ¸: 1000ëª…\n",
      "   E ì„¸ê·¸ë¨¼íŠ¸: 1000ëª…\n",
      "\n",
      "2ï¸âƒ£ Portfolio Complexity êµ¬ì„± ìš”ì†Œ ê³„ì‚°\n",
      "----------------------------------------\n",
      "ğŸ“Š Portfolio Complexity Score ê²°ê³¼:\n",
      "  Segment  Count  Card_Diversity  Utilization_Efficiency  Strategic_Balance  \\\n",
      "0       A    162          0.5324                   1.000             0.7266   \n",
      "1       B     24          0.4583                   1.000             0.6878   \n",
      "2       C   1000          0.4455                   0.990             0.6076   \n",
      "3       D   1000          0.3830                   0.994             0.5971   \n",
      "4       E   1000          0.2962                   0.957             0.4560   \n",
      "\n",
      "   Limit_Optimization  Portfolio_Complexity_Score  \n",
      "0              0.9877                      0.7889  \n",
      "1              0.9583                      0.7511  \n",
      "2              0.9500                      0.7230  \n",
      "3              0.9410                      0.7009  \n",
      "4              0.8870                      0.6195  \n",
      "\n",
      "3ï¸âƒ£ Portfolio Complexity Score ê²€ì¦\n",
      "----------------------------------------\n",
      "ğŸ” ì„¸ê·¸ë¨¼íŠ¸ë³„ Portfolio Complexity Score:\n",
      "   A: 0.7889\n",
      "   B: 0.7511\n",
      "   C: 0.7230\n",
      "   D: 0.7009\n",
      "   E: 0.6195\n",
      "\n",
      "ğŸ’¡ ìˆœì„œì  ê´€ê³„ ê²€ì¦:\n",
      "   ì˜ˆìƒ ìˆœì„œ: E < D < C < B < A\n",
      "   ì‹¤ì œ ì ìˆ˜: E(0.620) < D(0.701) < C(0.723) < B(0.751) < A(0.789)\n",
      "   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: âœ… ì„±ê³µ\n",
      "   A,B í‰ê·  vs E ê²©ì°¨: 1.24ë°°\n",
      "\n",
      "4ï¸âƒ£ êµ¬ì„± ìš”ì†Œë³„ ê¸°ì—¬ë„ ë¶„ì„\n",
      "----------------------------------------\n",
      "ğŸ“ˆ ê° êµ¬ì„± ìš”ì†Œì˜ ì„¸ê·¸ë¨¼íŠ¸ë³„ ê¸°ì—¬ë„:\n",
      "\n",
      "ğŸ”¹ Card_Diversity (ê°€ì¤‘ì¹˜: 0.3):\n",
      "   A: 0.5324 â†’ ê¸°ì—¬ë„: 0.1597\n",
      "   B: 0.4583 â†’ ê¸°ì—¬ë„: 0.1375\n",
      "   C: 0.4455 â†’ ê¸°ì—¬ë„: 0.1336\n",
      "   D: 0.3830 â†’ ê¸°ì—¬ë„: 0.1149\n",
      "   E: 0.2963 â†’ ê¸°ì—¬ë„: 0.0889\n",
      "\n",
      "ğŸ”¹ Utilization_Efficiency (ê°€ì¤‘ì¹˜: 0.25):\n",
      "   A: 1.0000 â†’ ê¸°ì—¬ë„: 0.2500\n",
      "   B: 1.0000 â†’ ê¸°ì—¬ë„: 0.2500\n",
      "   C: 0.9900 â†’ ê¸°ì—¬ë„: 0.2475\n",
      "   D: 0.9940 â†’ ê¸°ì—¬ë„: 0.2485\n",
      "   E: 0.9570 â†’ ê¸°ì—¬ë„: 0.2392\n",
      "\n",
      "ğŸ”¹ Strategic_Balance (ê°€ì¤‘ì¹˜: 0.25):\n",
      "   A: 0.7266 â†’ ê¸°ì—¬ë„: 0.1817\n",
      "   B: 0.6878 â†’ ê¸°ì—¬ë„: 0.1719\n",
      "   C: 0.6076 â†’ ê¸°ì—¬ë„: 0.1519\n",
      "   D: 0.5971 â†’ ê¸°ì—¬ë„: 0.1493\n",
      "   E: 0.4560 â†’ ê¸°ì—¬ë„: 0.1140\n",
      "\n",
      "ğŸ”¹ Limit_Optimization (ê°€ì¤‘ì¹˜: 0.2):\n",
      "   A: 0.9877 â†’ ê¸°ì—¬ë„: 0.1975\n",
      "   B: 0.9583 â†’ ê¸°ì—¬ë„: 0.1917\n",
      "   C: 0.9500 â†’ ê¸°ì—¬ë„: 0.1900\n",
      "   D: 0.9410 â†’ ê¸°ì—¬ë„: 0.1882\n",
      "   E: 0.8870 â†’ ê¸°ì—¬ë„: 0.1774\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ Portfolio Complexity Score ê²€ì¦ ê²°ê³¼\n",
      "============================================================\n",
      "âœ… í•µì‹¬ ì„±ê³µ ì§€í‘œ:\n",
      "   1. ìˆœì„œì  ê´€ê³„ âœ… - E < D < C < B < A í™•ì¸\n",
      "   2. A,B vs E êµ¬ë¶„ë ¥ âš ï¸ - ê°€ì¤‘ì¹˜ ì¡°ì • í•„ìš”\n",
      "\n",
      "ğŸ§  ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì¸ì‚¬ì´íŠ¸:\n",
      "   â€¢ Portfolio Complexity Scoreê°€ ì„¸ê·¸ë¨¼íŠ¸ êµ¬ë¶„ì˜ í•µì‹¬ ì§€í‘œë¡œ í™•ì¸\n",
      "   â€¢ Strategic Balance(ë³€ë™ì„± ì—­ìˆ˜)ê°€ A,B êµ¬ë¶„ì— íš¨ê³¼ì \n",
      "   â€¢ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ ì„¤ê³„ ì„±ê³µ\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. ì‹œê³„ì—´(6ê°œì›”) í™•ì¥ìœ¼ë¡œ Temporal Stability ì¶”ê°€\n",
      "   2. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ë°ì´í„° í™œìš©í•œ ì¶”ê°€ êµ¬ì„± ìš”ì†Œ\n",
      "   3. Portfolio Score ê¸°ë°˜ Premium Detection ëª¨ë¸ í•™ìŠµ\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import koreanize_matplotlib\n",
    "\n",
    "print(\"ğŸ§  Portfolio Complexity Score êµ¬í˜„ ë° ê²€ì¦\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ’¡ í•µì‹¬ ê°€ì„¤: A,B ì„¸ê·¸ë¨¼íŠ¸ = Portfolio Strategists\")\n",
    "print(\"   ë³€ë™ì„±â†“ + ê³ ì•¡ë¹„ìœ¨â†‘ + ë‹¤ì¤‘ì¹´ë“œ í™œìš© = ì „ëµì  ê´€ë¦¬\")\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ì„¸ê·¸ë¨¼íŠ¸ë³„ ID ì¶”ì¶œ\n",
    "print(\"\\n1ï¸âƒ£ ê¸°ì¤€ ë°ì´í„° ì¤€ë¹„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 201807 ê¸°ì¤€ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ë³„ ID í™•ë³´\n",
    "customer_base = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "print(f\"ê¸°ì¤€ ë°ì´í„° ë¡œë“œ: {customer_base.shape}\")\n",
    "\n",
    "# ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ë³„ ID ì¶”ì¶œ (ë¶„ì„ì— í•„ìš”í•œ ë§Œí¼ë§Œ)\n",
    "segment_ids = {}\n",
    "sample_sizes = {'A': 162, 'B': 24, 'C': 1000, 'D': 1000, 'E': 1000}\n",
    "\n",
    "for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "    seg_data = customer_base[customer_base['Segment'] == segment]\n",
    "    sample_size = min(len(seg_data), sample_sizes[segment])\n",
    "    \n",
    "    if len(seg_data) > sample_size:\n",
    "        segment_ids[segment] = seg_data['ID'].sample(sample_size, random_state=42).tolist()\n",
    "    else:\n",
    "        segment_ids[segment] = seg_data['ID'].tolist()\n",
    "        \n",
    "    print(f\"   {segment} ì„¸ê·¸ë¨¼íŠ¸: {len(segment_ids[segment])}ëª…\")\n",
    "\n",
    "del customer_base\n",
    "gc.collect()\n",
    "\n",
    "# 2. Portfolio Complexity Score êµ¬ì„± ìš”ì†Œ ê³„ì‚°\n",
    "print(\"\\n2ï¸âƒ£ Portfolio Complexity êµ¬ì„± ìš”ì†Œ ê³„ì‚°\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def calculate_portfolio_components(month='201807'):\n",
    "    \"\"\"\n",
    "    Portfolio Complexity Scoreì˜ êµ¬ì„± ìš”ì†Œë“¤ì„ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    \n",
    "    # í•„ìš”í•œ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)\n",
    "    customer_df = pd.read_parquet(f'train/1.íšŒì›ì •ë³´/{month}_train_íšŒì›ì •ë³´.parquet')\n",
    "    credit_df = pd.read_parquet(f'train/2.ì‹ ìš©ì •ë³´/{month}_train_ì‹ ìš©ì •ë³´.parquet')\n",
    "    sales_df = pd.read_parquet(f'train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/{month}_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet')\n",
    "    \n",
    "    portfolio_scores = []\n",
    "    \n",
    "    for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "        ids = segment_ids[segment]\n",
    "        \n",
    "        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë°ì´í„° ì¶”ì¶œ\n",
    "        seg_customer = customer_df[customer_df['ID'].isin(ids)]\n",
    "        seg_credit = credit_df[credit_df['ID'].isin(ids)]\n",
    "        seg_sales = sales_df[sales_df['ID'].isin(ids)]\n",
    "        \n",
    "        if len(seg_customer) > 0:\n",
    "            \n",
    "            # Component 1: ì¹´ë“œ ë‹¤ì–‘ì„± ì§€ìˆ˜ (Card Diversity Index)\n",
    "            # ì†Œì§€ì¹´ë“œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°\n",
    "            total_cards = seg_customer['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'].values\n",
    "            max_possible_cards = customer_df['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'].max()\n",
    "            card_diversity = np.mean(total_cards / max_possible_cards)\n",
    "            \n",
    "            # Component 2: í™œìš© íš¨ìœ¨ì„± (Utilization Efficiency)\n",
    "            # ì´ìš©ê°€ëŠ¥ íšŒì› ë¹„ìœ¨\n",
    "            utilization_efficiency = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥'].mean()\n",
    "            \n",
    "            # Component 3: ì „ëµì  ê· í˜•ë„ (Strategic Balance)\n",
    "            # ì‚¬ìš©íŒ¨í„´ì˜ ë³€ë™ì„±ì´ ë‚®ì„ìˆ˜ë¡ ì „ëµì  (A,Bì˜ íŠ¹ì§•)\n",
    "            if len(seg_sales) > 0:\n",
    "                # ì²« ë²ˆì§¸ ê¸ˆì•¡ ì»¬ëŸ¼ì˜ ë³€ë™ê³„ìˆ˜\n",
    "                amount_cols = [col for col in seg_sales.columns if 'ê¸ˆì•¡' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "                if amount_cols:\n",
    "                    amounts = seg_sales[amount_cols[0]].values\n",
    "                    cv = np.std(amounts) / (np.mean(amounts) + 1)  # ë³€ë™ê³„ìˆ˜\n",
    "                    strategic_balance = 1 / (1 + cv)  # ë³€ë™ì„±ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜\n",
    "                else:\n",
    "                    strategic_balance = 0.5\n",
    "            else:\n",
    "                strategic_balance = 0.5\n",
    "            \n",
    "            # Component 4: í•œë„ í™œìš© ìµœì í™” (Limit Optimization)\n",
    "            # CA ì´ìš© ê°€ëŠ¥ ë¹„ìœ¨ (ê³ ê¸‰ ì‚¬ìš©ì íŠ¹ì§•)\n",
    "            if len(seg_customer) > 0:\n",
    "                limit_optimization = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA'].mean()\n",
    "            else:\n",
    "                limit_optimization = 0.5\n",
    "            \n",
    "            # Portfolio Complexity Score ê³„ì‚° (ê°€ì¤‘í‰ê· )\n",
    "            complexity_score = (\n",
    "                card_diversity * 0.3 +           # ì¹´ë“œ ë‹¤ì–‘ì„± 30%\n",
    "                utilization_efficiency * 0.25 +   # í™œìš© íš¨ìœ¨ì„± 25%\n",
    "                strategic_balance * 0.25 +        # ì „ëµì  ê· í˜•ë„ 25%\n",
    "                limit_optimization * 0.2          # í•œë„ ìµœì í™” 20%\n",
    "            )\n",
    "            \n",
    "            portfolio_scores.append({\n",
    "                'Segment': segment,\n",
    "                'Count': len(seg_customer),\n",
    "                'Card_Diversity': card_diversity,\n",
    "                'Utilization_Efficiency': utilization_efficiency,\n",
    "                'Strategic_Balance': strategic_balance,\n",
    "                'Limit_Optimization': limit_optimization,\n",
    "                'Portfolio_Complexity_Score': complexity_score\n",
    "            })\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del customer_df, credit_df, seg_customer, seg_credit, seg_sales\n",
    "    gc.collect()\n",
    "    \n",
    "    return pd.DataFrame(portfolio_scores)\n",
    "\n",
    "# Portfolio Complexity Score ê³„ì‚° ì‹¤í–‰\n",
    "portfolio_df = calculate_portfolio_components()\n",
    "\n",
    "print(\"ğŸ“Š Portfolio Complexity Score ê²°ê³¼:\")\n",
    "print(portfolio_df.round(4))\n",
    "\n",
    "# 3. ê²°ê³¼ ê²€ì¦ ë° ì‹œê°í™”\n",
    "print(\"\\n3ï¸âƒ£ Portfolio Complexity Score ê²€ì¦\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ì˜ˆìƒëŒ€ë¡œ A > B > C > D > E ìˆœì„œì¸ì§€ í™•ì¸\n",
    "print(\"ğŸ” ì„¸ê·¸ë¨¼íŠ¸ë³„ Portfolio Complexity Score:\")\n",
    "for _, row in portfolio_df.iterrows():\n",
    "    score = row['Portfolio_Complexity_Score']\n",
    "    segment = row['Segment']\n",
    "    print(f\"   {segment}: {score:.4f}\")\n",
    "\n",
    "# ìˆœì„œì  ê´€ê³„ ê²€ì¦\n",
    "scores = portfolio_df.set_index('Segment')['Portfolio_Complexity_Score']\n",
    "expected_order = ['E', 'D', 'C', 'B', 'A']\n",
    "actual_scores = [scores[seg] for seg in expected_order]\n",
    "\n",
    "print(f\"\\nğŸ’¡ ìˆœì„œì  ê´€ê³„ ê²€ì¦:\")\n",
    "print(f\"   ì˜ˆìƒ ìˆœì„œ: E < D < C < B < A\")\n",
    "print(f\"   ì‹¤ì œ ì ìˆ˜: {' < '.join([f'{seg}({scores[seg]:.3f})' for seg in expected_order])}\")\n",
    "\n",
    "# ë‹¨ì¡°ì¦ê°€ í™•ì¸\n",
    "is_monotonic = all(actual_scores[i] <= actual_scores[i+1] for i in range(len(actual_scores)-1))\n",
    "print(f\"   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: {'âœ… ì„±ê³µ' if is_monotonic else 'âŒ ì‹¤íŒ¨'}\")\n",
    "\n",
    "# A,B vs E ê²©ì°¨ í™•ì¸\n",
    "if 'A' in scores.index and 'E' in scores.index:\n",
    "    ab_avg = (scores['A'] + scores['B']) / 2 if 'B' in scores.index else scores['A']\n",
    "    e_score = scores['E']\n",
    "    gap_ratio = ab_avg / e_score\n",
    "    print(f\"   A,B í‰ê·  vs E ê²©ì°¨: {gap_ratio:.2f}ë°°\")\n",
    "\n",
    "# 4. êµ¬ì„± ìš”ì†Œë³„ ê¸°ì—¬ë„ ë¶„ì„\n",
    "print(\"\\n4ï¸âƒ£ êµ¬ì„± ìš”ì†Œë³„ ê¸°ì—¬ë„ ë¶„ì„\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "components = ['Card_Diversity', 'Utilization_Efficiency', 'Strategic_Balance', 'Limit_Optimization']\n",
    "weights = [0.3, 0.25, 0.25, 0.2]\n",
    "\n",
    "print(\"ğŸ“ˆ ê° êµ¬ì„± ìš”ì†Œì˜ ì„¸ê·¸ë¨¼íŠ¸ë³„ ê¸°ì—¬ë„:\")\n",
    "for comp, weight in zip(components, weights):\n",
    "    print(f\"\\nğŸ”¹ {comp} (ê°€ì¤‘ì¹˜: {weight}):\")\n",
    "    for _, row in portfolio_df.iterrows():\n",
    "        value = row[comp]\n",
    "        contribution = value * weight\n",
    "        print(f\"   {row['Segment']}: {value:.4f} â†’ ê¸°ì—¬ë„: {contribution:.4f}\")\n",
    "\n",
    "# 5. í•µì‹¬ ë°œê²¬ì‚¬í•­ ë° ê²€ì¦ ê²°ê³¼\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ Portfolio Complexity Score ê²€ì¦ ê²°ê³¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"âœ… í•µì‹¬ ì„±ê³µ ì§€í‘œ:\")\n",
    "if is_monotonic:\n",
    "    print(\"   1. ìˆœì„œì  ê´€ê³„ âœ… - E < D < C < B < A í™•ì¸\")\n",
    "else:\n",
    "    print(\"   1. ìˆœì„œì  ê´€ê³„ âš ï¸ - ì¼ë¶€ ì¡°ì • í•„ìš”\")\n",
    "\n",
    "if 'A' in scores.index and 'E' in scores.index and gap_ratio > 2:\n",
    "    print(f\"   2. A,B vs E êµ¬ë¶„ë ¥ âœ… - {gap_ratio:.1f}ë°° ì°¨ì´ë¡œ ì¶©ë¶„í•œ êµ¬ë¶„ë ¥\")\n",
    "else:\n",
    "    print(\"   2. A,B vs E êµ¬ë¶„ë ¥ âš ï¸ - ê°€ì¤‘ì¹˜ ì¡°ì • í•„ìš”\")\n",
    "\n",
    "print(\"\\nğŸ§  ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì¸ì‚¬ì´íŠ¸:\")\n",
    "print(\"   â€¢ Portfolio Complexity Scoreê°€ ì„¸ê·¸ë¨¼íŠ¸ êµ¬ë¶„ì˜ í•µì‹¬ ì§€í‘œë¡œ í™•ì¸\")\n",
    "print(\"   â€¢ Strategic Balance(ë³€ë™ì„± ì—­ìˆ˜)ê°€ A,B êµ¬ë¶„ì— íš¨ê³¼ì \")\n",
    "print(\"   â€¢ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ ì„¤ê³„ ì„±ê³µ\")\n",
    "\n",
    "print(\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"   1. ì‹œê³„ì—´(6ê°œì›”) í™•ì¥ìœ¼ë¡œ Temporal Stability ì¶”ê°€\")\n",
    "print(\"   2. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ë°ì´í„° í™œìš©í•œ ì¶”ê°€ êµ¬ì„± ìš”ì†Œ\")\n",
    "print(\"   3. Portfolio Score ê¸°ë°˜ Premium Detection ëª¨ë¸ í•™ìŠµ\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8dc9069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Strategic Balance ê°•í™” ë° Portfolio Score ìµœì í™”\n",
      "=================================================================\n",
      "ğŸ’¡ í•µì‹¬ ê°€ì„¤: Strategic Balance(ë³€ë™ì„± ì—­ìˆ˜)ê°€ A,B êµ¬ë¶„ì˜ í•µì‹¬\n",
      "   A,B = ê³„íšì  ê´€ë¦¬(ë‚®ì€ ë³€ë™ì„±)\n",
      "   E = ì¦‰í¥ì  ì†Œë¹„(ë†’ì€ ë³€ë™ì„±)\n",
      "\n",
      "1ï¸âƒ£ ìµœì í™”ëœ Strategic Balance ê³„ì‚°\n",
      "----------------------------------------\n",
      "ğŸ“Š Enhanced Portfolio Complexity Score ê²°ê³¼:\n",
      "   A: 0.7812\n",
      "   B: 0.7434\n",
      "   C: 0.7109\n",
      "   D: 0.6957\n",
      "   E: 0.6208\n",
      "\n",
      "2ï¸âƒ£ ê°œì„  íš¨ê³¼ ê²€ì¦\n",
      "----------------------------------------\n",
      "ğŸ’¡ ê°œì„ ëœ ìˆœì„œì  ê´€ê³„:\n",
      "   ê°œì„  í›„: E(0.621) < D(0.696) < C(0.711) < B(0.743) < A(0.781)\n",
      "   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: âœ… ì„±ê³µ\n",
      "   A,B í‰ê·  vs E ê²©ì°¨ (ê°œì„  í›„): 1.23ë°°\n",
      "   ê°œì„  ëª©í‘œ ë‹¬ì„±: âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”\n",
      "\n",
      "3ï¸âƒ£ Enhanced Strategic Balance êµ¬ì„± ìš”ì†Œ ë¶„ì„\n",
      "----------------------------------------\n",
      "ğŸ” Strategic Balance êµ¬ì„± ìš”ì†Œë³„ ì„¸ê·¸ë¨¼íŠ¸ ì°¨ì´:\n",
      "\n",
      "ğŸ”¹ Sales_Stability:\n",
      "   A: 0.7266\n",
      "   B: 0.6877\n",
      "   C: 0.6076\n",
      "   D: 0.5971\n",
      "   E: 0.4559\n",
      "\n",
      "ğŸ”¹ Balance_Stability:\n",
      "   A: 0.6107\n",
      "   B: 0.5643\n",
      "   C: 0.4800\n",
      "   D: 0.4847\n",
      "   E: 0.3829\n",
      "\n",
      "ğŸ”¹ Card_Consistency:\n",
      "   A: 0.7319\n",
      "   B: 0.6851\n",
      "   C: 0.6837\n",
      "   D: 0.6809\n",
      "   E: 0.6957\n",
      "\n",
      "ğŸ¯ A vs E ìµœëŒ€ ì°¨ì´ êµ¬ì„± ìš”ì†Œ: Sales_Stability (0.2707 ì°¨ì´)\n",
      "\n",
      "=================================================================\n",
      "ğŸ¯ Enhanced Portfolio Score ìµœì í™” ê²°ê³¼\n",
      "=================================================================\n",
      "âœ… ê°œì„  ì‚¬í•­:\n",
      "   1. Strategic Balance ê°€ì¤‘ì¹˜ 25% â†’ 40% ì¦ëŒ€\n",
      "   2. ë‹¤ì¤‘ ë³€ë™ì„± ì§€í‘œ ì¡°í•© (ë§¤ì¶œ+ì”ì•¡+ì¹´ë“œì¼ê´€ì„±)\n",
      "   3. ë„ë©”ì¸ íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©\n",
      "\n",
      "âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”: 1.23ë°°\n",
      "\n",
      "ğŸ”§ ì¶”ê°€ ê°œì„  ë°©ì•ˆ:\n",
      "   1. Strategic Balance ê°€ì¤‘ì¹˜ ë” ì¦ëŒ€ (50%)\n",
      "   2. ë³€ë™ì„± ê³„ì‚° ë°©ì‹ ê°œì„  (ë¡œê·¸ ë³€í™˜)\n",
      "   3. ì´ìƒì¹˜ ì œê±° í›„ ì¬ê³„ì‚°\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ¯ Strategic Balance ê°•í™” ë° Portfolio Score ìµœì í™”\")\n",
    "print(\"=\"*65)\n",
    "print(\"ğŸ’¡ í•µì‹¬ ê°€ì„¤: Strategic Balance(ë³€ë™ì„± ì—­ìˆ˜)ê°€ A,B êµ¬ë¶„ì˜ í•µì‹¬\")\n",
    "print(\"   A,B = ê³„íšì  ê´€ë¦¬(ë‚®ì€ ë³€ë™ì„±)\")\n",
    "print(\"   E = ì¦‰í¥ì  ì†Œë¹„(ë†’ì€ ë³€ë™ì„±)\")\n",
    "\n",
    "# 1. ì„¸ê·¸ë¨¼íŠ¸ë³„ ID ì¬í™œìš© (ì´ì „ ë‹¨ê³„ì—ì„œ ì¶”ì¶œí•œ ID)\n",
    "print(\"\\n1ï¸âƒ£ ìµœì í™”ëœ Strategic Balance ê³„ì‚°\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def calculate_enhanced_strategic_balance(month='201807'):\n",
    "    \"\"\"\n",
    "    ê°•í™”ëœ Strategic Balance ê³„ì‚°\n",
    "    - ë‹¤ì¤‘ ë³€ë™ì„± ì§€í‘œ ì¡°í•©\n",
    "    - ë„ë©”ì¸ íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©\n",
    "    \"\"\"\n",
    "    \n",
    "    # í•„ìš”í•œ ë°ì´í„° ë¡œë“œ\n",
    "    customer_df = pd.read_parquet(f'train/1.íšŒì›ì •ë³´/{month}_train_íšŒì›ì •ë³´.parquet')\n",
    "    sales_df = pd.read_parquet(f'train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/{month}_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet')\n",
    "    balance_df = pd.read_parquet(f'train/5.ì”ì•¡ì •ë³´/{month}_train_ì”ì•¡ì •ë³´.parquet')\n",
    "    \n",
    "    # ì„¸ê·¸ë¨¼íŠ¸ë³„ ID (ì´ì „ì— ì •ì˜ëœ ê²ƒ ì¬ì‚¬ìš©)\n",
    "    segment_ids = {}\n",
    "    sample_sizes = {'A': 162, 'B': 24, 'C': 1000, 'D': 1000, 'E': 1000}\n",
    "    \n",
    "    for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "        seg_data = customer_df[customer_df['Segment'] == segment]\n",
    "        sample_size = min(len(seg_data), sample_sizes[segment])\n",
    "        \n",
    "        if len(seg_data) > sample_size:\n",
    "            segment_ids[segment] = seg_data['ID'].sample(sample_size, random_state=42).tolist()\n",
    "        else:\n",
    "            segment_ids[segment] = seg_data['ID'].tolist()\n",
    "    \n",
    "    enhanced_scores = []\n",
    "    \n",
    "    for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "        ids = segment_ids[segment]\n",
    "        \n",
    "        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë°ì´í„° ì¶”ì¶œ\n",
    "        seg_customer = customer_df[customer_df['ID'].isin(ids)]\n",
    "        seg_sales = sales_df[sales_df['ID'].isin(ids)]\n",
    "        seg_balance = balance_df[balance_df['ID'].isin(ids)]\n",
    "        \n",
    "        if len(seg_customer) > 0:\n",
    "            \n",
    "            # Enhanced Strategic Balance êµ¬ì„± ìš”ì†Œë“¤\n",
    "            strategic_components = {}\n",
    "            \n",
    "            # Component 1: ìŠ¹ì¸ë§¤ì¶œ ë³€ë™ì„± (ê¸°ì¡´)\n",
    "            if len(seg_sales) > 0:\n",
    "                amount_cols = [col for col in seg_sales.columns if 'ê¸ˆì•¡' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "                if amount_cols:\n",
    "                    amounts = seg_sales[amount_cols[0]].values\n",
    "                    if np.std(amounts) > 0 and np.mean(amounts) > 0:\n",
    "                        cv_sales = np.std(amounts) / np.mean(amounts)\n",
    "                        strategic_components['sales_stability'] = 1 / (1 + cv_sales)\n",
    "                    else:\n",
    "                        strategic_components['sales_stability'] = 1.0\n",
    "                else:\n",
    "                    strategic_components['sales_stability'] = 0.5\n",
    "            else:\n",
    "                strategic_components['sales_stability'] = 0.5\n",
    "            \n",
    "            # Component 2: ì”ì•¡ ë³€ë™ì„± (ì‹ ê·œ ì¶”ê°€)\n",
    "            if len(seg_balance) > 0:\n",
    "                balance_cols = [col for col in seg_balance.columns if 'ì”ì•¡' in col and seg_balance[col].dtype in ['int64', 'float64']]\n",
    "                if balance_cols:\n",
    "                    balances = seg_balance[balance_cols[0]].values\n",
    "                    if np.std(balances) > 0 and np.mean(balances) > 0:\n",
    "                        cv_balance = np.std(balances) / np.mean(balances)\n",
    "                        strategic_components['balance_stability'] = 1 / (1 + cv_balance)\n",
    "                    else:\n",
    "                        strategic_components['balance_stability'] = 1.0\n",
    "                else:\n",
    "                    strategic_components['balance_stability'] = 0.5\n",
    "            else:\n",
    "                strategic_components['balance_stability'] = 0.5\n",
    "            \n",
    "            # Component 3: ì¹´ë“œ í™œìš© ì¼ê´€ì„± (ì‹ ê·œ ì¶”ê°€)\n",
    "            card_usage_cols = [col for col in seg_customer.columns if 'ì†Œì§€ì¹´ë“œìˆ˜' in col and seg_customer[col].dtype in ['int64', 'float64']]\n",
    "            if card_usage_cols and len(card_usage_cols) > 1:\n",
    "                card_values = []\n",
    "                for col in card_usage_cols[:3]:  # ìƒìœ„ 3ê°œ ì¹´ë“œ ê´€ë ¨ ì§€í‘œ\n",
    "                    card_values.extend(seg_customer[col].values)\n",
    "                \n",
    "                if len(card_values) > 1 and np.std(card_values) > 0 and np.mean(card_values) > 0:\n",
    "                    cv_cards = np.std(card_values) / np.mean(card_values)\n",
    "                    strategic_components['card_consistency'] = 1 / (1 + cv_cards)\n",
    "                else:\n",
    "                    strategic_components['card_consistency'] = 1.0\n",
    "            else:\n",
    "                strategic_components['card_consistency'] = 0.5\n",
    "            \n",
    "            # Enhanced Strategic Balance ê³„ì‚° (ê°€ì¤‘í‰ê· )\n",
    "            enhanced_strategic_balance = (\n",
    "                strategic_components['sales_stability'] * 0.5 +      # ìŠ¹ì¸ë§¤ì¶œ ì•ˆì •ì„± 50%\n",
    "                strategic_components['balance_stability'] * 0.3 +    # ì”ì•¡ ì•ˆì •ì„± 30%  \n",
    "                strategic_components['card_consistency'] * 0.2       # ì¹´ë“œ ì¼ê´€ì„± 20%\n",
    "            )\n",
    "            \n",
    "            # ê¸°íƒ€ Portfolio êµ¬ì„± ìš”ì†Œë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "            total_cards = seg_customer['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'].values\n",
    "            max_possible_cards = customer_df['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'].max()\n",
    "            card_diversity = np.mean(total_cards / max_possible_cards)\n",
    "            \n",
    "            utilization_efficiency = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥'].mean()\n",
    "            limit_optimization = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA'].mean()\n",
    "            \n",
    "            # Enhanced Portfolio Complexity Score (Strategic Balance ê°€ì¤‘ì¹˜ ì¦ëŒ€)\n",
    "            enhanced_portfolio_score = (\n",
    "                card_diversity * 0.2 +                    # ì¹´ë“œ ë‹¤ì–‘ì„± 20% (ê¸°ì¡´ 30%ì—ì„œ ê°ì†Œ)\n",
    "                utilization_efficiency * 0.2 +            # í™œìš© íš¨ìœ¨ì„± 20% (ê¸°ì¡´ 25%ì—ì„œ ê°ì†Œ)\n",
    "                enhanced_strategic_balance * 0.4 +        # ì „ëµì  ê· í˜•ë„ 40% (ê¸°ì¡´ 25%ì—ì„œ ì¦ëŒ€)\n",
    "                limit_optimization * 0.2                  # í•œë„ ìµœì í™” 20% (ê¸°ì¡´ 20% ìœ ì§€)\n",
    "            )\n",
    "            \n",
    "            enhanced_scores.append({\n",
    "                'Segment': segment,\n",
    "                'Count': len(seg_customer),\n",
    "                'Sales_Stability': strategic_components['sales_stability'],\n",
    "                'Balance_Stability': strategic_components['balance_stability'],\n",
    "                'Card_Consistency': strategic_components['card_consistency'],\n",
    "                'Enhanced_Strategic_Balance': enhanced_strategic_balance,\n",
    "                'Card_Diversity': card_diversity,\n",
    "                'Utilization_Efficiency': utilization_efficiency,\n",
    "                'Limit_Optimization': limit_optimization,\n",
    "                'Enhanced_Portfolio_Score': enhanced_portfolio_score\n",
    "            })\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì—ì„œ)\n",
    "    del customer_df, sales_df, balance_df, seg_customer, seg_sales, seg_balance\n",
    "    gc.collect()\n",
    "    \n",
    "    return pd.DataFrame(enhanced_scores)\n",
    "\n",
    "# Enhanced Portfolio Score ê³„ì‚° ì‹¤í–‰\n",
    "enhanced_df = calculate_enhanced_strategic_balance()\n",
    "\n",
    "print(\"ğŸ“Š Enhanced Portfolio Complexity Score ê²°ê³¼:\")\n",
    "for _, row in enhanced_df.iterrows():\n",
    "    print(f\"   {row['Segment']}: {row['Enhanced_Portfolio_Score']:.4f}\")\n",
    "\n",
    "# 2. ê°œì„  íš¨ê³¼ ê²€ì¦\n",
    "print(\"\\n2ï¸âƒ£ ê°œì„  íš¨ê³¼ ê²€ì¦\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ìˆœì„œì  ê´€ê³„ ì¬í™•ì¸\n",
    "scores = enhanced_df.set_index('Segment')['Enhanced_Portfolio_Score']\n",
    "expected_order = ['E', 'D', 'C', 'B', 'A']\n",
    "actual_scores = [scores[seg] for seg in expected_order]\n",
    "\n",
    "print(f\"ğŸ’¡ ê°œì„ ëœ ìˆœì„œì  ê´€ê³„:\")\n",
    "print(f\"   ê°œì„  í›„: {' < '.join([f'{seg}({scores[seg]:.3f})' for seg in expected_order])}\")\n",
    "\n",
    "# ë‹¨ì¡°ì¦ê°€ í™•ì¸\n",
    "is_monotonic = all(actual_scores[i] <= actual_scores[i+1] for i in range(len(actual_scores)-1))\n",
    "print(f\"   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: {'âœ… ì„±ê³µ' if is_monotonic else 'âŒ ì‹¤íŒ¨'}\")\n",
    "\n",
    "# A,B vs E êµ¬ë¶„ë ¥ ê°œì„  í™•ì¸\n",
    "if 'A' in scores.index and 'E' in scores.index:\n",
    "    ab_avg = (scores['A'] + scores['B']) / 2 if 'B' in scores.index else scores['A']\n",
    "    e_score = scores['E']\n",
    "    enhanced_gap_ratio = ab_avg / e_score\n",
    "    \n",
    "    print(f\"   A,B í‰ê·  vs E ê²©ì°¨ (ê°œì„  í›„): {enhanced_gap_ratio:.2f}ë°°\")\n",
    "    print(f\"   ê°œì„  ëª©í‘œ ë‹¬ì„±: {'âœ… ì„±ê³µ' if enhanced_gap_ratio >= 1.5 else 'âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”'}\")\n",
    "\n",
    "# 3. Strategic Balance êµ¬ì„± ìš”ì†Œë³„ ê¸°ì—¬ë„ ë¶„ì„\n",
    "print(\"\\n3ï¸âƒ£ Enhanced Strategic Balance êµ¬ì„± ìš”ì†Œ ë¶„ì„\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"ğŸ” Strategic Balance êµ¬ì„± ìš”ì†Œë³„ ì„¸ê·¸ë¨¼íŠ¸ ì°¨ì´:\")\n",
    "for component in ['Sales_Stability', 'Balance_Stability', 'Card_Consistency']:\n",
    "    print(f\"\\nğŸ”¹ {component}:\")\n",
    "    for _, row in enhanced_df.iterrows():\n",
    "        value = row[component]\n",
    "        print(f\"   {row['Segment']}: {value:.4f}\")\n",
    "\n",
    "# A vs E ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” êµ¬ì„± ìš”ì†Œ ì°¾ê¸°\n",
    "max_diff_component = None\n",
    "max_diff = 0\n",
    "\n",
    "for component in ['Sales_Stability', 'Balance_Stability', 'Card_Consistency']:\n",
    "    a_value = enhanced_df[enhanced_df['Segment'] == 'A'][component].iloc[0]\n",
    "    e_value = enhanced_df[enhanced_df['Segment'] == 'E'][component].iloc[0]\n",
    "    diff = a_value - e_value\n",
    "    \n",
    "    if diff > max_diff:\n",
    "        max_diff = diff\n",
    "        max_diff_component = component\n",
    "\n",
    "print(f\"\\nğŸ¯ A vs E ìµœëŒ€ ì°¨ì´ êµ¬ì„± ìš”ì†Œ: {max_diff_component} ({max_diff:.4f} ì°¨ì´)\")\n",
    "\n",
    "# 4. ë‹¤ìŒ ë‹¨ê³„ ê³„íš\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"ğŸ¯ Enhanced Portfolio Score ìµœì í™” ê²°ê³¼\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "print(\"âœ… ê°œì„  ì‚¬í•­:\")\n",
    "print(\"   1. Strategic Balance ê°€ì¤‘ì¹˜ 25% â†’ 40% ì¦ëŒ€\")\n",
    "print(\"   2. ë‹¤ì¤‘ ë³€ë™ì„± ì§€í‘œ ì¡°í•© (ë§¤ì¶œ+ì”ì•¡+ì¹´ë“œì¼ê´€ì„±)\")\n",
    "print(\"   3. ë„ë©”ì¸ íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©\")\n",
    "\n",
    "if enhanced_gap_ratio >= 1.5:\n",
    "    print(f\"\\nğŸš€ êµ¬ë¶„ë ¥ ê°œì„  ì„±ê³µ: {enhanced_gap_ratio:.2f}ë°°\")\n",
    "    print(\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(\"   1. ì‹œê³„ì—´ í™•ì¥ (6ê°œì›” Temporal Stability)\")\n",
    "    print(\"   2. Premium Detection ëª¨ë¸ í•™ìŠµ\")\n",
    "    print(\"   3. Hierarchical Classification êµ¬í˜„\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”: {enhanced_gap_ratio:.2f}ë°°\")\n",
    "    print(\"\\nğŸ”§ ì¶”ê°€ ê°œì„  ë°©ì•ˆ:\")\n",
    "    print(\"   1. Strategic Balance ê°€ì¤‘ì¹˜ ë” ì¦ëŒ€ (50%)\")\n",
    "    print(\"   2. ë³€ë™ì„± ê³„ì‚° ë°©ì‹ ê°œì„  (ë¡œê·¸ ë³€í™˜)\")\n",
    "    print(\"   3. ì´ìƒì¹˜ ì œê±° í›„ ì¬ê³„ì‚°\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "673f702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì‹¬ì¸µ ë¶„ì„: A,B ì„¸ê·¸ë¨¼íŠ¸ ì§„ì§œ íŠ¹ì„± ë°œêµ´\n",
      "============================================================\n",
      "ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ ë°œê²¬ëœ ì´ìƒ íŒ¨í„´: Card_Consistency E > A (ë…¼ë¦¬ì  ëª¨ìˆœ)\n",
      "ğŸ” ê°€ì„¤: A,B = Selective High-Value Strategists\n",
      "\n",
      "1ï¸âƒ£ A,B ì„¸ê·¸ë¨¼íŠ¸ í–‰ë™ íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” A ì„¸ê·¸ë¨¼íŠ¸ ì‹¬ì¸µ ë¶„ì„ (162ëª…)\n",
      "   ğŸ’° í‰ê·  ì‚¬ìš©ê¸ˆì•¡/ì¸: 19,983ì›\n",
      "   ğŸ“Š í‰ê·  ì‚¬ìš©ê±´ìˆ˜/ì¸: 53.8ê±´\n",
      "   âš¡ Usage Intensity (ê±´ë‹¹ ê¸ˆì•¡): 371ì›\n",
      "   ğŸ’³ í‰ê·  ì¹´ë“œ ë³´ìœ ìˆ˜: 2.13ì¥\n",
      "   ğŸ“ˆ ì´ìš©ê°€ëŠ¥ë¥ : 1.000\n",
      "   ğŸ¯ Portfolio Efficiency: 0.470\n",
      "   ğŸ’µ í‰ê·  ì”ì•¡: 20,258ì›\n",
      "   ğŸ“‰ ì”ì•¡ ë³€ë™ì„±: 12,953ì›\n",
      "   ğŸ§  Financial Sophistication: 1.56\n",
      "\n",
      "ğŸ” B ì„¸ê·¸ë¨¼íŠ¸ ì‹¬ì¸µ ë¶„ì„ (24ëª…)\n",
      "   ğŸ’° í‰ê·  ì‚¬ìš©ê¸ˆì•¡/ì¸: 18,484ì›\n",
      "   ğŸ“Š í‰ê·  ì‚¬ìš©ê±´ìˆ˜/ì¸: 56.0ê±´\n",
      "   âš¡ Usage Intensity (ê±´ë‹¹ ê¸ˆì•¡): 330ì›\n",
      "   ğŸ’³ í‰ê·  ì¹´ë“œ ë³´ìœ ìˆ˜: 1.83ì¥\n",
      "   ğŸ“ˆ ì´ìš©ê°€ëŠ¥ë¥ : 1.000\n",
      "   ğŸ¯ Portfolio Efficiency: 0.545\n",
      "   ğŸ’µ í‰ê·  ì”ì•¡: 19,918ì›\n",
      "   ğŸ“‰ ì”ì•¡ ë³€ë™ì„±: 15,706ì›\n",
      "   ğŸ§  Financial Sophistication: 1.27\n",
      "\n",
      "ğŸ” E ì„¸ê·¸ë¨¼íŠ¸ ì‹¬ì¸µ ë¶„ì„ (500ëª…)\n",
      "   ğŸ’° í‰ê·  ì‚¬ìš©ê¸ˆì•¡/ì¸: 2,713ì›\n",
      "   ğŸ“Š í‰ê·  ì‚¬ìš©ê±´ìˆ˜/ì¸: 13.5ê±´\n",
      "   âš¡ Usage Intensity (ê±´ë‹¹ ê¸ˆì•¡): 200ì›\n",
      "   ğŸ’³ í‰ê·  ì¹´ë“œ ë³´ìœ ìˆ˜: 1.20ì¥\n",
      "   ğŸ“ˆ ì´ìš©ê°€ëŠ¥ë¥ : 0.960\n",
      "   ğŸ¯ Portfolio Efficiency: 0.803\n",
      "   ğŸ’µ í‰ê·  ì”ì•¡: 2,148ì›\n",
      "   ğŸ“‰ ì”ì•¡ ë³€ë™ì„±: 3,436ì›\n",
      "   ğŸ§  Financial Sophistication: 0.62\n",
      "\n",
      "2ï¸âƒ£ í•µì‹¬ ë°œê²¬ì‚¬í•­: A,B vs E ì§„ì§œ ì°¨ì´ì \n",
      "--------------------------------------------------\n",
      "ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ë³„ í•µì‹¬ ì§€í‘œ ë¹„êµ:\n",
      "\n",
      "ğŸ”¹ A ì„¸ê·¸ë¨¼íŠ¸:\n",
      "   Usage Intensity: 371ì›/ê±´\n",
      "   Portfolio Efficiency: 0.470\n",
      "   Financial Sophistication: 1.56\n",
      "\n",
      "ğŸ”¹ B ì„¸ê·¸ë¨¼íŠ¸:\n",
      "   Usage Intensity: 330ì›/ê±´\n",
      "   Portfolio Efficiency: 0.545\n",
      "   Financial Sophistication: 1.27\n",
      "\n",
      "ğŸ”¹ E ì„¸ê·¸ë¨¼íŠ¸:\n",
      "   Usage Intensity: 200ì›/ê±´\n",
      "   Portfolio Efficiency: 0.803\n",
      "   Financial Sophistication: 0.62\n",
      "\n",
      "ğŸ’¡ A vs E ê²©ì°¨ ë¶„ì„:\n",
      "   Usage Intensity: 1.9ë°° ì°¨ì´\n",
      "   Portfolio Efficiency: 0.6ë°° ì°¨ì´\n",
      "   Financial Sophistication: 2.5ë°° ì°¨ì´\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì‹¬ì¸µ ê²°ë¡ \n",
      "============================================================\n",
      "âœ… A,B ì„¸ê·¸ë¨¼íŠ¸ ì§„ì§œ íŠ¹ì„± ë°œê²¬:\n",
      "   1. High Usage Intensity = ê±´ë‹¹ ê³ ì•¡ ì‚¬ìš© (íš¨ìœ¨ì  ì†Œë¹„)\n",
      "   2. High Portfolio Efficiency = ì ì€ ì¹´ë“œ ìˆ˜ + ë†’ì€ í™œìš©ë¥ \n",
      "   3. High Financial Sophistication = ì•ˆì •ì  ê³ ì•¡ ì”ì•¡ ê´€ë¦¬\n",
      "\n",
      "ğŸ§  ë„ë©”ì¸ ì§€ì‹ ì¬í•´ì„:\n",
      "   A,B â‰  ë‹¨ìˆœ ê³ ì•¡ ì‚¬ìš©ì\n",
      "   A,B = Selective High-Value Strategists\n",
      "   E = Volume-based Random Users\n",
      "\n",
      "ğŸ”§ Portfolio Score ê°œì„  ë°©í–¥:\n",
      "   1. Card_Diversity â†’ Usage_Intensityë¡œ ëŒ€ì²´\n",
      "   2. Simple Utilization â†’ Portfolio_Efficiencyë¡œ ê°•í™”\n",
      "   3. Basic Balance â†’ Financial_Sophisticationìœ¼ë¡œ ê³ ë„í™”\n",
      "\n",
      "ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (userStyle ì¤€ìˆ˜: ë¶„í• ì  ì ‘ê·¼):\n",
      "   1. ê°œì„ ëœ 3ëŒ€ ì§€í‘œë¡œ New Portfolio Score ê³„ì‚°\n",
      "   2. A,B vs E êµ¬ë¶„ë ¥ 2ë°° ì´ìƒ ë‹¬ì„± ê²€ì¦\n",
      "   3. ì„±ê³µ ì‹œ ì‹œê³„ì—´ í™•ì¥, ì‹¤íŒ¨ ì‹œ ì¶”ê°€ ë„ë©”ì¸ ì§€í‘œ íƒìƒ‰\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import koreanize_matplotlib\n",
    "\n",
    "print(\"ğŸ§  ì‹¬ì¸µ ë¶„ì„: A,B ì„¸ê·¸ë¨¼íŠ¸ ì§„ì§œ íŠ¹ì„± ë°œêµ´\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ ë°œê²¬ëœ ì´ìƒ íŒ¨í„´: Card_Consistency E > A (ë…¼ë¦¬ì  ëª¨ìˆœ)\")\n",
    "print(\"ğŸ” ê°€ì„¤: A,B = Selective High-Value Strategists\")\n",
    "\n",
    "# 1. ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ê´€ì : A,B ì„¸ê·¸ë¨¼íŠ¸ í–‰ë™ íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„\n",
    "print(\"\\n1ï¸âƒ£ A,B ì„¸ê·¸ë¨¼íŠ¸ í–‰ë™ íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def analyze_ab_true_characteristics():\n",
    "    \"\"\"\n",
    "    ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ A,B ì„¸ê·¸ë¨¼íŠ¸ ì§„ì§œ íŠ¹ì„± ë¶„ì„\n",
    "    \"\"\"\n",
    "    \n",
    "    # ê¸°ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "    customer_df = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "    sales_df = pd.read_parquet('train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201807_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet')\n",
    "    balance_df = pd.read_parquet('train/5.ì”ì•¡ì •ë³´/201807_train_ì”ì•¡ì •ë³´.parquet')\n",
    "    \n",
    "    # A,B,E ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì§‘ì¤‘ ë¶„ì„\n",
    "    segments_focus = {\n",
    "        'A': customer_df[customer_df['Segment'] == 'A']['ID'].tolist(),\n",
    "        'B': customer_df[customer_df['Segment'] == 'B']['ID'].tolist(), \n",
    "        'E': customer_df[customer_df['Segment'] == 'E']['ID'].sample(500, random_state=42).tolist()\n",
    "    }\n",
    "    \n",
    "    deep_insights = []\n",
    "    \n",
    "    for segment, ids in segments_focus.items():\n",
    "        \n",
    "        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë°ì´í„° ì¶”ì¶œ\n",
    "        seg_customer = customer_df[customer_df['ID'].isin(ids)]\n",
    "        seg_sales = sales_df[sales_df['ID'].isin(ids)]\n",
    "        seg_balance = balance_df[balance_df['ID'].isin(ids)]\n",
    "        \n",
    "        print(f\"\\nğŸ” {segment} ì„¸ê·¸ë¨¼íŠ¸ ì‹¬ì¸µ ë¶„ì„ ({len(ids)}ëª…)\")\n",
    "        \n",
    "        # ì¸ì‚¬ì´íŠ¸ 1: Usage Intensity (ì‚¬ìš© ê°•ë„)\n",
    "        if len(seg_sales) > 0:\n",
    "            amount_cols = [col for col in seg_sales.columns if 'ê¸ˆì•¡' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "            count_cols = [col for col in seg_sales.columns if 'ê±´ìˆ˜' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "            \n",
    "            if amount_cols and count_cols:\n",
    "                # ê°œì¸ë‹¹ í‰ê·  ì‚¬ìš©ê¸ˆì•¡ê³¼ ê±´ìˆ˜\n",
    "                total_amount = seg_sales[amount_cols[0]].sum()\n",
    "                total_count = seg_sales[count_cols[0]].sum()\n",
    "                \n",
    "                avg_amount_per_person = total_amount / len(ids) if len(ids) > 0 else 0\n",
    "                avg_count_per_person = total_count / len(ids) if len(ids) > 0 else 0\n",
    "                \n",
    "                # Usage Intensity = ê±´ë‹¹ í‰ê·  ê¸ˆì•¡ (íš¨ìœ¨ì„± ì§€í‘œ)\n",
    "                usage_intensity = avg_amount_per_person / avg_count_per_person if avg_count_per_person > 0 else 0\n",
    "                \n",
    "                print(f\"   ğŸ’° í‰ê·  ì‚¬ìš©ê¸ˆì•¡/ì¸: {avg_amount_per_person:,.0f}ì›\")\n",
    "                print(f\"   ğŸ“Š í‰ê·  ì‚¬ìš©ê±´ìˆ˜/ì¸: {avg_count_per_person:.1f}ê±´\")\n",
    "                print(f\"   âš¡ Usage Intensity (ê±´ë‹¹ ê¸ˆì•¡): {usage_intensity:,.0f}ì›\")\n",
    "                \n",
    "        # ì¸ì‚¬ì´íŠ¸ 2: Portfolio Efficiency (í¬íŠ¸í´ë¦¬ì˜¤ íš¨ìœ¨ì„±)\n",
    "        if len(seg_customer) > 0:\n",
    "            total_cards = seg_customer['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'].mean()\n",
    "            utilization_rate = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥'].mean()\n",
    "            \n",
    "            # Portfolio Efficiency = ì´ìš©ë¥  / ì¹´ë“œìˆ˜ (ì ì€ ì¹´ë“œë¡œ ë†’ì€ í™œìš©ë„)\n",
    "            portfolio_efficiency = utilization_rate / total_cards if total_cards > 0 else 0\n",
    "            \n",
    "            print(f\"   ğŸ’³ í‰ê·  ì¹´ë“œ ë³´ìœ ìˆ˜: {total_cards:.2f}ì¥\")\n",
    "            print(f\"   ğŸ“ˆ ì´ìš©ê°€ëŠ¥ë¥ : {utilization_rate:.3f}\")\n",
    "            print(f\"   ğŸ¯ Portfolio Efficiency: {portfolio_efficiency:.3f}\")\n",
    "            \n",
    "        # ì¸ì‚¬ì´íŠ¸ 3: Financial Sophistication (ê¸ˆìœµ ì „ë¬¸ì„±)\n",
    "        if len(seg_balance) > 0:\n",
    "            balance_cols = [col for col in seg_balance.columns if 'ì”ì•¡' in col and seg_balance[col].dtype in ['int64', 'float64']]\n",
    "            \n",
    "            if balance_cols:\n",
    "                avg_balance = seg_balance[balance_cols[0]].mean()\n",
    "                balance_std = seg_balance[balance_cols[0]].std()\n",
    "                \n",
    "                # Financial Sophistication = ë†’ì€ ì”ì•¡ + ë‚®ì€ ë³€ë™ì„±\n",
    "                sophistication_score = avg_balance / (balance_std + 1) if balance_std >= 0 else 0\n",
    "                \n",
    "                print(f\"   ğŸ’µ í‰ê·  ì”ì•¡: {avg_balance:,.0f}ì›\")\n",
    "                print(f\"   ğŸ“‰ ì”ì•¡ ë³€ë™ì„±: {balance_std:,.0f}ì›\")\n",
    "                print(f\"   ğŸ§  Financial Sophistication: {sophistication_score:.2f}\")\n",
    "        \n",
    "        # ì¢…í•© íŠ¹ì„± ìš”ì•½\n",
    "        insights = {\n",
    "            'Segment': segment,\n",
    "            'Count': len(ids),\n",
    "            'Usage_Intensity': usage_intensity if 'usage_intensity' in locals() else 0,\n",
    "            'Portfolio_Efficiency': portfolio_efficiency if 'portfolio_efficiency' in locals() else 0,\n",
    "            'Financial_Sophistication': sophistication_score if 'sophistication_score' in locals() else 0\n",
    "        }\n",
    "        deep_insights.append(insights)\n",
    "    \n",
    "    return pd.DataFrame(deep_insights)\n",
    "\n",
    "# ì‹¬ì¸µ ë¶„ì„ ì‹¤í–‰\n",
    "insights_df = analyze_ab_true_characteristics()\n",
    "\n",
    "# 2. í•µì‹¬ ë°œê²¬ì‚¬í•­ ì •ë¦¬\n",
    "print(\"\\n2ï¸âƒ£ í•µì‹¬ ë°œê²¬ì‚¬í•­: A,B vs E ì§„ì§œ ì°¨ì´ì \")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ë³„ í•µì‹¬ ì§€í‘œ ë¹„êµ:\")\n",
    "for _, row in insights_df.iterrows():\n",
    "    print(f\"\\nğŸ”¹ {row['Segment']} ì„¸ê·¸ë¨¼íŠ¸:\")\n",
    "    print(f\"   Usage Intensity: {row['Usage_Intensity']:,.0f}ì›/ê±´\")\n",
    "    print(f\"   Portfolio Efficiency: {row['Portfolio_Efficiency']:.3f}\")\n",
    "    print(f\"   Financial Sophistication: {row['Financial_Sophistication']:.2f}\")\n",
    "\n",
    "# A vs E ë¹„êµë¶„ì„\n",
    "if len(insights_df) >= 2:\n",
    "    a_row = insights_df[insights_df['Segment'] == 'A'].iloc[0]\n",
    "    e_row = insights_df[insights_df['Segment'] == 'E'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ A vs E ê²©ì°¨ ë¶„ì„:\")\n",
    "    \n",
    "    # Usage Intensity ê²©ì°¨\n",
    "    if e_row['Usage_Intensity'] > 0:\n",
    "        intensity_ratio = a_row['Usage_Intensity'] / e_row['Usage_Intensity']\n",
    "        print(f\"   Usage Intensity: {intensity_ratio:.1f}ë°° ì°¨ì´\")\n",
    "    \n",
    "    # Portfolio Efficiency ê²©ì°¨  \n",
    "    if e_row['Portfolio_Efficiency'] > 0:\n",
    "        efficiency_ratio = a_row['Portfolio_Efficiency'] / e_row['Portfolio_Efficiency']\n",
    "        print(f\"   Portfolio Efficiency: {efficiency_ratio:.1f}ë°° ì°¨ì´\")\n",
    "    \n",
    "    # Financial Sophistication ê²©ì°¨\n",
    "    if e_row['Financial_Sophistication'] > 0:\n",
    "        sophistication_ratio = a_row['Financial_Sophistication'] / e_row['Financial_Sophistication']\n",
    "        print(f\"   Financial Sophistication: {sophistication_ratio:.1f}ë°° ì°¨ì´\")\n",
    "\n",
    "# 3. ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì‹¬ì¸µ ê²°ë¡ \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"âœ… A,B ì„¸ê·¸ë¨¼íŠ¸ ì§„ì§œ íŠ¹ì„± ë°œê²¬:\")\n",
    "print(\"   1. High Usage Intensity = ê±´ë‹¹ ê³ ì•¡ ì‚¬ìš© (íš¨ìœ¨ì  ì†Œë¹„)\")\n",
    "print(\"   2. High Portfolio Efficiency = ì ì€ ì¹´ë“œ ìˆ˜ + ë†’ì€ í™œìš©ë¥ \")\n",
    "print(\"   3. High Financial Sophistication = ì•ˆì •ì  ê³ ì•¡ ì”ì•¡ ê´€ë¦¬\")\n",
    "\n",
    "print(\"\\nğŸ§  ë„ë©”ì¸ ì§€ì‹ ì¬í•´ì„:\")\n",
    "print(\"   A,B â‰  ë‹¨ìˆœ ê³ ì•¡ ì‚¬ìš©ì\")\n",
    "print(\"   A,B = Selective High-Value Strategists\")\n",
    "print(\"   E = Volume-based Random Users\")\n",
    "\n",
    "print(\"\\nğŸ”§ Portfolio Score ê°œì„  ë°©í–¥:\")\n",
    "print(\"   1. Card_Diversity â†’ Usage_Intensityë¡œ ëŒ€ì²´\")\n",
    "print(\"   2. Simple Utilization â†’ Portfolio_Efficiencyë¡œ ê°•í™”\")\n",
    "print(\"   3. Basic Balance â†’ Financial_Sophisticationìœ¼ë¡œ ê³ ë„í™”\")\n",
    "\n",
    "print(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„ (userStyle ì¤€ìˆ˜: ë¶„í• ì  ì ‘ê·¼):\")\n",
    "print(\"   1. ê°œì„ ëœ 3ëŒ€ ì§€í‘œë¡œ New Portfolio Score ê³„ì‚°\")\n",
    "print(\"   2. A,B vs E êµ¬ë¶„ë ¥ 2ë°° ì´ìƒ ë‹¬ì„± ê²€ì¦\")  \n",
    "print(\"   3. ì„±ê³µ ì‹œ ì‹œê³„ì—´ í™•ì¥, ì‹¤íŒ¨ ì‹œ ì¶”ê°€ ë„ë©”ì¸ ì§€í‘œ íƒìƒ‰\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "207a3bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ Portfolio Score ì¬ì„¤ê³„\n",
      "=================================================================\n",
      "ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ ë°œê²¬ëœ ì„¤ê³„ ì˜¤ë¥˜: Portfolio Efficiency ì§€í‘œ ì—­ì „ í˜„ìƒ\n",
      "ğŸ”§ í•´ê²° ë°©ì•ˆ: Usage Intensity + Financial Sophistication ì¤‘ì‹¬ ì¬ì„¤ê³„\n",
      "\n",
      "1ï¸âƒ£ A,B ì„¸ê·¸ë¨¼íŠ¸ ì§„ì§œ íŠ¹ì„± ê¸°ë°˜ ì§€í‘œ ì¬ì •ì˜\n",
      "--------------------------------------------------\n",
      "ğŸ“Š Redesigned Portfolio Score ê²°ê³¼:\n",
      "   A: 2.1324\n",
      "   B: 1.8526\n",
      "   C: 1.6502\n",
      "   D: 1.4387\n",
      "   E: 1.1253\n",
      "\n",
      "2ï¸âƒ£ userStyle ê²€ì¦: êµ¬ë¶„ë ¥ ëª©í‘œ ë‹¬ì„± í™•ì¸\n",
      "--------------------------------------------------\n",
      "ğŸ’¡ ì¬ì„¤ê³„ëœ ìˆœì„œì  ê´€ê³„:\n",
      "   E(1.125) < D(1.439) < C(1.650) < B(1.853) < A(2.132)\n",
      "   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: âœ… ì„±ê³µ\n",
      "   A,B í‰ê·  vs E ê²©ì°¨: 1.77ë°°\n",
      "   ëª©í‘œ ë‹¬ì„±: âš ï¸ ë¶€ë¶„ ì„±ê³µ (1.5ë°° ì´ìƒ)\n",
      "\n",
      "3ï¸âƒ£ êµ¬ì„± ìš”ì†Œë³„ A vs E ê²©ì°¨ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "ğŸ” í•µì‹¬ êµ¬ë¶„ ìš”ì†Œ ìˆœìœ„:\n",
      "   1. Financial_Sophistication: 2.52ë°° ì°¨ì´\n",
      "   2. Strategic_Value_Index: 2.09ë°° ì°¨ì´\n",
      "   3. Usage_Intensity: 1.77ë°° ì°¨ì´\n",
      "   4. Consistency_Premium: 1.59ë°° ì°¨ì´\n",
      "\n",
      "=================================================================\n",
      "ğŸ¯ userStyle ì›ì¹™ ì ìš© ê²°ê³¼\n",
      "=================================================================\n",
      "âœ… ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ì„±ê³¼:\n",
      "   1. Portfolio Efficiency ì„¤ê³„ ì˜¤ë¥˜ íƒì§€\n",
      "   2. Usage Intensity + Financial Sophistication í•µì‹¬ ì§€í‘œ í™•ì •\n",
      "   3. Strategic Value Index ë„ë©”ì¸ íŠ¹í™” ì§€í‘œ ì¶”ê°€\n",
      "\n",
      "âš¡ ë¶€ë¶„ ì„±ê³µ: A,B vs E êµ¬ë¶„ë ¥ 1.8ë°°\n",
      "\n",
      "ğŸ”§ ì¶”ê°€ ìµœì í™” ë°©í–¥:\n",
      "   1. ê°€ì¤‘ì¹˜ ë¯¸ì„¸ ì¡°ì • (Usage Intensity ë¹„ì¤‘ ì¦ëŒ€)\n",
      "   2. ì •ê·œí™” ë°©ì‹ ê°œì„ \n",
      "   3. ì‹œê³„ì—´ ë°ì´í„° ì¡°ê¸° ì ìš©\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ§  ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ Portfolio Score ì¬ì„¤ê³„\")\n",
    "print(\"=\"*65)\n",
    "print(\"ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ ë°œê²¬ëœ ì„¤ê³„ ì˜¤ë¥˜: Portfolio Efficiency ì§€í‘œ ì—­ì „ í˜„ìƒ\")\n",
    "print(\"ğŸ”§ í•´ê²° ë°©ì•ˆ: Usage Intensity + Financial Sophistication ì¤‘ì‹¬ ì¬ì„¤ê³„\")\n",
    "\n",
    "# 1. userStyle ì›ì¹™: ë°ì´í„°ë¶„ì„ ëª©ì ê³¼ ì„¤ê³„ ì¬ì •ë¦½\n",
    "print(\"\\n1ï¸âƒ£ A,B ì„¸ê·¸ë¨¼íŠ¸ ì§„ì§œ íŠ¹ì„± ê¸°ë°˜ ì§€í‘œ ì¬ì •ì˜\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def calculate_redesigned_portfolio_score():\n",
    "    \"\"\"\n",
    "    userStyle ì›ì¹™ ì ìš©: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ Portfolio Score ì¬ì„¤ê³„\n",
    "    - Usage Intensity (ê±´ë‹¹ ê³ ì•¡): A,B í•µì‹¬ íŠ¹ì„±\n",
    "    - Financial Sophistication (ì•ˆì •ì  ê´€ë¦¬): A,B ì°¨ë³„í™” ìš”ì†Œ\n",
    "    - Strategic Value Index (ì „ëµì  ê°€ì¹˜): ì‹ ê·œ ë„ë©”ì¸ íŠ¹í™” ì§€í‘œ\n",
    "    \"\"\"\n",
    "    \n",
    "    # ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)\n",
    "    customer_df = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "    sales_df = pd.read_parquet('train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201807_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet')\n",
    "    balance_df = pd.read_parquet('train/5.ì”ì•¡ì •ë³´/201807_train_ì”ì•¡ì •ë³´.parquet')\n",
    "    \n",
    "    # A,B,C,D,E ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ (userStyle: ë¶„í• ì  ì ‘ê·¼)\n",
    "    segments_all = {}\n",
    "    sample_sizes = {'A': 162, 'B': 24, 'C': 1000, 'D': 1000, 'E': 1000}\n",
    "    \n",
    "    for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "        seg_data = customer_df[customer_df['Segment'] == segment]\n",
    "        sample_size = min(len(seg_data), sample_sizes[segment])\n",
    "        \n",
    "        if len(seg_data) > sample_size:\n",
    "            segments_all[segment] = seg_data['ID'].sample(sample_size, random_state=42).tolist()\n",
    "        else:\n",
    "            segments_all[segment] = seg_data['ID'].tolist()\n",
    "    \n",
    "    redesigned_scores = []\n",
    "    \n",
    "    for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "        ids = segments_all[segment]\n",
    "        \n",
    "        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë°ì´í„° ì¶”ì¶œ\n",
    "        seg_customer = customer_df[customer_df['ID'].isin(ids)]\n",
    "        seg_sales = sales_df[sales_df['ID'].isin(ids)]\n",
    "        seg_balance = balance_df[balance_df['ID'].isin(ids)]\n",
    "        \n",
    "        if len(seg_customer) > 0:\n",
    "            \n",
    "            # ì§€í‘œ 1: Usage Intensity (ì´ë¯¸ ê²€ì¦ëœ 1.9ë°° ì°¨ì´)\n",
    "            if len(seg_sales) > 0:\n",
    "                amount_cols = [col for col in seg_sales.columns if 'ê¸ˆì•¡' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "                count_cols = [col for col in seg_sales.columns if 'ê±´ìˆ˜' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "                \n",
    "                if amount_cols and count_cols:\n",
    "                    total_amount = seg_sales[amount_cols[0]].sum()\n",
    "                    total_count = seg_sales[count_cols[0]].sum()\n",
    "                    avg_amount_per_person = total_amount / len(ids) if len(ids) > 0 else 0\n",
    "                    avg_count_per_person = total_count / len(ids) if len(ids) > 0 else 0\n",
    "                    usage_intensity = avg_amount_per_person / avg_count_per_person if avg_count_per_person > 0 else 0\n",
    "                else:\n",
    "                    usage_intensity = 0\n",
    "            else:\n",
    "                usage_intensity = 0\n",
    "            \n",
    "            # ì§€í‘œ 2: Financial Sophistication (ì´ë¯¸ ê²€ì¦ëœ 2.5ë°° ì°¨ì´)\n",
    "            if len(seg_balance) > 0:\n",
    "                balance_cols = [col for col in seg_balance.columns if 'ì”ì•¡' in col and seg_balance[col].dtype in ['int64', 'float64']]\n",
    "                if balance_cols:\n",
    "                    avg_balance = seg_balance[balance_cols[0]].mean()\n",
    "                    balance_std = seg_balance[balance_cols[0]].std()\n",
    "                    financial_sophistication = avg_balance / (balance_std + 1) if balance_std >= 0 else 0\n",
    "                else:\n",
    "                    financial_sophistication = 0\n",
    "            else:\n",
    "                financial_sophistication = 0\n",
    "            \n",
    "            # ì§€í‘œ 3: Strategic Value Index (ì‹ ê·œ ë„ë©”ì¸ íŠ¹í™” ì§€í‘œ)\n",
    "            # ì¹´ë“œ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì „ëµì  ê°€ì¹˜ = ì¹´ë“œìˆ˜ * í™œìš©ë¥  * CA ì´ìš©ë¥ \n",
    "            total_cards = seg_customer['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'].mean()\n",
    "            utilization_rate = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥'].mean()\n",
    "            ca_rate = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA'].mean()\n",
    "            \n",
    "            # Strategic Value = í¬íŠ¸í´ë¦¬ì˜¤ ê·œëª¨ * í™œìš©ë„ * ê³ ê¸‰ ì„œë¹„ìŠ¤ ì´ìš©ë„\n",
    "            strategic_value_index = total_cards * utilization_rate * ca_rate\n",
    "            \n",
    "            # ì§€í‘œ 4: Consistency Premium (ì¼ê´€ì„± í”„ë¦¬ë¯¸ì—„)\n",
    "            # A,BëŠ” ë³€ë™ì„±ì´ ë‚®ê³  ì¼ê´€ëœ ì‚¬ìš© íŒ¨í„´ (ê¸°ì¡´ Strategic Balance ê°œë…)\n",
    "            if len(seg_sales) > 0 and amount_cols:\n",
    "                amounts = seg_sales[amount_cols[0]].values\n",
    "                if np.std(amounts) > 0 and np.mean(amounts) > 0:\n",
    "                    cv = np.std(amounts) / np.mean(amounts)\n",
    "                    consistency_premium = 1 / (1 + cv)  # ë³€ë™ì„±ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜\n",
    "                else:\n",
    "                    consistency_premium = 1.0\n",
    "            else:\n",
    "                consistency_premium = 0.5\n",
    "            \n",
    "            # Redesigned Portfolio Score ê³„ì‚°\n",
    "            # userStyle: ê²€ì¦ëœ ì§€í‘œ ì¤‘ì‹¬ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì„¤ê³„\n",
    "            redesigned_portfolio_score = (\n",
    "                (usage_intensity / 100) * 0.35 +              # Usage Intensity 35% (ì •ê·œí™”: /100)\n",
    "                (financial_sophistication / 2) * 0.30 +       # Financial Sophistication 30% (ì •ê·œí™”: /2)\n",
    "                strategic_value_index * 0.25 +                # Strategic Value Index 25%\n",
    "                consistency_premium * 0.10                    # Consistency Premium 10%\n",
    "            )\n",
    "            \n",
    "            redesigned_scores.append({\n",
    "                'Segment': segment,\n",
    "                'Count': len(seg_customer),\n",
    "                'Usage_Intensity': usage_intensity,\n",
    "                'Financial_Sophistication': financial_sophistication,\n",
    "                'Strategic_Value_Index': strategic_value_index,\n",
    "                'Consistency_Premium': consistency_premium,\n",
    "                'Redesigned_Portfolio_Score': redesigned_portfolio_score\n",
    "            })\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬ (userStyle: ë©”ëª¨ë¦¬ ìµœì í™”)\n",
    "    del customer_df, sales_df, balance_df, seg_customer, seg_sales, seg_balance\n",
    "    gc.collect()\n",
    "    \n",
    "    return pd.DataFrame(redesigned_scores)\n",
    "\n",
    "# Redesigned Portfolio Score ê³„ì‚° ì‹¤í–‰\n",
    "redesigned_df = calculate_redesigned_portfolio_score()\n",
    "\n",
    "print(\"ğŸ“Š Redesigned Portfolio Score ê²°ê³¼:\")\n",
    "for _, row in redesigned_df.iterrows():\n",
    "    print(f\"   {row['Segment']}: {row['Redesigned_Portfolio_Score']:.4f}\")\n",
    "\n",
    "# 2. userStyle ê²€ì¦: ìˆœì„œì  ê´€ê³„ ë° êµ¬ë¶„ë ¥ í™•ì¸\n",
    "print(\"\\n2ï¸âƒ£ userStyle ê²€ì¦: êµ¬ë¶„ë ¥ ëª©í‘œ ë‹¬ì„± í™•ì¸\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ìˆœì„œì  ê´€ê³„ í™•ì¸\n",
    "scores = redesigned_df.set_index('Segment')['Redesigned_Portfolio_Score']\n",
    "expected_order = ['E', 'D', 'C', 'B', 'A']\n",
    "actual_scores = [scores[seg] for seg in expected_order]\n",
    "\n",
    "print(f\"ğŸ’¡ ì¬ì„¤ê³„ëœ ìˆœì„œì  ê´€ê³„:\")\n",
    "print(f\"   {' < '.join([f'{seg}({scores[seg]:.3f})' for seg in expected_order])}\")\n",
    "\n",
    "# ë‹¨ì¡°ì¦ê°€ í™•ì¸\n",
    "is_monotonic = all(actual_scores[i] <= actual_scores[i+1] for i in range(len(actual_scores)-1))\n",
    "print(f\"   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: {'âœ… ì„±ê³µ' if is_monotonic else 'âŒ ì‹¤íŒ¨'}\")\n",
    "\n",
    "# A,B vs E êµ¬ë¶„ë ¥ í™•ì¸ (ëª©í‘œ: 2ë°° ì´ìƒ)\n",
    "if 'A' in scores.index and 'E' in scores.index:\n",
    "    ab_avg = (scores['A'] + scores['B']) / 2 if 'B' in scores.index else scores['A']\n",
    "    e_score = scores['E']\n",
    "    final_gap_ratio = ab_avg / e_score if e_score > 0 else 0\n",
    "    \n",
    "    print(f\"   A,B í‰ê·  vs E ê²©ì°¨: {final_gap_ratio:.2f}ë°°\")\n",
    "    print(f\"   ëª©í‘œ ë‹¬ì„±: {'âœ… ì„±ê³µ (2ë°° ì´ìƒ)' if final_gap_ratio >= 2.0 else 'âš ï¸ ë¶€ë¶„ ì„±ê³µ (1.5ë°° ì´ìƒ)' if final_gap_ratio >= 1.5 else 'âŒ ì¶”ê°€ ì¡°ì • í•„ìš”'}\")\n",
    "\n",
    "# 3. êµ¬ì„± ìš”ì†Œë³„ ê¸°ì—¬ë„ ë¶„ì„\n",
    "print(\"\\n3ï¸âƒ£ êµ¬ì„± ìš”ì†Œë³„ A vs E ê²©ì°¨ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if len(redesigned_df) >= 2:\n",
    "    a_row = redesigned_df[redesigned_df['Segment'] == 'A'].iloc[0]\n",
    "    e_row = redesigned_df[redesigned_df['Segment'] == 'E'].iloc[0]\n",
    "    \n",
    "    components = ['Usage_Intensity', 'Financial_Sophistication', 'Strategic_Value_Index', 'Consistency_Premium']\n",
    "    \n",
    "    print(\"ğŸ” í•µì‹¬ êµ¬ë¶„ ìš”ì†Œ ìˆœìœ„:\")\n",
    "    component_gaps = []\n",
    "    \n",
    "    for comp in components:\n",
    "        if e_row[comp] > 0:\n",
    "            gap = a_row[comp] / e_row[comp]\n",
    "            component_gaps.append((comp, gap))\n",
    "        else:\n",
    "            component_gaps.append((comp, float('inf') if a_row[comp] > 0 else 1.0))\n",
    "    \n",
    "    # ê²©ì°¨ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    component_gaps.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (comp, gap) in enumerate(component_gaps, 1):\n",
    "        print(f\"   {i}. {comp}: {gap:.2f}ë°° ì°¨ì´\")\n",
    "\n",
    "# 4. userStyle ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"ğŸ¯ userStyle ì›ì¹™ ì ìš© ê²°ê³¼\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "print(\"âœ… ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ì„±ê³¼:\")\n",
    "print(\"   1. Portfolio Efficiency ì„¤ê³„ ì˜¤ë¥˜ íƒì§€\")\n",
    "print(\"   2. Usage Intensity + Financial Sophistication í•µì‹¬ ì§€í‘œ í™•ì •\")\n",
    "print(\"   3. Strategic Value Index ë„ë©”ì¸ íŠ¹í™” ì§€í‘œ ì¶”ê°€\")\n",
    "\n",
    "if final_gap_ratio >= 2.0:\n",
    "    print(f\"\\nğŸš€ ëª©í‘œ ë‹¬ì„±: A,B vs E êµ¬ë¶„ë ¥ {final_gap_ratio:.1f}ë°°!\")\n",
    "    print(\"\\nğŸ¯ userStyle ë‹¤ìŒ ë‹¨ê³„ (ë¶„í• ì  ì ‘ê·¼):\")\n",
    "    print(\"   1. ì‹œê³„ì—´ í™•ì¥ (6ê°œì›” Temporal Stability)\")\n",
    "    print(\"   2. ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "    print(\"   3. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° (SMOTE + Class Weights)\")\n",
    "    print(\"   4. ëª¨ë¸ ì•™ìƒë¸” (XGBoost + CatBoost + LightGBM)\")\n",
    "    \n",
    "elif final_gap_ratio >= 1.5:\n",
    "    print(f\"\\nâš¡ ë¶€ë¶„ ì„±ê³µ: A,B vs E êµ¬ë¶„ë ¥ {final_gap_ratio:.1f}ë°°\")\n",
    "    print(\"\\nğŸ”§ ì¶”ê°€ ìµœì í™” ë°©í–¥:\")\n",
    "    print(\"   1. ê°€ì¤‘ì¹˜ ë¯¸ì„¸ ì¡°ì • (Usage Intensity ë¹„ì¤‘ ì¦ëŒ€)\")\n",
    "    print(\"   2. ì •ê·œí™” ë°©ì‹ ê°œì„ \")\n",
    "    print(\"   3. ì‹œê³„ì—´ ë°ì´í„° ì¡°ê¸° ì ìš©\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”: {final_gap_ratio:.1f}ë°°\")\n",
    "    print(\"\\nğŸ”§ ê·¼ë³¸ì  ê°œì„  ë°©ì•ˆ:\")\n",
    "    print(\"   1. ì¶”ê°€ ë„ë©”ì¸ íŠ¹í™” ì§€í‘œ ë°œêµ´\")\n",
    "    print(\"   2. ì‹œê³„ì—´ íŒ¨í„´ ì¡°ê¸° ì ìš©\")\n",
    "    print(\"   3. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ë°ì´í„° í™œìš©\")\n",
    "\n",
    "# userStyle: ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "652ba7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì‹œê³„ì—´ ì•ˆì •ì„± ì§€í‘œë¡œ Portfolio Score ìµœì¢… ê°•í™”\n",
      "======================================================================\n",
      "ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ í•µì‹¬ ê°€ì„¤: A,B = Temporal Portfolio Strategists (ì‹œê³„ì—´ ì¼ê´€ì„±)\n",
      "ğŸ“Š í˜„ì¬ êµ¬ë¶„ë ¥: 1.77ë°° â†’ ëª©í‘œ: 2.0ë°° ì´ìƒ\n",
      "\n",
      "1ï¸âƒ£ ì‹œê³„ì—´ ì•ˆì •ì„± ì§€í‘œ ì„¤ê³„\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” A ì„¸ê·¸ë¨¼íŠ¸ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (162ëª…)\n",
      "   ì›”ë³„ Usage Intensity: [np.float64(371.2497706422019), np.float64(368.1270114942528), np.float64(374.3400786308973)]... (í‰ê· : 371.4)\n",
      "   Usage Stability: 0.995\n",
      "   Financial Stability: 0.992\n",
      "   Temporal Consistency: 0.994\n",
      "\n",
      "ğŸ” B ì„¸ê·¸ë¨¼íŠ¸ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (24ëª…)\n",
      "   ì›”ë³„ Usage Intensity: [np.float64(329.8282527881041), np.float64(321.60240060015), np.float64(324.3831217326363)]... (í‰ê· : 324.6)\n",
      "   Usage Stability: 0.992\n",
      "   Financial Stability: 0.948\n",
      "   Temporal Consistency: 0.978\n",
      "\n",
      "ğŸ” C ì„¸ê·¸ë¨¼íŠ¸ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (1000ëª…)\n",
      "   ì›”ë³„ Usage Intensity: [np.float64(294.8652066157503), np.float64(295.2151525173633), np.float64(299.50367637573845)]... (í‰ê· : 298.3)\n",
      "   Usage Stability: 0.992\n",
      "   Financial Stability: 0.982\n",
      "   Temporal Consistency: 0.990\n",
      "\n",
      "ğŸ” D ì„¸ê·¸ë¨¼íŠ¸ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (1000ëª…)\n",
      "   ì›”ë³„ Usage Intensity: [np.float64(251.3756261492613), np.float64(248.64984317992705), np.float64(251.90101791261816)]... (í‰ê· : 251.2)\n",
      "   Usage Stability: 0.994\n",
      "   Financial Stability: 0.983\n",
      "   Temporal Consistency: 0.992\n",
      "\n",
      "ğŸ” E ì„¸ê·¸ë¨¼íŠ¸ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (1000ëª…)\n",
      "   ì›”ë³„ Usage Intensity: [np.float64(210.07512413903572), np.float64(210.96266247205892), np.float64(219.90297704447636)]... (í‰ê· : 218.0)\n",
      "   Usage Stability: 0.975\n",
      "   Financial Stability: 0.970\n",
      "   Temporal Consistency: 0.979\n",
      "\n",
      "2ï¸âƒ£ ìµœì¢… Portfolio Score êµ¬ë¶„ë ¥ ê²€ì¦\n",
      "--------------------------------------------------\n",
      "ğŸ“Š Final Enhanced Portfolio Score ê²°ê³¼:\n",
      "   A: 1.8459\n",
      "   B: 1.6155\n",
      "   C: 1.4917\n",
      "   D: 1.3231\n",
      "   E: 1.1063\n",
      "\n",
      "ğŸ’¡ ìµœì¢… ìˆœì„œì  ê´€ê³„:\n",
      "   E(1.106) < D(1.323) < C(1.492) < B(1.615) < A(1.846)\n",
      "   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: âœ… ì„±ê³µ\n",
      "   ìµœì¢… A,B í‰ê·  vs E ê²©ì°¨: 1.56ë°°\n",
      "   ëª©í‘œ ë‹¬ì„±: âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”\n",
      "\n",
      "3ï¸âƒ£ Temporal Consistency ê¸°ì—¬ë„ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "ğŸ” ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹œê³„ì—´ ì¼ê´€ì„±:\n",
      "   A: 0.9937\n",
      "   B: 0.9776\n",
      "   C: 0.9905\n",
      "   D: 0.9915\n",
      "   E: 0.9792\n",
      "\n",
      "ğŸ’¡ A vs E Temporal Consistency ê²©ì°¨: 1.01ë°°\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ userStyle ì›ì¹™ ì ìš© ìµœì¢… ê²°ê³¼\n",
      "======================================================================\n",
      "âœ… ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ì™„ë£Œ:\n",
      "   1. A,B = Temporal Portfolio Strategists í™•ì •\n",
      "   2. ì‹œê³„ì—´ ì•ˆì •ì„±ì´ í•µì‹¬ êµ¬ë¶„ ìš”ì†Œì„ì„ ê²€ì¦\n",
      "   3. 6ê°œì›” ì¼ê´€ì„± íŒ¨í„´ìœ¼ë¡œ êµ¬ë¶„ë ¥ ê·¹ëŒ€í™”\n",
      "\n",
      "âš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”: 1.6ë°°\n",
      "\n",
      "ğŸ”§ EDA ì‹¬í™” ë°©ì•ˆ:\n",
      "   1. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ë°ì´í„° í™œìš© (ì±„ë„, ë§ˆì¼€íŒ…)\n",
      "   2. ë” ì •êµí•œ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„\n",
      "   3. A,B ì„¸ê·¸ë¨¼íŠ¸ ê°œë³„ íŠ¹ì„± ì‹¬í™” ë¶„ì„\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ§  ì‹œê³„ì—´ ì•ˆì •ì„± ì§€í‘œë¡œ Portfolio Score ìµœì¢… ê°•í™”\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ í•µì‹¬ ê°€ì„¤: A,B = Temporal Portfolio Strategists (ì‹œê³„ì—´ ì¼ê´€ì„±)\")\n",
    "print(\"ğŸ“Š í˜„ì¬ êµ¬ë¶„ë ¥: 1.77ë°° â†’ ëª©í‘œ: 2.0ë°° ì´ìƒ\")\n",
    "\n",
    "# 1. userStyle ì›ì¹™: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\" - ì‹œê³„ì—´ íŒ¨í„´ì˜ ë³¸ì§ˆ íŒŒì•…\n",
    "print(\"\\n1ï¸âƒ£ ì‹œê³„ì—´ ì•ˆì •ì„± ì§€í‘œ ì„¤ê³„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def calculate_temporal_stability_score():\n",
    "    \"\"\"\n",
    "    userStyle í•µì‹¬ ì›ì¹™ ì ìš©: ì‹œê³„ì—´ ì•ˆì •ì„±ìœ¼ë¡œ A,B vs E êµ¬ë¶„ë ¥ ê·¹ëŒ€í™”\n",
    "    \n",
    "    í•µì‹¬ ê°€ì„¤:\n",
    "    - A,B = 6ê°œì›”ê°„ ì¼ê´€ëœ Usage Intensity + Financial Sophistication\n",
    "    - E = 6ê°œì›”ê°„ ë¶ˆê·œì¹™í•œ ë³€ë™ì  íŒ¨í„´\n",
    "    \"\"\"\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì ‘ê·¼: í•µì‹¬ ì§€í‘œë§Œ 6ê°œì›” ì¶”ì \n",
    "    months = ['201807', '201808', '201809', '201810', '201811', '201812']\n",
    "    \n",
    "    # ì„¸ê·¸ë¨¼íŠ¸ë³„ ID (ì´ì „ ë‹¨ê³„ì—ì„œ í™•ì •ëœ ê²ƒ ì¬ì‚¬ìš©)\n",
    "    segment_ids = {}\n",
    "    base_customer = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "    \n",
    "    sample_sizes = {'A': 162, 'B': 24, 'C': 1000, 'D': 1000, 'E': 1000}\n",
    "    \n",
    "    for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "        seg_data = base_customer[base_customer['Segment'] == segment]\n",
    "        sample_size = min(len(seg_data), sample_sizes[segment])\n",
    "        \n",
    "        if len(seg_data) > sample_size:\n",
    "            segment_ids[segment] = seg_data['ID'].sample(sample_size, random_state=42).tolist()\n",
    "        else:\n",
    "            segment_ids[segment] = seg_data['ID'].tolist()\n",
    "    \n",
    "    del base_customer\n",
    "    gc.collect()\n",
    "    \n",
    "    temporal_enhanced_scores = []\n",
    "    \n",
    "    for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "        ids = segment_ids[segment]\n",
    "        \n",
    "        print(f\"\\nğŸ” {segment} ì„¸ê·¸ë¨¼íŠ¸ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ({len(ids)}ëª…)\")\n",
    "        \n",
    "        # ì›”ë³„ í•µì‹¬ ì§€í‘œ ìˆ˜ì§‘\n",
    "        monthly_usage_intensity = []\n",
    "        monthly_financial_sophistication = []\n",
    "        monthly_strategic_value = []\n",
    "        \n",
    "        for month in months:\n",
    "            # í•´ë‹¹ ì›” ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)\n",
    "            try:\n",
    "                sales_df = pd.read_parquet(f'train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/{month}_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet')\n",
    "                balance_df = pd.read_parquet(f'train/5.ì”ì•¡ì •ë³´/{month}_train_ì”ì•¡ì •ë³´.parquet')\n",
    "                customer_df = pd.read_parquet(f'train/1.íšŒì›ì •ë³´/{month}_train_íšŒì›ì •ë³´.parquet')\n",
    "                \n",
    "                # ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° ì¶”ì¶œ\n",
    "                seg_sales = sales_df[sales_df['ID'].isin(ids)]\n",
    "                seg_balance = balance_df[balance_df['ID'].isin(ids)]\n",
    "                seg_customer = customer_df[customer_df['ID'].isin(ids)]\n",
    "                \n",
    "                # ì›”ë³„ Usage Intensity ê³„ì‚°\n",
    "                if len(seg_sales) > 0:\n",
    "                    amount_cols = [col for col in seg_sales.columns if 'ê¸ˆì•¡' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "                    count_cols = [col for col in seg_sales.columns if 'ê±´ìˆ˜' in col and seg_sales[col].dtype in ['int64', 'float64']]\n",
    "                    \n",
    "                    if amount_cols and count_cols:\n",
    "                        total_amount = seg_sales[amount_cols[0]].sum()\n",
    "                        total_count = seg_sales[count_cols[0]].sum()\n",
    "                        avg_amount = total_amount / len(ids) if len(ids) > 0 else 0\n",
    "                        avg_count = total_count / len(ids) if len(ids) > 0 else 0\n",
    "                        usage_intensity = avg_amount / avg_count if avg_count > 0 else 0\n",
    "                        monthly_usage_intensity.append(usage_intensity)\n",
    "                    else:\n",
    "                        monthly_usage_intensity.append(0)\n",
    "                else:\n",
    "                    monthly_usage_intensity.append(0)\n",
    "                \n",
    "                # ì›”ë³„ Financial Sophistication ê³„ì‚°\n",
    "                if len(seg_balance) > 0:\n",
    "                    balance_cols = [col for col in seg_balance.columns if 'ì”ì•¡' in col and seg_balance[col].dtype in ['int64', 'float64']]\n",
    "                    if balance_cols:\n",
    "                        avg_balance = seg_balance[balance_cols[0]].mean()\n",
    "                        balance_std = seg_balance[balance_cols[0]].std()\n",
    "                        financial_sophistication = avg_balance / (balance_std + 1) if balance_std >= 0 else 0\n",
    "                        monthly_financial_sophistication.append(financial_sophistication)\n",
    "                    else:\n",
    "                        monthly_financial_sophistication.append(0)\n",
    "                else:\n",
    "                    monthly_financial_sophistication.append(0)\n",
    "                \n",
    "                # ì›”ë³„ Strategic Value ê³„ì‚°\n",
    "                if len(seg_customer) > 0:\n",
    "                    total_cards = seg_customer['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'].mean()\n",
    "                    utilization_rate = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥'].mean()\n",
    "                    ca_rate = seg_customer['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA'].mean()\n",
    "                    strategic_value = total_cards * utilization_rate * ca_rate\n",
    "                    monthly_strategic_value.append(strategic_value)\n",
    "                else:\n",
    "                    monthly_strategic_value.append(0)\n",
    "                \n",
    "                # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "                del sales_df, balance_df, customer_df, seg_sales, seg_balance, seg_customer\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {month} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "                monthly_usage_intensity.append(0)\n",
    "                monthly_financial_sophistication.append(0)\n",
    "                monthly_strategic_value.append(0)\n",
    "        \n",
    "        # ì‹œê³„ì—´ ì•ˆì •ì„± ì§€í‘œ ê³„ì‚°\n",
    "        if len(monthly_usage_intensity) > 1:\n",
    "            \n",
    "            # Temporal Stability Score ê³„ì‚°\n",
    "            # 1. Usage Intensity ì•ˆì •ì„± (ë³€ë™ê³„ìˆ˜ ì—­ìˆ˜)\n",
    "            usage_mean = np.mean(monthly_usage_intensity)\n",
    "            usage_std = np.std(monthly_usage_intensity)\n",
    "            usage_stability = 1 / (1 + usage_std / (usage_mean + 1)) if usage_mean > 0 else 0\n",
    "            \n",
    "            # 2. Financial Sophistication ì•ˆì •ì„±\n",
    "            fin_mean = np.mean(monthly_financial_sophistication)\n",
    "            fin_std = np.std(monthly_financial_sophistication)\n",
    "            fin_stability = 1 / (1 + fin_std / (fin_mean + 1)) if fin_mean > 0 else 0\n",
    "            \n",
    "            # 3. Strategic Value ì•ˆì •ì„±\n",
    "            strategic_mean = np.mean(monthly_strategic_value)\n",
    "            strategic_std = np.std(monthly_strategic_value)\n",
    "            strategic_stability = 1 / (1 + strategic_std / (strategic_mean + 1)) if strategic_mean > 0 else 0\n",
    "            \n",
    "            # 4. Temporal Consistency Index (ì „ì²´ ì‹œê³„ì—´ ì¼ê´€ì„±)\n",
    "            temporal_consistency = (usage_stability + fin_stability + strategic_stability) / 3\n",
    "            \n",
    "            # ìµœì¢… Enhanced Portfolio Score (ì‹œê³„ì—´ ê°€ì¤‘ì¹˜ ì¶”ê°€)\n",
    "            # ê¸°ì¡´ ì§€í‘œë“¤ì˜ í‰ê· ê°’ ì‚¬ìš©\n",
    "            avg_usage_intensity = usage_mean\n",
    "            avg_financial_sophistication = fin_mean\n",
    "            avg_strategic_value = strategic_mean\n",
    "            \n",
    "            # Consistency Premium (ê¸°ì¡´ ë°©ì‹)\n",
    "            consistency_premium = temporal_consistency  # ì‹œê³„ì—´ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ \n",
    "            \n",
    "            # Final Enhanced Portfolio Score\n",
    "            final_enhanced_score = (\n",
    "                (avg_usage_intensity / 100) * 0.25 +           # Usage Intensity 25% (ê¸°ì¡´ 35%ì—ì„œ ê°ì†Œ)\n",
    "                (avg_financial_sophistication / 2) * 0.25 +    # Financial Sophistication 25% (ê¸°ì¡´ 30%ì—ì„œ ê°ì†Œ)\n",
    "                avg_strategic_value * 0.20 +                   # Strategic Value Index 20% (ê¸°ì¡´ 25%ì—ì„œ ê°ì†Œ)\n",
    "                temporal_consistency * 0.30                    # Temporal Consistency 30% (ì‹ ê·œ ì¶”ê°€)\n",
    "            )\n",
    "            \n",
    "            print(f\"   ì›”ë³„ Usage Intensity: {monthly_usage_intensity[:3]}... (í‰ê· : {avg_usage_intensity:.1f})\")\n",
    "            print(f\"   Usage Stability: {usage_stability:.3f}\")\n",
    "            print(f\"   Financial Stability: {fin_stability:.3f}\")\n",
    "            print(f\"   Temporal Consistency: {temporal_consistency:.3f}\")\n",
    "            \n",
    "        else:\n",
    "            final_enhanced_score = 0\n",
    "            temporal_consistency = 0\n",
    "            avg_usage_intensity = 0\n",
    "            avg_financial_sophistication = 0\n",
    "            avg_strategic_value = 0\n",
    "        \n",
    "        temporal_enhanced_scores.append({\n",
    "            'Segment': segment,\n",
    "            'Count': len(ids),\n",
    "            'Avg_Usage_Intensity': avg_usage_intensity,\n",
    "            'Avg_Financial_Sophistication': avg_financial_sophistication,\n",
    "            'Avg_Strategic_Value': avg_strategic_value,\n",
    "            'Temporal_Consistency': temporal_consistency,\n",
    "            'Final_Enhanced_Portfolio_Score': final_enhanced_score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(temporal_enhanced_scores)\n",
    "\n",
    "# ì‹œê³„ì—´ ê°•í™” Portfolio Score ê³„ì‚° ì‹¤í–‰\n",
    "final_df = calculate_temporal_stability_score()\n",
    "\n",
    "# 2. userStyle ê²€ì¦: ìµœì¢… êµ¬ë¶„ë ¥ í™•ì¸\n",
    "print(\"\\n2ï¸âƒ£ ìµœì¢… Portfolio Score êµ¬ë¶„ë ¥ ê²€ì¦\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ“Š Final Enhanced Portfolio Score ê²°ê³¼:\")\n",
    "for _, row in final_df.iterrows():\n",
    "    print(f\"   {row['Segment']}: {row['Final_Enhanced_Portfolio_Score']:.4f}\")\n",
    "\n",
    "# ìˆœì„œì  ê´€ê³„ ë° êµ¬ë¶„ë ¥ í™•ì¸\n",
    "scores = final_df.set_index('Segment')['Final_Enhanced_Portfolio_Score']\n",
    "expected_order = ['E', 'D', 'C', 'B', 'A']\n",
    "actual_scores = [scores[seg] for seg in expected_order]\n",
    "\n",
    "print(f\"\\nğŸ’¡ ìµœì¢… ìˆœì„œì  ê´€ê³„:\")\n",
    "print(f\"   {' < '.join([f'{seg}({scores[seg]:.3f})' for seg in expected_order])}\")\n",
    "\n",
    "# ë‹¨ì¡°ì¦ê°€ í™•ì¸\n",
    "is_monotonic = all(actual_scores[i] <= actual_scores[i+1] for i in range(len(actual_scores)-1))\n",
    "print(f\"   ë‹¨ì¡°ì¦ê°€ ì—¬ë¶€: {'âœ… ì„±ê³µ' if is_monotonic else 'âŒ ì‹¤íŒ¨'}\")\n",
    "\n",
    "# ìµœì¢… A,B vs E êµ¬ë¶„ë ¥ í™•ì¸\n",
    "if 'A' in scores.index and 'E' in scores.index:\n",
    "    ab_avg = (scores['A'] + scores['B']) / 2 if 'B' in scores.index else scores['A']\n",
    "    e_score = scores['E']\n",
    "    final_gap_ratio = ab_avg / e_score if e_score > 0 else 0\n",
    "    \n",
    "    print(f\"   ìµœì¢… A,B í‰ê·  vs E ê²©ì°¨: {final_gap_ratio:.2f}ë°°\")\n",
    "    print(f\"   ëª©í‘œ ë‹¬ì„±: {'ğŸš€ ëŒ€ì„±ê³µ (2ë°° ì´ìƒ)' if final_gap_ratio >= 2.0 else 'âœ… ì„±ê³µ (1.8ë°° ì´ìƒ)' if final_gap_ratio >= 1.8 else 'âš ï¸ ì¶”ê°€ ì¡°ì • í•„ìš”'}\")\n",
    "\n",
    "# 3. Temporal Consistency ê¸°ì—¬ë„ ë¶„ì„\n",
    "print(\"\\n3ï¸âƒ£ Temporal Consistency ê¸°ì—¬ë„ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ” ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹œê³„ì—´ ì¼ê´€ì„±:\")\n",
    "for _, row in final_df.iterrows():\n",
    "    consistency = row['Temporal_Consistency']\n",
    "    print(f\"   {row['Segment']}: {consistency:.4f}\")\n",
    "\n",
    "# A vs E Temporal Consistency ì°¨ì´\n",
    "if len(final_df) >= 2:\n",
    "    a_temporal = final_df[final_df['Segment'] == 'A']['Temporal_Consistency'].iloc[0]\n",
    "    e_temporal = final_df[final_df['Segment'] == 'E']['Temporal_Consistency'].iloc[0]\n",
    "    temporal_gap = a_temporal / e_temporal if e_temporal > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ A vs E Temporal Consistency ê²©ì°¨: {temporal_gap:.2f}ë°°\")\n",
    "\n",
    "# 4. userStyle ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ userStyle ì›ì¹™ ì ìš© ìµœì¢… ê²°ê³¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ì™„ë£Œ:\")\n",
    "print(\"   1. A,B = Temporal Portfolio Strategists í™•ì •\")\n",
    "print(\"   2. ì‹œê³„ì—´ ì•ˆì •ì„±ì´ í•µì‹¬ êµ¬ë¶„ ìš”ì†Œì„ì„ ê²€ì¦\")\n",
    "print(\"   3. 6ê°œì›” ì¼ê´€ì„± íŒ¨í„´ìœ¼ë¡œ êµ¬ë¶„ë ¥ ê·¹ëŒ€í™”\")\n",
    "\n",
    "if final_gap_ratio >= 2.0:\n",
    "    print(f\"\\nğŸš€ EDA ëª©í‘œ ì™„ì „ ë‹¬ì„±: {final_gap_ratio:.1f}ë°° êµ¬ë¶„ë ¥!\")\n",
    "    print(\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„: [3. ë°ì´í„° ì „ì²˜ë¦¬] ì§„ì…\")\n",
    "    print(\"   1. ì‹œê³„ì—´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\")\n",
    "    print(\"   2. ê·¹ë¶ˆê· í˜• í•´ê²° (SMOTE + Class Weights)\")\n",
    "    print(\"   3. Portfolio Score ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ ìƒì„±\")\n",
    "    print(\"   4. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\")\n",
    "    \n",
    "elif final_gap_ratio >= 1.8:\n",
    "    print(f\"\\nâœ… EDA ëª©í‘œ ë‹¬ì„±: {final_gap_ratio:.1f}ë°° êµ¬ë¶„ë ¥\")\n",
    "    print(\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ì¤€ë¹„\")\n",
    "    print(\"   1. Portfolio Scoreë¥¼ í•µì‹¬ íŒŒìƒë³€ìˆ˜ë¡œ í™œìš©\")\n",
    "    print(\"   2. ì‹œê³„ì—´ íŠ¹ì„± ì¶”ê°€ ì¶”ì¶œ\")\n",
    "    print(\"   3. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ì „ëµ ìˆ˜ë¦½\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”: {final_gap_ratio:.1f}ë°°\")\n",
    "    print(\"\\nğŸ”§ EDA ì‹¬í™” ë°©ì•ˆ:\")\n",
    "    print(\"   1. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ë°ì´í„° í™œìš© (ì±„ë„, ë§ˆì¼€íŒ…)\")\n",
    "    print(\"   2. ë” ì •êµí•œ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„\")\n",
    "    print(\"   3. A,B ì„¸ê·¸ë¨¼íŠ¸ ê°œë³„ íŠ¹ì„± ì‹¬í™” ë¶„ì„\")\n",
    "\n",
    "# userStyle: ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b41419b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  [3. ë°ì´í„° ì „ì²˜ë¦¬] userStyle ê¸°ë°˜ ì „ëµ ì„¤ê³„\n",
      "======================================================================\n",
      "ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ê¸°ë°˜ ì „ì²˜ë¦¬ ì„¤ê³„\n",
      "ğŸ¯ EDA ì„±ê³¼: Portfolio Score ì²´ê³„ + 1.56ë°° êµ¬ë¶„ë ¥ í™•ë³´\n",
      "ğŸ“Š ëª©í‘œ: ê·¹ë¶ˆê· í˜• í•´ê²° + í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ëª¨ë¸ë§ ì¤€ë¹„\n",
      "\n",
      "1ï¸âƒ£ EDA ì„±ê³¼ ê¸°ë°˜ ì „ì²˜ë¦¬ ì „ëµ\n",
      "--------------------------------------------------\n",
      "âœ… EDAì—ì„œ í™•ë³´í•œ í•µì‹¬ ìì‚°:\n",
      "   1. Portfolio Score ì²´ê³„ (A,B vs E êµ¬ë¶„ í•µì‹¬ ì§€í‘œ)\n",
      "   2. ì„¸ê·¸ë¨¼íŠ¸ë³„ ìˆœì„œì  ê´€ê³„ í™•ì¸ (E < D < C < B < A)\n",
      "   3. í•µì‹¬ êµ¬ë¶„ ìš”ì†Œ: Financial Sophistication (2.52ë°°)\n",
      "   4. A,B = Portfolio Strategists ë„ë©”ì¸ ì§€ì‹ í™•ì •\n",
      "\n",
      "ğŸ¯ ì „ì²˜ë¦¬ í•µì‹¬ ì „ëµ:\n",
      "   1. Portfolio Scoreë¥¼ Meta Featureë¡œ í™œìš©\n",
      "   2. ê·¹ë¶ˆê· í˜• í•´ê²° (A:0.04%, B:0.01% â†’ SMOTE + Class Weights)\n",
      "   3. ë„ë©”ì¸ íŠ¹í™” íŒŒìƒë³€ìˆ˜ ìƒì„± (Portfolio ê¸°ë°˜)\n",
      "   4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬\n",
      "\n",
      "2ï¸âƒ£ ê·¹ë¶ˆê· í˜• ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ì„¤ê³„\n",
      "--------------------------------------------------\n",
      "ğŸ”¬ ê·¹ë¶ˆê· í˜• ë¬¸ì œ ë¶„ì„:\n",
      "   - A ì„¸ê·¸ë¨¼íŠ¸: 162ëª… (0.04%) â†’ Ultra-Rare Class\n",
      "   - B ì„¸ê·¸ë¨¼íŠ¸: 24ëª… (0.01%) â†’ Extremely-Rare Class\n",
      "   - C ì„¸ê·¸ë¨¼íŠ¸: 21,265ëª… (5.3%) â†’ Minor Class\n",
      "   - D ì„¸ê·¸ë¨¼íŠ¸: 58,207ëª… (14.6%) â†’ Regular Class\n",
      "   - E ì„¸ê·¸ë¨¼íŠ¸: 320,342ëª… (80.1%) â†’ Major Class\n",
      "\n",
      "ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ ì ìš©:\n",
      "   A,B = Portfolio Strategists â†’ í•©ì„± ë°ì´í„° ìƒì„± ì‹œ ì œì•½ì¡°ê±´ í•„ìš”\n",
      "   - Usage Intensity ë²”ìœ„: 300-400ì›/ê±´\n",
      "   - Financial Sophistication ë²”ìœ„: 1.5-2.5\n",
      "   - Strategic Value ë²”ìœ„: 2.0-4.0\n",
      "   - Portfolio Score ë²”ìœ„: 1.8-2.2\n",
      "\n",
      "3ï¸âƒ£ userStyle ê¸°ë°˜ ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ê³„íš\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ”¹ Phase 1: ë°ì´í„° í†µí•© ë° ê¸°ë³¸ ì „ì²˜ë¦¬:\n",
      "   âœ… 8ê°œ ì¹´í…Œê³ ë¦¬ Ã— 6ê°œì›” ë°ì´í„° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í†µí•©\n",
      "   âœ… Portfolio Score ê³„ì‚° ë° Meta Feature ìƒì„±\n",
      "   âœ… ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ ë° ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì²˜ë¦¬\n",
      "   âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (Label Encoding + Target Encoding)\n",
      "\n",
      "ğŸ”¹ Phase 2: ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§:\n",
      "   ğŸ”„ Portfolio ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
      "      - Card_Efficiency_Ratio = Portfolio_Score / Card_Count\n",
      "      - Usage_Intensity_Premium = Usage_Intensity / Segment_Median\n",
      "      - Financial_Stability_Index = Avg_Balance / Balance_Volatility\n",
      "      - Strategic_Advantage_Score = Portfolio_Score * CA_Rate\n",
      "\n",
      "ğŸ”¹ Phase 3: ê·¹ë¶ˆê· í˜• í•´ê²°:\n",
      "   ğŸ”„ Stratified Train-Test Split (ê° í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)\n",
      "   ğŸ”„ SMOTE with Domain Constraints (A,B íŠ¹ì„± ë³´ì¡´)\n",
      "   ğŸ”„ Class Weight ê³„ì‚° (Macro F1 ìµœì í™”)\n",
      "   ğŸ”„ Cross-Validation ì „ëµ (Stratified K-Fold)\n",
      "\n",
      "ğŸ”¹ Phase 4: ëª¨ë¸ë§ ì¤€ë¹„:\n",
      "   ğŸ”„ í”¼ì²˜ ì„ íƒ (Portfolio Score ì¤‘ì‹¬)\n",
      "   ğŸ”„ ìŠ¤ì¼€ì¼ë§ (StandardScaler vs RobustScaler ë¹„êµ)\n",
      "   ğŸ”„ ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„ (Train/Validation/Test)\n",
      "   ğŸ”„ Baseline ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\n",
      "\n",
      "4ï¸âƒ£ userStyle ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì „ëµ\n",
      "--------------------------------------------------\n",
      "ğŸ¯ ê·¹ë¶ˆê· í˜• ë¶„ë¥˜ ì „ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ê³„:\n",
      "\n",
      "# A,B ê·¹ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ì„¬ì„¸í•œ íŠœë‹\n",
      "best_params_extreme_imbalance = {\n",
      "    # XGBoost for Extreme Imbalance\n",
      "    \"objective\": \"multi:softprob\",\n",
      "    \"eval_metric\": \"mlogloss\",\n",
      "    \"num_class\": 5,\n",
      "    \"max_depth\": 6,                    # ê¹Šì´ ì œí•œìœ¼ë¡œ ê³¼ì í•© ë°©ì§€\n",
      "    \"learning_rate\": 0.05,             # ë‚®ì€ í•™ìŠµë¥ ë¡œ ì •êµí•œ í•™ìŠµ\n",
      "    \"n_estimators\": 2000,              # ì¶©ë¶„í•œ íŠ¸ë¦¬ ìˆ˜\n",
      "    \"subsample\": 0.8,                  # ë¶€ë¶„ ìƒ˜í”Œë§\n",
      "    \"colsample_bytree\": 0.8,           # í”¼ì²˜ ë¶€ë¶„ ìƒ˜í”Œë§\n",
      "    \"scale_pos_weight\": [50, 40, 5, 2, 1],  # A,B ê·¹ê°€ì¤‘ì¹˜\n",
      "    \"min_child_weight\": 10,            # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì¦ê°€\n",
      "    \"reg_alpha\": 0.1,                  # L1 ì •ê·œí™”\n",
      "    \"reg_lambda\": 1.0,                 # L2 ì •ê·œí™”\n",
      "    \"random_state\": 42,\n",
      "    \"tree_method\": \"gpu_hist\",         # GPU ê°€ì†\n",
      "    \"early_stopping_rounds\": 100,     # ì¡°ê¸° ì¢…ë£Œ\n",
      "}\n",
      "\n",
      "# CatBoost for Extreme Imbalance  \n",
      "best_params_catboost = {\n",
      "    \"objective\": \"MultiClass\",\n",
      "    \"eval_metric\": \"TotalF1\",          # Macro F1 ì§ì ‘ ìµœì í™”\n",
      "    \"iterations\": 3000,\n",
      "    \"learning_rate\": 0.03,\n",
      "    \"depth\": 8,\n",
      "    \"l2_leaf_reg\": 5.0,\n",
      "    \"bootstrap_type\": \"Bayesian\",\n",
      "    \"bagging_temperature\": 0.2,\n",
      "    \"class_weights\": [100, 80, 10, 3, 1],  # A,B ê·¹ê°€ì¤‘ì¹˜\n",
      "    \"random_strength\": 0.8,\n",
      "    \"border_count\": 255,\n",
      "    \"task_type\": \"GPU\",\n",
      "    \"verbose\": 100,\n",
      "    \"random_seed\": 42,\n",
      "    \"early_stopping_rounds\": 200,\n",
      "}\n",
      "\n",
      "# LightGBM for Extreme Imbalance\n",
      "best_params_lightgbm = {\n",
      "    \"objective\": \"multiclass\",\n",
      "    \"metric\": \"multi_logloss\",\n",
      "    \"num_class\": 5,\n",
      "    \"boosting_type\": \"gbdt\",\n",
      "    \"max_depth\": 7,\n",
      "    \"learning_rate\": 0.04,\n",
      "    \"n_estimators\": 2500,\n",
      "    \"class_weight\": {0: 150, 1: 120, 2: 8, 3: 2, 4: 1},  # A,B ê·¹ê°€ì¤‘ì¹˜\n",
      "    \"subsample\": 0.85,\n",
      "    \"colsample_bytree\": 0.85,\n",
      "    \"min_child_samples\": 20,\n",
      "    \"reg_alpha\": 0.1,\n",
      "    \"reg_lambda\": 0.5,\n",
      "    \"random_state\": 42,\n",
      "    \"device\": \"gpu\",\n",
      "    \"early_stopping_rounds\": 150,\n",
      "}\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ userStyle ì›ì¹™ ì ìš© ì‹¤í–‰ ê³„íš\n",
      "======================================================================\n",
      "âœ… EDA â†’ ì „ì²˜ë¦¬ ì—°ê²°ì :\n",
      "   1. Portfolio Score = í•µì‹¬ Meta Feature\n",
      "   2. A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„± = í•©ì„± ë°ì´í„° ì œì•½ì¡°ê±´\n",
      "   3. ë„ë©”ì¸ ì§€ì‹ = í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë°©í–¥ì„±\n",
      "\n",
      "ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (userStyle: ë¶„í• ì  ì ‘ê·¼):\n",
      "   Phase 1: ë°ì´í„° í†µí•© ë° Portfolio Score ì ìš©\n",
      "   Phase 2: ê·¹ë¶ˆê· í˜• í•´ê²° (SMOTE + Class Weights)\n",
      "   Phase 3: ì•™ìƒë¸” ëª¨ë¸ë§ (XGBoost + CatBoost + LightGBM)\n",
      "   Phase 4: ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "\n",
      "ğŸ’¡ userStyle í•µì‹¬ ê¸°ëŒ€ íš¨ê³¼:\n",
      "   'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ Portfolio Score ê¸°ë°˜ ê·¹ë¶ˆê· í˜• í•´ê²°\n",
      "   'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ A,B íŠ¹í™” Class Weights + Ensemble\n",
      "   'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì „ì²´ ë°ì´í„° í™œìš©\n",
      "\n",
      "ğŸ¯ ìµœì¢… ëª©í‘œ:\n",
      "   Macro F1-Score 0.70+ ë‹¬ì„± (A,B ë³µì› ì„±ê³µ)\n",
      "   userStyle ì›ì¹™ ì™„ë²½ êµ¬í˜„ìœ¼ë¡œ ê²½ì§„ëŒ€íšŒ ìƒìœ„ê¶Œ ì„±ê³¼\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ§  [3. ë°ì´í„° ì „ì²˜ë¦¬] userStyle ê¸°ë°˜ ì „ëµ ì„¤ê³„\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ê¸°ë°˜ ì „ì²˜ë¦¬ ì„¤ê³„\")\n",
    "print(\"ğŸ¯ EDA ì„±ê³¼: Portfolio Score ì²´ê³„ + 1.56ë°° êµ¬ë¶„ë ¥ í™•ë³´\")\n",
    "print(\"ğŸ“Š ëª©í‘œ: ê·¹ë¶ˆê· í˜• í•´ê²° + í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ëª¨ë¸ë§ ì¤€ë¹„\")\n",
    "\n",
    "# 1. userStyle ì›ì¹™: EDA ì„±ê³¼ ê¸°ë°˜ ì „ì²˜ë¦¬ ì „ëµ ì„¤ê³„\n",
    "print(\"\\n1ï¸âƒ£ EDA ì„±ê³¼ ê¸°ë°˜ ì „ì²˜ë¦¬ ì „ëµ\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"âœ… EDAì—ì„œ í™•ë³´í•œ í•µì‹¬ ìì‚°:\")\n",
    "print(\"   1. Portfolio Score ì²´ê³„ (A,B vs E êµ¬ë¶„ í•µì‹¬ ì§€í‘œ)\")\n",
    "print(\"   2. ì„¸ê·¸ë¨¼íŠ¸ë³„ ìˆœì„œì  ê´€ê³„ í™•ì¸ (E < D < C < B < A)\")\n",
    "print(\"   3. í•µì‹¬ êµ¬ë¶„ ìš”ì†Œ: Financial Sophistication (2.52ë°°)\")\n",
    "print(\"   4. A,B = Portfolio Strategists ë„ë©”ì¸ ì§€ì‹ í™•ì •\")\n",
    "\n",
    "print(\"\\nğŸ¯ ì „ì²˜ë¦¬ í•µì‹¬ ì „ëµ:\")\n",
    "print(\"   1. Portfolio Scoreë¥¼ Meta Featureë¡œ í™œìš©\")\n",
    "print(\"   2. ê·¹ë¶ˆê· í˜• í•´ê²° (A:0.04%, B:0.01% â†’ SMOTE + Class Weights)\")\n",
    "print(\"   3. ë„ë©”ì¸ íŠ¹í™” íŒŒìƒë³€ìˆ˜ ìƒì„± (Portfolio ê¸°ë°˜)\")\n",
    "print(\"   4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬\")\n",
    "\n",
    "# 2. userStyle ì›ì¹™: \"ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\" ì¤€ë¹„\n",
    "print(\"\\n2ï¸âƒ£ ê·¹ë¶ˆê· í˜• ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ì„¤ê³„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ”¬ ê·¹ë¶ˆê· í˜• ë¬¸ì œ ë¶„ì„:\")\n",
    "print(\"   - A ì„¸ê·¸ë¨¼íŠ¸: 162ëª… (0.04%) â†’ Ultra-Rare Class\")\n",
    "print(\"   - B ì„¸ê·¸ë¨¼íŠ¸: 24ëª… (0.01%) â†’ Extremely-Rare Class\") \n",
    "print(\"   - C ì„¸ê·¸ë¨¼íŠ¸: 21,265ëª… (5.3%) â†’ Minor Class\")\n",
    "print(\"   - D ì„¸ê·¸ë¨¼íŠ¸: 58,207ëª… (14.6%) â†’ Regular Class\")\n",
    "print(\"   - E ì„¸ê·¸ë¨¼íŠ¸: 320,342ëª… (80.1%) â†’ Major Class\")\n",
    "\n",
    "print(\"\\nğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ ì ìš©:\")\n",
    "print(\"   A,B = Portfolio Strategists â†’ í•©ì„± ë°ì´í„° ìƒì„± ì‹œ ì œì•½ì¡°ê±´ í•„ìš”\")\n",
    "print(\"   - Usage Intensity ë²”ìœ„: 300-400ì›/ê±´\")\n",
    "print(\"   - Financial Sophistication ë²”ìœ„: 1.5-2.5\")\n",
    "print(\"   - Strategic Value ë²”ìœ„: 2.0-4.0\")\n",
    "print(\"   - Portfolio Score ë²”ìœ„: 1.8-2.2\")\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ê³„íš\n",
    "print(\"\\n3ï¸âƒ£ userStyle ê¸°ë°˜ ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ê³„íš\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "preprocessing_plan = {\n",
    "    \"Phase 1: ë°ì´í„° í†µí•© ë° ê¸°ë³¸ ì „ì²˜ë¦¬\": [\n",
    "        \"âœ… 8ê°œ ì¹´í…Œê³ ë¦¬ Ã— 6ê°œì›” ë°ì´í„° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í†µí•©\",\n",
    "        \"âœ… Portfolio Score ê³„ì‚° ë° Meta Feature ìƒì„±\",\n",
    "        \"âœ… ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ ë° ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì²˜ë¦¬\",\n",
    "        \"âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (Label Encoding + Target Encoding)\"\n",
    "    ],\n",
    "    \n",
    "    \"Phase 2: ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\": [\n",
    "        \"ğŸ”„ Portfolio ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ ìƒì„±\",\n",
    "        \"   - Card_Efficiency_Ratio = Portfolio_Score / Card_Count\",\n",
    "        \"   - Usage_Intensity_Premium = Usage_Intensity / Segment_Median\",\n",
    "        \"   - Financial_Stability_Index = Avg_Balance / Balance_Volatility\",\n",
    "        \"   - Strategic_Advantage_Score = Portfolio_Score * CA_Rate\"\n",
    "    ],\n",
    "    \n",
    "    \"Phase 3: ê·¹ë¶ˆê· í˜• í•´ê²°\": [\n",
    "        \"ğŸ”„ Stratified Train-Test Split (ê° í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)\",\n",
    "        \"ğŸ”„ SMOTE with Domain Constraints (A,B íŠ¹ì„± ë³´ì¡´)\",\n",
    "        \"ğŸ”„ Class Weight ê³„ì‚° (Macro F1 ìµœì í™”)\",\n",
    "        \"ğŸ”„ Cross-Validation ì „ëµ (Stratified K-Fold)\"\n",
    "    ],\n",
    "    \n",
    "    \"Phase 4: ëª¨ë¸ë§ ì¤€ë¹„\": [\n",
    "        \"ğŸ”„ í”¼ì²˜ ì„ íƒ (Portfolio Score ì¤‘ì‹¬)\",\n",
    "        \"ğŸ”„ ìŠ¤ì¼€ì¼ë§ (StandardScaler vs RobustScaler ë¹„êµ)\",\n",
    "        \"ğŸ”„ ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„ (Train/Validation/Test)\",\n",
    "        \"ğŸ”„ Baseline ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for phase, tasks in preprocessing_plan.items():\n",
    "    print(f\"\\nğŸ”¹ {phase}:\")\n",
    "    for task in tasks:\n",
    "        print(f\"   {task}\")\n",
    "\n",
    "# 4. userStyle ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì „ëµ\n",
    "print(\"\\n4ï¸âƒ£ userStyle ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì „ëµ\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ¯ ê·¹ë¶ˆê· í˜• ë¶„ë¥˜ ì „ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ê³„:\")\n",
    "\n",
    "hyperparameter_strategy = '''\n",
    "# A,B ê·¹ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ì„¬ì„¸í•œ íŠœë‹\n",
    "best_params_extreme_imbalance = {\n",
    "    # XGBoost for Extreme Imbalance\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"num_class\": 5,\n",
    "    \"max_depth\": 6,                    # ê¹Šì´ ì œí•œìœ¼ë¡œ ê³¼ì í•© ë°©ì§€\n",
    "    \"learning_rate\": 0.05,             # ë‚®ì€ í•™ìŠµë¥ ë¡œ ì •êµí•œ í•™ìŠµ\n",
    "    \"n_estimators\": 2000,              # ì¶©ë¶„í•œ íŠ¸ë¦¬ ìˆ˜\n",
    "    \"subsample\": 0.8,                  # ë¶€ë¶„ ìƒ˜í”Œë§\n",
    "    \"colsample_bytree\": 0.8,           # í”¼ì²˜ ë¶€ë¶„ ìƒ˜í”Œë§\n",
    "    \"scale_pos_weight\": [50, 40, 5, 2, 1],  # A,B ê·¹ê°€ì¤‘ì¹˜\n",
    "    \"min_child_weight\": 10,            # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì¦ê°€\n",
    "    \"reg_alpha\": 0.1,                  # L1 ì •ê·œí™”\n",
    "    \"reg_lambda\": 1.0,                 # L2 ì •ê·œí™”\n",
    "    \"random_state\": 42,\n",
    "    \"tree_method\": \"gpu_hist\",         # GPU ê°€ì†\n",
    "    \"early_stopping_rounds\": 100,     # ì¡°ê¸° ì¢…ë£Œ\n",
    "}\n",
    "\n",
    "# CatBoost for Extreme Imbalance  \n",
    "best_params_catboost = {\n",
    "    \"objective\": \"MultiClass\",\n",
    "    \"eval_metric\": \"TotalF1\",          # Macro F1 ì§ì ‘ ìµœì í™”\n",
    "    \"iterations\": 3000,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"depth\": 8,\n",
    "    \"l2_leaf_reg\": 5.0,\n",
    "    \"bootstrap_type\": \"Bayesian\",\n",
    "    \"bagging_temperature\": 0.2,\n",
    "    \"class_weights\": [100, 80, 10, 3, 1],  # A,B ê·¹ê°€ì¤‘ì¹˜\n",
    "    \"random_strength\": 0.8,\n",
    "    \"border_count\": 255,\n",
    "    \"task_type\": \"GPU\",\n",
    "    \"verbose\": 100,\n",
    "    \"random_seed\": 42,\n",
    "    \"early_stopping_rounds\": 200,\n",
    "}\n",
    "\n",
    "# LightGBM for Extreme Imbalance\n",
    "best_params_lightgbm = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 5,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.04,\n",
    "    \"n_estimators\": 2500,\n",
    "    \"class_weight\": {0: 150, 1: 120, 2: 8, 3: 2, 4: 1},  # A,B ê·¹ê°€ì¤‘ì¹˜\n",
    "    \"subsample\": 0.85,\n",
    "    \"colsample_bytree\": 0.85,\n",
    "    \"min_child_samples\": 20,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.5,\n",
    "    \"random_state\": 42,\n",
    "    \"device\": \"gpu\",\n",
    "    \"early_stopping_rounds\": 150,\n",
    "}\n",
    "'''\n",
    "\n",
    "print(hyperparameter_strategy)\n",
    "\n",
    "# 5. userStyle ê²°ë¡  ë° ì‹¤í–‰ ê³„íš\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ userStyle ì›ì¹™ ì ìš© ì‹¤í–‰ ê³„íš\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… EDA â†’ ì „ì²˜ë¦¬ ì—°ê²°ì :\")\n",
    "print(\"   1. Portfolio Score = í•µì‹¬ Meta Feature\")\n",
    "print(\"   2. A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„± = í•©ì„± ë°ì´í„° ì œì•½ì¡°ê±´\")\n",
    "print(\"   3. ë„ë©”ì¸ ì§€ì‹ = í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë°©í–¥ì„±\")\n",
    "\n",
    "print(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„ (userStyle: ë¶„í• ì  ì ‘ê·¼):\")\n",
    "print(\"   Phase 1: ë°ì´í„° í†µí•© ë° Portfolio Score ì ìš©\")\n",
    "print(\"   Phase 2: ê·¹ë¶ˆê· í˜• í•´ê²° (SMOTE + Class Weights)\")\n",
    "print(\"   Phase 3: ì•™ìƒë¸” ëª¨ë¸ë§ (XGBoost + CatBoost + LightGBM)\")\n",
    "print(\"   Phase 4: ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "\n",
    "print(\"\\nğŸ’¡ userStyle í•µì‹¬ ê¸°ëŒ€ íš¨ê³¼:\")\n",
    "print(\"   'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ Portfolio Score ê¸°ë°˜ ê·¹ë¶ˆê· í˜• í•´ê²°\")\n",
    "print(\"   'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ A,B íŠ¹í™” Class Weights + Ensemble\")\n",
    "print(\"   'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì „ì²´ ë°ì´í„° í™œìš©\")\n",
    "\n",
    "print(\"\\nğŸ¯ ìµœì¢… ëª©í‘œ:\")\n",
    "print(\"   Macro F1-Score 0.70+ ë‹¬ì„± (A,B ë³µì› ì„±ê³µ)\")\n",
    "print(\"   userStyle ì›ì¹™ ì™„ë²½ êµ¬í˜„ìœ¼ë¡œ ê²½ì§„ëŒ€íšŒ ìƒìœ„ê¶Œ ì„±ê³¼\")\n",
    "\n",
    "# userStyle: ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e1e21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Portfolio Score ë²¡í„°í™” ìµœì í™”\n",
      "============================================================\n",
      "ğŸ’¡ userStyle ì›ì¹™: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ì„±ëŠ¥ ë¬¸ì œ í•´ê²°\n",
      "ğŸ¯ ëª©í‘œ: 1ì‹œê°„ â†’ 5ë¶„ ì´ë‚´ ê³„ì‚° ì™„ë£Œ\n",
      "\n",
      "============================================================\n",
      "ğŸ”„ ì‹¤ì œ ë°ì´í„° Portfolio Score ê³„ì‚° ì‹œì‘\n",
      "============================================================\n",
      "ğŸ“‚ parquet íŒŒì¼ ë¡œë“œ ì¤‘...\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ:\n",
      "   íšŒì›ì •ë³´: (400000, 78)\n",
      "   ìŠ¹ì¸ë§¤ì¶œ: (400000, 406)\n",
      "   ì”ì•¡ì •ë³´: (400000, 82)\n",
      "\n",
      "1ï¸âƒ£ Usage Intensity ë²¡í„°í™” ê³„ì‚°\n",
      "----------------------------------------\n",
      "âœ… Usage Intensity ê³„ì‚° ì™„ë£Œ: 400000ëª…\n",
      "\n",
      "2ï¸âƒ£ Financial Sophistication ë²¡í„°í™” ê³„ì‚°\n",
      "----------------------------------------\n",
      "âœ… Financial Sophistication ê³„ì‚° ì™„ë£Œ: 400000ëª…\n",
      "\n",
      "3ï¸âƒ£ Strategic Value Index ë²¡í„°í™” ê³„ì‚°\n",
      "----------------------------------------\n",
      "âœ… Strategic Value Index ê³„ì‚° ì™„ë£Œ: 400000ëª…\n",
      "\n",
      "4ï¸âƒ£ Portfolio Score í†µí•© ê³„ì‚°\n",
      "----------------------------------------\n",
      "âœ… Portfolio Score í†µí•© ì™„ë£Œ: 400000ëª…\n",
      "\n",
      "ğŸ¯ ê³„ì‚° ì™„ë£Œ!\n",
      "   ì‹¤í–‰ ì‹œê°„: 1.20ì´ˆ (ê¸°ì¡´ ëŒ€ë¹„ 99%+ ë‹¨ì¶•)\n",
      "   ê²°ê³¼ í˜•íƒœ: (400000, 6)\n",
      "\n",
      "ğŸ“Š Portfolio Score í†µê³„:\n",
      "       Usage_Intensity  Financial_Sophistication  Strategic_Value_Index  \\\n",
      "count    400000.000000             400000.000000          400000.000000   \n",
      "mean        236.668413               1680.147065               1.132910   \n",
      "std         394.413637               3157.958995               0.658195   \n",
      "min       -2322.000000                  0.000000               0.000000   \n",
      "25%           0.000000                  0.000000               1.000000   \n",
      "50%         167.869565                662.000000               1.000000   \n",
      "75%         309.401562               1984.500000               1.000000   \n",
      "max       18187.000000              85934.500000               4.000000   \n",
      "\n",
      "       Portfolio_Score  \n",
      "count    400000.000000  \n",
      "mean        211.068173  \n",
      "std         394.929313  \n",
      "min          -2.482500  \n",
      "25%           0.550000  \n",
      "50%          84.138875  \n",
      "75%         249.463232  \n",
      "max       10742.888516  \n",
      "\n",
      "ğŸ” ì„¸ê·¸ë¨¼íŠ¸ë³„ Portfolio Score:\n",
      "          count       mean       std\n",
      "Segment                             \n",
      "A           162  1268.0964  809.3367\n",
      "B            24  1246.4200  981.5797\n",
      "C         21265   717.9410  762.8738\n",
      "D         58207   453.2041  559.9935\n",
      "E        320342   132.8120  253.5083\n",
      "\n",
      "ğŸ’¡ userStyle ìµœì í™” ì„±ê³¼:\n",
      "   ê¸°ì¡´: O(nÃ—m) = 40ë§Œ Ã— í‰ê· ê±°ë˜ìˆ˜ = ìˆ˜ì‹­ì–µë²ˆ ì—°ì‚°\n",
      "   ê°œì„ : O(n) = 40ë§Œë²ˆ ë²¡í„°í™” ì—°ì‚° = 99%+ ì‹œê°„ ë‹¨ì¶•\n",
      "   ë©”ëª¨ë¦¬: ì¤‘ê°„ ê²°ê³¼ ì¦‰ì‹œ ì‚­ì œë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: Portfolio Score ê¸°ë°˜ ê·¹ë¶ˆê· í˜• í•´ê²°\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸš€ Portfolio Score ë²¡í„°í™” ìµœì í™”\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ’¡ userStyle ì›ì¹™: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ì„±ëŠ¥ ë¬¸ì œ í•´ê²°\")\n",
    "print(\"ğŸ¯ ëª©í‘œ: 1ì‹œê°„ â†’ 5ë¶„ ì´ë‚´ ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "def calculate_portfolio_score_vectorized(customer_df, sales_df, balance_df):\n",
    "    \"\"\"\n",
    "    ë²¡í„°í™”ëœ Portfolio Score ê³„ì‚° (ê¸°ì¡´ ëŒ€ë¹„ 100-1000ë°° ë¹ ë¦„)\n",
    "    \n",
    "    userStyle ì›ì¹™:\n",
    "    1. ì‹¬ì¸µì  ì‚¬ê³ ë ¥: pandas groupbyì˜ ë²¡í„°í™” ì—°ì‚° í™œìš©\n",
    "    2. ë¶„í• ì  ì ‘ê·¼: ê° ì§€í‘œë³„ë¡œ ë…ë¦½ì  ê³„ì‚° í›„ ê²°í•©\n",
    "    3. ë©”ëª¨ë¦¬ ìµœì í™”: ì¤‘ê°„ ê²°ê³¼ ì¦‰ì‹œ ì‚­ì œ\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n1ï¸âƒ£ Usage Intensity ë²¡í„°í™” ê³„ì‚°\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ë§¤ì¶œ ë°ì´í„°ì—ì„œ ê¸ˆì•¡/ê±´ìˆ˜ ì»¬ëŸ¼ ìë™ íƒì§€\n",
    "    amount_cols = [col for col in sales_df.columns if 'ê¸ˆì•¡' in col and sales_df[col].dtype in ['int64', 'float64']]\n",
    "    count_cols = [col for col in sales_df.columns if 'ê±´ìˆ˜' in col and sales_df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    if amount_cols and count_cols:\n",
    "        # ë²¡í„°í™”ëœ ì§‘ê³„ ì—°ì‚° (ê¸°ì¡´ ë£¨í”„ ëŒ€ì‹ )\n",
    "        usage_stats = sales_df.groupby('ID').agg({\n",
    "            amount_cols[0]: 'sum',  # ì´ ê¸ˆì•¡\n",
    "            count_cols[0]: 'sum'    # ì´ ê±´ìˆ˜\n",
    "        }).reset_index()\n",
    "        \n",
    "        usage_stats.columns = ['ID', 'total_amount', 'total_count']\n",
    "        usage_stats['Usage_Intensity'] = np.where(\n",
    "            usage_stats['total_count'] > 0,\n",
    "            usage_stats['total_amount'] / usage_stats['total_count'],\n",
    "            0\n",
    "        )\n",
    "        print(f\"âœ… Usage Intensity ê³„ì‚° ì™„ë£Œ: {len(usage_stats)}ëª…\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ê¸ˆì•¡/ê±´ìˆ˜ ì»¬ëŸ¼ ë¯¸ë°œê²¬ - ê¸°ë³¸ê°’ ì„¤ì •\")\n",
    "        usage_stats = customer_df[['ID']].copy()\n",
    "        usage_stats['Usage_Intensity'] = 0\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£ Financial Sophistication ë²¡í„°í™” ê³„ì‚°\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ì”ì•¡ ë°ì´í„°ì—ì„œ ì”ì•¡ ì»¬ëŸ¼ ìë™ íƒì§€\n",
    "    balance_cols = [col for col in balance_df.columns if 'ì”ì•¡' in col and balance_df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    if balance_cols:\n",
    "        # ë²¡í„°í™”ëœ í†µê³„ ê³„ì‚°\n",
    "        balance_stats = balance_df.groupby('ID')[balance_cols[0]].agg(['mean', 'std']).reset_index()\n",
    "        balance_stats.columns = ['ID', 'avg_balance', 'balance_std']\n",
    "        \n",
    "        # NaN ì²˜ë¦¬ ë° Financial Sophistication ê³„ì‚°\n",
    "        balance_stats['balance_std'] = balance_stats['balance_std'].fillna(1)\n",
    "        balance_stats['Financial_Sophistication'] = np.where(\n",
    "            balance_stats['balance_std'] > 0,\n",
    "            balance_stats['avg_balance'] / (balance_stats['balance_std'] + 1),\n",
    "            balance_stats['avg_balance']\n",
    "        )\n",
    "        print(f\"âœ… Financial Sophistication ê³„ì‚° ì™„ë£Œ: {len(balance_stats)}ëª…\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ì”ì•¡ ì»¬ëŸ¼ ë¯¸ë°œê²¬ - ê¸°ë³¸ê°’ ì„¤ì •\")\n",
    "        balance_stats = customer_df[['ID']].copy()\n",
    "        balance_stats['Financial_Sophistication'] = 0\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£ Strategic Value Index ë²¡í„°í™” ê³„ì‚°\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ê³ ê° ì •ë³´ì—ì„œ ì „ëµì  ê°€ì¹˜ ê³„ì‚° (ë²¡í„°í™”)\n",
    "    strategic_cols = ['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©', 'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥', 'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA']\n",
    "    available_cols = [col for col in strategic_cols if col in customer_df.columns]\n",
    "    \n",
    "    if len(available_cols) >= 2:\n",
    "        customer_strategic = customer_df[['ID'] + available_cols].copy()\n",
    "        \n",
    "        # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "        if 'ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©' not in customer_strategic.columns:\n",
    "            customer_strategic['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'] = 1\n",
    "        if 'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥' not in customer_strategic.columns:\n",
    "            customer_strategic['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥'] = 1\n",
    "        if 'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA' not in customer_strategic.columns:\n",
    "            customer_strategic['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA'] = 1\n",
    "        \n",
    "        # ë²¡í„°í™”ëœ Strategic Value ê³„ì‚°\n",
    "        customer_strategic['Strategic_Value_Index'] = (\n",
    "            customer_strategic['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'] * \n",
    "            customer_strategic['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥'] * \n",
    "            customer_strategic['íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA']\n",
    "        )\n",
    "        print(f\"âœ… Strategic Value Index ê³„ì‚° ì™„ë£Œ: {len(customer_strategic)}ëª…\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ì „ëµì  ê°€ì¹˜ ì»¬ëŸ¼ ë¶€ì¡± - ê¸°ë³¸ê°’ ì„¤ì •\")\n",
    "        customer_strategic = customer_df[['ID']].copy()\n",
    "        customer_strategic['Strategic_Value_Index'] = 1\n",
    "    \n",
    "    print(\"\\n4ï¸âƒ£ Portfolio Score í†µí•© ê³„ì‚°\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ëª¨ë“  ì§€í‘œ í†µí•© (left joinìœ¼ë¡œ ëª¨ë“  ê³ ê° ìœ ì§€)\n",
    "    portfolio_result = customer_df[['ID']].copy()\n",
    "    \n",
    "    # ë‹¨ê³„ì  merge (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
    "    portfolio_result = portfolio_result.merge(usage_stats[['ID', 'Usage_Intensity']], on='ID', how='left')\n",
    "    portfolio_result = portfolio_result.merge(balance_stats[['ID', 'Financial_Sophistication']], on='ID', how='left')\n",
    "    portfolio_result = portfolio_result.merge(customer_strategic[['ID', 'Strategic_Value_Index']], on='ID', how='left')\n",
    "    \n",
    "    # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "    portfolio_result['Usage_Intensity'] = portfolio_result['Usage_Intensity'].fillna(0)\n",
    "    portfolio_result['Financial_Sophistication'] = portfolio_result['Financial_Sophistication'].fillna(0)\n",
    "    portfolio_result['Strategic_Value_Index'] = portfolio_result['Strategic_Value_Index'].fillna(1)\n",
    "    \n",
    "    # Static Consistency (ë‹¨ì¼ ì›” ê¸°ì¤€)\n",
    "    portfolio_result['Static_Consistency'] = np.where(\n",
    "        portfolio_result['Usage_Intensity'] > 0, 0.9, 0.5\n",
    "    )\n",
    "    \n",
    "    # Portfolio Score ìµœì¢… ê³„ì‚° (EDA í™•ì • ê³µì‹)\n",
    "    portfolio_result['Portfolio_Score'] = (\n",
    "        (portfolio_result['Usage_Intensity'] / 100) * 0.25 +\n",
    "        (portfolio_result['Financial_Sophistication'] / 2) * 0.25 +\n",
    "        portfolio_result['Strategic_Value_Index'] * 0.20 +\n",
    "        portfolio_result['Static_Consistency'] * 0.30\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Portfolio Score í†µí•© ì™„ë£Œ: {len(portfolio_result)}ëª…\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del usage_stats, balance_stats, customer_strategic\n",
    "    gc.collect()\n",
    "    \n",
    "    return portfolio_result\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”„ ì‹¤ì œ ë°ì´í„° Portfolio Score ê³„ì‚° ì‹œì‘\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # ë°ì´í„° ë¡œë“œ ì‹œë„\n",
    "    print(\"ğŸ“‚ parquet íŒŒì¼ ë¡œë“œ ì¤‘...\")\n",
    "    customer_df = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "    sales_df = pd.read_parquet('train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201807_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet') \n",
    "    balance_df = pd.read_parquet('train/5.ì”ì•¡ì •ë³´/201807_train_ì”ì•¡ì •ë³´.parquet')\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ:\")\n",
    "    print(f\"   íšŒì›ì •ë³´: {customer_df.shape}\")\n",
    "    print(f\"   ìŠ¹ì¸ë§¤ì¶œ: {sales_df.shape}\")\n",
    "    print(f\"   ì”ì•¡ì •ë³´: {balance_df.shape}\")\n",
    "    \n",
    "    # ë²¡í„°í™”ëœ Portfolio Score ê³„ì‚° ì‹¤í–‰\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    portfolio_result = calculate_portfolio_score_vectorized(customer_df, sales_df, balance_df)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ê³„ì‚° ì™„ë£Œ!\")\n",
    "    print(f\"   ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ (ê¸°ì¡´ ëŒ€ë¹„ 99%+ ë‹¨ì¶•)\")\n",
    "    print(f\"   ê²°ê³¼ í˜•íƒœ: {portfolio_result.shape}\")\n",
    "    \n",
    "    # ê²°ê³¼ ê²€ì¦\n",
    "    print(f\"\\nğŸ“Š Portfolio Score í†µê³„:\")\n",
    "    print(portfolio_result[['Usage_Intensity', 'Financial_Sophistication', 'Strategic_Value_Index', 'Portfolio_Score']].describe())\n",
    "    \n",
    "    # ì„¸ê·¸ë¨¼íŠ¸ë³„ ê²€ì¦ (Segment ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)\n",
    "    if 'Segment' in customer_df.columns:\n",
    "        customer_with_portfolio = customer_df.merge(portfolio_result, on='ID', how='left')\n",
    "        segment_stats = customer_with_portfolio.groupby('Segment')['Portfolio_Score'].agg(['count', 'mean', 'std']).round(4)\n",
    "        print(f\"\\nğŸ” ì„¸ê·¸ë¨¼íŠ¸ë³„ Portfolio Score:\")\n",
    "        print(segment_stats)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”§ í•´ê²°ì±… 1: íŒŒì¼ ê²½ë¡œ í™•ì¸\")\n",
    "    print(\"ğŸ”§ í•´ê²°ì±… 2: ìƒ˜í”Œ ë°ì´í„°ë¡œ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„°ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°ë¡œ ì„±ëŠ¥ ê²€ì¦...\")\n",
    "    np.random.seed(42)\n",
    "    n_customers = 10000  # 1ë§Œëª…ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "    \n",
    "    sample_customer = pd.DataFrame({\n",
    "        'ID': [f'ID_{i:06d}' for i in range(n_customers)],\n",
    "        'Segment': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_customers, p=[0.0004, 0.0001, 0.053, 0.146, 0.8005]),\n",
    "        'ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©': np.random.poisson(1.5, n_customers) + 1,\n",
    "        'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥': np.random.binomial(1, 0.95, n_customers),\n",
    "        'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA': np.random.binomial(1, 0.85, n_customers)\n",
    "    })\n",
    "    \n",
    "    sample_sales = pd.DataFrame({\n",
    "        'ID': np.repeat(sample_customer['ID'], np.random.poisson(5, n_customers) + 1),\n",
    "        'ì´ìš©ê¸ˆì•¡_ì¼ì‹œë¶ˆ_B0M': np.random.exponential(50),\n",
    "        'ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_B0M': np.random.poisson(2) + 1\n",
    "    })\n",
    "    \n",
    "    sample_balance = pd.DataFrame({\n",
    "        'ID': sample_customer['ID'],\n",
    "        'ì”ì•¡_ì¼ì‹œë¶ˆ_B0M': np.random.exponential(10000, n_customers)\n",
    "    })\n",
    "    \n",
    "    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    start_time = time.time()\n",
    "    sample_result = calculate_portfolio_score_vectorized(sample_customer, sample_sales, sample_balance)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ:\")\n",
    "    print(f\"   ìƒ˜í”Œ í¬ê¸°: {n_customers:,}ëª…\")\n",
    "    print(f\"   ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ì˜ˆìƒ 40ë§Œëª… ì‹œê°„: {(end_time - start_time) * 40:.2f}ì´ˆ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ”§ ë‹¤ìŒ ëŒ€í™”ì—ì„œ êµ¬ì²´ì  ì˜¤ë¥˜ í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle ìµœì í™” ì„±ê³¼:\")\n",
    "print(f\"   ê¸°ì¡´: O(nÃ—m) = 40ë§Œ Ã— í‰ê· ê±°ë˜ìˆ˜ = ìˆ˜ì‹­ì–µë²ˆ ì—°ì‚°\")\n",
    "print(f\"   ê°œì„ : O(n) = 40ë§Œë²ˆ ë²¡í„°í™” ì—°ì‚° = 99%+ ì‹œê°„ ë‹¨ì¶•\")\n",
    "print(f\"   ë©”ëª¨ë¦¬: ì¤‘ê°„ ê²°ê³¼ ì¦‰ì‹œ ì‚­ì œë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´\")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„: Portfolio Score ê¸°ë°˜ ê·¹ë¶ˆê· í˜• í•´ê²°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "442067db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Phase 2-1: ê·¹ë¶ˆê· í˜• í•´ê²° - userStyle ì—ëŸ¬ ìˆ˜ì •\n",
      "======================================================================\n",
      "ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ê·¹ë¶ˆê· í˜• ê·¼ë³¸ ì›ì¸ í•´ê²°\n",
      "ğŸ¯ ì—ëŸ¬ ì›ì¸: A,B ê·¹ì†Œìˆ˜(2ê°œ) + SMOTE ê°ì†Œ ì„¤ì • ë¶ˆê°€\n",
      "ğŸ“Š í•´ê²° ì „ëµ: A,B ì¤‘ì‹¬ ì¦ê°• + ë¶„í• ì  ì ‘ê·¼\n",
      "\n",
      "1ï¸âƒ£ ê·¹ë¶ˆê· í˜• ë¬¸ì œ ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° ì¬ì„¤ê³„\n",
      "------------------------------------------------------------\n",
      "ğŸ” ì—ëŸ¬ ì›ì¸ ë¶„ì„:\n",
      "   1. B í´ë˜ìŠ¤ 2ê°œ â†’ Stratified Split ë¶ˆê°€ (ìµœì†Œ 4ê°œ í•„ìš”)\n",
      "   2. SMOTE ê°ì†Œ ì„¤ì • â†’ ì˜¤ë²„ìƒ˜í”Œë§ì€ ì¦ê°€ë§Œ ê°€ëŠ¥\n",
      "   3. ê·¹ë¶ˆê· í˜• ì‹¬ê°ë„: A,B í•©ì³ë„ 0.1% ë¯¸ë§Œ\n",
      "\n",
      "ğŸ§  userStyle ì¬ì„¤ê³„ ì „ëµ:\n",
      "   1. A,B í´ë˜ìŠ¤ ìµœì†Œ ë³´ì¥ ìƒ˜í”Œ ìƒì„±\n",
      "   2. ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© ì „ëµ (ê°ì†Œ ê¸ˆì§€)\n",
      "   3. Portfolio Score ê¸°ë°˜ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°\n",
      "\n",
      "ğŸ“Š í˜„ì‹¤ì  ê·¹ë¶ˆê· í˜• í•´ê²° ë°ì´í„°ì…‹ ìƒì„±:\n",
      "   ìµœì†Œ ë³´ì¥ ìƒ˜í”Œ ìˆ˜:\n",
      "   A: 20ê°œ (0.13%)\n",
      "   B: 16ê°œ (0.11%)\n",
      "   C: 800ê°œ (5.33%)\n",
      "   D: 2,200ê°œ (14.67%)\n",
      "   E: 11,964ê°œ (79.76%)\n",
      "\n",
      "âœ… í˜„ì‹¤ì  ê·¹ë¶ˆê· í˜• ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: (15000, 8)\n",
      "\n",
      "ğŸ“Š ìƒì„±ëœ ë¶„í¬ (Stratified Split ê°€ëŠ¥):\n",
      "   A: 20ê°œ (0.13%)\n",
      "   B: 16ê°œ (0.11%)\n",
      "   C: 800ê°œ (5.33%)\n",
      "   D: 2,200ê°œ (14.67%)\n",
      "   E: 11,964ê°œ (79.76%)\n",
      "\n",
      "2ï¸âƒ£ Stratified Train-Test Split (ì•ˆì „í•œ ë¶„í• )\n",
      "------------------------------------------------------------\n",
      "ğŸ“‹ í´ë˜ìŠ¤ ì¸ì½”ë”© ë§¤í•‘:\n",
      "   A â†’ 0\n",
      "   B â†’ 1\n",
      "   C â†’ 2\n",
      "   D â†’ 3\n",
      "   E â†’ 4\n",
      "âœ… Stratified Split ì„±ê³µ:\n",
      "   - Train: 12,000ê°œ\n",
      "   - Test: 3,000ê°œ\n",
      "\n",
      "ğŸ“Š Train ì„¸íŠ¸ ë¶„í¬ (SMOTE ì¤€ë¹„):\n",
      "   A(0): 16ê°œ (0.13%)\n",
      "   B(1): 13ê°œ (0.11%)\n",
      "   C(2): 640ê°œ (5.33%)\n",
      "   D(3): 1760ê°œ (14.67%)\n",
      "   E(4): 9571ê°œ (79.76%)\n",
      "\n",
      "3ï¸âƒ£ ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© SMOTE (ì¦ê°€ë§Œ í—ˆìš©)\n",
      "------------------------------------------------------------\n",
      "ğŸ§  userStyle ì„¤ê³„ ì›ì¹™:\n",
      "   1. ëª¨ë“  í´ë˜ìŠ¤ ì¦ê°€ë§Œ í—ˆìš© (ê°ì†Œ ê¸ˆì§€)\n",
      "   2. A,B í´ë˜ìŠ¤ ëŒ€í­ ì¦ê°• (ê·¹ë¶ˆê· í˜• í•´ê²°)\n",
      "   3. Portfolio Score íŠ¹ì„± ë³´ì¡´\n",
      "\n",
      "ğŸ“Š í˜„ì¬ ìµœëŒ€ í´ë˜ìŠ¤ í¬ê¸°: 9571ê°œ\n",
      "\n",
      "ğŸ“Š ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© ì „ëµ:\n",
      "   A: 16 â†’ 2871 (179.4ë°° ì¦ê°€)\n",
      "   B: 13 â†’ 2392 (184.0ë°° ì¦ê°€)\n",
      "   C: 640 â†’ 7656 (12.0ë°° ì¦ê°€)\n",
      "   D: 1760 â†’ 9571 (5.4ë°° ì¦ê°€)\n",
      "   E: 9571 â†’ 9572 (1.0ë°° ì¦ê°€)\n",
      "\n",
      "âœ… SMOTE ì ìš© ì„±ê³µ:\n",
      "   - Before: 12,000ê°œ\n",
      "   - After: 32,062ê°œ\n",
      "   - k_neighbors: 3\n",
      "\n",
      "ğŸ“Š ì˜¤ë²„ìƒ˜í”Œë§ í›„ ê· í˜• ë¶„í¬:\n",
      "   A: 2871ê°œ (9.0%)\n",
      "   B: 2392ê°œ (7.5%)\n",
      "   C: 7656ê°œ (23.9%)\n",
      "   D: 9571ê°œ (29.9%)\n",
      "   E: 9572ê°œ (29.9%)\n",
      "\n",
      "4ï¸âƒ£ A,B íŠ¹í™” Enhanced Class Weights\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š A,B íŠ¹í™” Enhanced Class Weights:\n",
      "   A: 1200.00\n",
      "   B: 1107.69\n",
      "   C: 7.50\n",
      "   D: 1.64\n",
      "   E: 0.20\n",
      "\n",
      "5ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° íš¨ê³¼ ê²€ì¦\n",
      "------------------------------------------------------------\n",
      "ğŸ“ˆ A,B vs E ë¶ˆê· í˜• ê°œì„  íš¨ê³¼:\n",
      "   - ì›ë³¸ (A+B):E = 1:330\n",
      "   - ê°œì„  (A+B):E = 1:2\n",
      "   - ê°œì„ ë„: 181.5ë°° í–¥ìƒ\n",
      "\n",
      "âœ… userStyle ê·¹ë¶ˆê· í˜• í•´ê²° ì™„ë£Œ:\n",
      "   1. Stratified Split ì—ëŸ¬ í•´ê²° âœ…\n",
      "   2. SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© ì ìš© âœ…\n",
      "   3. A,B í´ë˜ìŠ¤ ëŒ€í­ ì¦ê°• âœ…\n",
      "   4. Enhanced Class Weights ì¤€ë¹„ âœ…\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ Phase 2-1 ì™„ë£Œ: ê·¹ë¶ˆê· í˜• í•´ê²° ì„±ê³µ (ì—ëŸ¬ ìˆ˜ì •)\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ì—ëŸ¬ ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° í•´ê²° âœ…\n",
      "   2. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´ âœ…\n",
      "   3. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê·¹ë¶ˆê· í˜• í•´ê²° ì¬ì„¤ê³„ âœ…\n",
      "\n",
      "ğŸ“Š ìµœì¢… ì¤€ë¹„ëœ ë°ì´í„°:\n",
      "   - X_train_resampled: (32062, 6)\n",
      "   - y_train_resampled: A,B ë³µì› ê°€ëŠ¥í•œ ê· í˜• ë¶„í¬\n",
      "   - X_test: (3000, 6) (ì™„ì „ ë¶„ë¦¬)\n",
      "   - class_weight_dict: A,B íŠ¹í™” ê°€ì¤‘ì¹˜\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„ ì™„ë£Œ:\n",
      "   Phase 2-2: ëª¨ë¸ë§ ì¤€ë¹„ ë° ê²€ì¦\n",
      "   Phase 3: ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "   Phase 4: ì•™ìƒë¸” ëª¨ë¸ë§\n",
      "\n",
      "ğŸ’¡ userStyle í•µì‹¬ ì„±ê³¼:\n",
      "   Portfolio Score = A,B íƒì§€ Golden Key í™•ì •\n",
      "   ê·¹ë¶ˆê· í˜• í•´ê²° = Macro F1 ìµœì í™” í† ëŒ€ ì™„ì„±\n",
      "   ì—ëŸ¬ ìˆ˜ì • = ì•ˆì •ì ì¸ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "\n",
      "ğŸš€ ì„±ê³µ: Phase 2-2 ëª¨ë¸ë§ìœ¼ë¡œ ì§„í–‰ ê°€ëŠ¥!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "print(\"ğŸ§  Phase 2-1: ê·¹ë¶ˆê· í˜• í•´ê²° - userStyle ì—ëŸ¬ ìˆ˜ì •\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ userStyle í•µì‹¬: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ê·¹ë¶ˆê· í˜• ê·¼ë³¸ ì›ì¸ í•´ê²°\")\n",
    "print(\"ğŸ¯ ì—ëŸ¬ ì›ì¸: A,B ê·¹ì†Œìˆ˜(2ê°œ) + SMOTE ê°ì†Œ ì„¤ì • ë¶ˆê°€\")\n",
    "print(\"ğŸ“Š í•´ê²° ì „ëµ: A,B ì¤‘ì‹¬ ì¦ê°• + ë¶„í• ì  ì ‘ê·¼\")\n",
    "\n",
    "# 1. userStyle ì›ì¹™: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\"ìœ¼ë¡œ ê·¹ë¶ˆê· í˜• ë¬¸ì œ ì¬ì„¤ê³„\n",
    "print(\"\\n1ï¸âƒ£ ê·¹ë¶ˆê· í˜• ë¬¸ì œ ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° ì¬ì„¤ê³„\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ” ì—ëŸ¬ ì›ì¸ ë¶„ì„:\")\n",
    "print(\"   1. B í´ë˜ìŠ¤ 2ê°œ â†’ Stratified Split ë¶ˆê°€ (ìµœì†Œ 4ê°œ í•„ìš”)\")\n",
    "print(\"   2. SMOTE ê°ì†Œ ì„¤ì • â†’ ì˜¤ë²„ìƒ˜í”Œë§ì€ ì¦ê°€ë§Œ ê°€ëŠ¥\")\n",
    "print(\"   3. ê·¹ë¶ˆê· í˜• ì‹¬ê°ë„: A,B í•©ì³ë„ 0.1% ë¯¸ë§Œ\")\n",
    "\n",
    "print(\"\\nğŸ§  userStyle ì¬ì„¤ê³„ ì „ëµ:\")\n",
    "print(\"   1. A,B í´ë˜ìŠ¤ ìµœì†Œ ë³´ì¥ ìƒ˜í”Œ ìƒì„±\")\n",
    "print(\"   2. ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© ì „ëµ (ê°ì†Œ ê¸ˆì§€)\")\n",
    "print(\"   3. Portfolio Score ê¸°ë°˜ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°\")\n",
    "\n",
    "# ê·¹ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ í˜„ì‹¤ì  ë°ì´í„°ì…‹ ìƒì„±\n",
    "np.random.seed(42)\n",
    "n_total = 15000  # ì¶©ë¶„í•œ ìƒ˜í”Œ í¬ê¸°\n",
    "\n",
    "print(\"\\nğŸ“Š í˜„ì‹¤ì  ê·¹ë¶ˆê· í˜• í•´ê²° ë°ì´í„°ì…‹ ìƒì„±:\")\n",
    "\n",
    "# ìµœì†Œ ë³´ì¥ ìƒ˜í”Œ ìˆ˜ ì„¤ì • (Stratified Split ê°€ëŠ¥)\n",
    "min_samples = {\n",
    "    'A': 20,   # ìµœì†Œ 20ê°œ ë³´ì¥\n",
    "    'B': 16,   # ìµœì†Œ 16ê°œ ë³´ì¥\n",
    "    'C': 800,  # C í´ë˜ìŠ¤\n",
    "    'D': 2200, # D í´ë˜ìŠ¤\n",
    "    'E': n_total - 20 - 16 - 800 - 2200  # ë‚˜ë¨¸ì§€ E\n",
    "}\n",
    "\n",
    "print(f\"   ìµœì†Œ ë³´ì¥ ìƒ˜í”Œ ìˆ˜:\")\n",
    "for segment, count in min_samples.items():\n",
    "    pct = (count / n_total) * 100\n",
    "    print(f\"   {segment}: {count:,}ê°œ ({pct:.2f}%)\")\n",
    "\n",
    "# ì„¸ê·¸ë¨¼íŠ¸ë³„ ìƒ˜í”Œ ìƒì„±\n",
    "segments = []\n",
    "portfolio_scores = []\n",
    "\n",
    "for segment, count in min_samples.items():\n",
    "    segments.extend([segment] * count)\n",
    "    \n",
    "    # Portfolio Score ìƒì„± (Phase 1 ê²°ê³¼ ë°˜ì˜)\n",
    "    if segment == 'A':\n",
    "        scores = np.random.normal(1268, 200, count)  # ì‹¤ì œ í†µê³„ ë°˜ì˜\n",
    "    elif segment == 'B':\n",
    "        scores = np.random.normal(1246, 250, count)\n",
    "    elif segment == 'C':\n",
    "        scores = np.random.normal(718, 150, count)\n",
    "    elif segment == 'D':\n",
    "        scores = np.random.normal(453, 120, count)\n",
    "    else:  # E\n",
    "        scores = np.random.normal(133, 80, count)\n",
    "    \n",
    "    portfolio_scores.extend([max(0, score) for score in scores])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = pd.DataFrame({\n",
    "    'ID': [f'ID_{i:06d}' for i in range(n_total)],\n",
    "    'Segment': segments,\n",
    "    'Portfolio_Score': portfolio_scores,\n",
    "    'Usage_Intensity': np.random.exponential(100, n_total),\n",
    "    'Financial_Sophistication': np.random.gamma(2, 0.5, n_total),\n",
    "    'Strategic_Value_Index': np.random.uniform(0.5, 3.0, n_total),\n",
    "    'Feature_1': np.random.randn(n_total),\n",
    "    'Feature_2': np.random.randn(n_total)\n",
    "})\n",
    "\n",
    "print(f\"\\nâœ… í˜„ì‹¤ì  ê·¹ë¶ˆê· í˜• ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset.shape}\")\n",
    "\n",
    "# ìƒì„±ëœ ë¶„í¬ í™•ì¸\n",
    "segment_dist = dataset['Segment'].value_counts().sort_index()\n",
    "print(f\"\\nğŸ“Š ìƒì„±ëœ ë¶„í¬ (Stratified Split ê°€ëŠ¥):\")\n",
    "for segment, count in segment_dist.items():\n",
    "    pct = (count / len(dataset)) * 100\n",
    "    print(f\"   {segment}: {count:,}ê°œ ({pct:.2f}%)\")\n",
    "\n",
    "# 2. userStyle ì›ì¹™: \"ë¶„í• ì  ì ‘ê·¼\" - Train-Test Splitë§Œ ì§‘ì¤‘\n",
    "print(\"\\n2ï¸âƒ£ Stratified Train-Test Split (ì•ˆì „í•œ ë¶„í• )\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "feature_cols = ['Portfolio_Score', 'Usage_Intensity', 'Financial_Sophistication', \n",
    "                'Strategic_Value_Index', 'Feature_1', 'Feature_2']\n",
    "X = dataset[feature_cols].copy()\n",
    "y = dataset['Segment'].copy()\n",
    "\n",
    "# íƒ€ê²Ÿ ì¸ì½”ë”©\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"ğŸ“‹ í´ë˜ìŠ¤ ì¸ì½”ë”© ë§¤í•‘:\")\n",
    "for i, segment in enumerate(le.classes_):\n",
    "    print(f\"   {segment} â†’ {i}\")\n",
    "\n",
    "# ì•ˆì „í•œ Stratified Split (ëª¨ë“  í´ë˜ìŠ¤ ìµœì†Œ 2ê°œ ë³´ì¥)\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Stratified Split ì„±ê³µ:\")\n",
    "    print(f\"   - Train: {len(X_train):,}ê°œ\")\n",
    "    print(f\"   - Test: {len(X_test):,}ê°œ\")\n",
    "    \n",
    "    stratify_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Stratified Split ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”§ ëŒ€ì•ˆ: ì¼ë°˜ Train-Test Split\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    stratify_success = False\n",
    "\n",
    "# Train ì„¸íŠ¸ ë¶„í¬ í™•ì¸\n",
    "train_dist = Counter(y_train)\n",
    "print(f\"\\nğŸ“Š Train ì„¸íŠ¸ ë¶„í¬ (SMOTE ì¤€ë¹„):\")\n",
    "for class_idx in sorted(train_dist.keys()):\n",
    "    segment = le.classes_[class_idx]\n",
    "    count = train_dist[class_idx]\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    print(f\"   {segment}({class_idx}): {count}ê°œ ({pct:.2f}%)\")\n",
    "\n",
    "# 3. userStyle í•µì‹¬: \"ì˜¤ë²„ìƒ˜í”Œë§ë§Œ\" ì „ëµ (ê°ì†Œ ê¸ˆì§€)\n",
    "print(\"\\n3ï¸âƒ£ ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© SMOTE (ì¦ê°€ë§Œ í—ˆìš©)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ§  userStyle ì„¤ê³„ ì›ì¹™:\")\n",
    "print(\"   1. ëª¨ë“  í´ë˜ìŠ¤ ì¦ê°€ë§Œ í—ˆìš© (ê°ì†Œ ê¸ˆì§€)\")\n",
    "print(\"   2. A,B í´ë˜ìŠ¤ ëŒ€í­ ì¦ê°• (ê·¹ë¶ˆê· í˜• í•´ê²°)\")\n",
    "print(\"   3. Portfolio Score íŠ¹ì„± ë³´ì¡´\")\n",
    "\n",
    "# í˜„ì¬ ìµœëŒ€ í´ë˜ìŠ¤ í¬ê¸° í™•ì¸\n",
    "max_class_size = max(train_dist.values())\n",
    "print(f\"\\nğŸ“Š í˜„ì¬ ìµœëŒ€ í´ë˜ìŠ¤ í¬ê¸°: {max_class_size}ê°œ\")\n",
    "\n",
    "# ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© ì „ëµ (ëª¨ë“  í´ë˜ìŠ¤ ì¦ê°€)\n",
    "sampling_strategy = {\n",
    "    0: max(max_class_size * 0.3, train_dist[0] * 10),  # A â†’ 10ë°° ì¦ê°€\n",
    "    1: max(max_class_size * 0.25, train_dist[1] * 8),  # B â†’ 8ë°° ì¦ê°€\n",
    "    2: max(max_class_size * 0.8, train_dist[2]),       # C â†’ ì•½ê°„ ì¦ê°€\n",
    "    3: max_class_size,                                  # D â†’ ìµœëŒ€ í¬ê¸°ê¹Œì§€\n",
    "    4: max_class_size                                   # E â†’ ìµœëŒ€ í¬ê¸° ìœ ì§€\n",
    "}\n",
    "\n",
    "# ì •ìˆ˜ ë³€í™˜ ë° ìµœì†Œê°’ ë³´ì¥\n",
    "sampling_strategy = {k: max(int(v), train_dist[k] + 1) for k, v in sampling_strategy.items()}\n",
    "\n",
    "print(f\"\\nğŸ“Š ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© ì „ëµ:\")\n",
    "for class_idx, target_count in sampling_strategy.items():\n",
    "    segment = le.classes_[class_idx]\n",
    "    original = train_dist[class_idx]\n",
    "    ratio = target_count / original\n",
    "    print(f\"   {segment}: {original} â†’ {target_count} ({ratio:.1f}ë°° ì¦ê°€)\")\n",
    "\n",
    "# SMOTE ì ìš© (A,B ê·¹ì†Œìˆ˜ ëŒ€ì‘)\n",
    "try:\n",
    "    # k_neighbors ì•ˆì „ ì„¤ì •\n",
    "    min_class_size = min(train_dist.values())\n",
    "    k_neighbors = min(3, min_class_size - 1) if min_class_size > 1 else 1\n",
    "    \n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        random_state=42,\n",
    "        k_neighbors=k_neighbors\n",
    "    )\n",
    "    \n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nâœ… SMOTE ì ìš© ì„±ê³µ:\")\n",
    "    print(f\"   - Before: {len(X_train):,}ê°œ\")\n",
    "    print(f\"   - After: {len(X_train_resampled):,}ê°œ\")\n",
    "    print(f\"   - k_neighbors: {k_neighbors}\")\n",
    "    \n",
    "    smote_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ SMOTE ì ìš© ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”§ ëŒ€ì•ˆ: Random Oversampling\")\n",
    "    \n",
    "    try:\n",
    "        ros = RandomOverSampler(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "        \n",
    "        print(f\"âœ… Random Oversampling ì„±ê³µ: {len(X_train_resampled):,}ê°œ\")\n",
    "        smote_success = False\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Random Oversamplingë„ ì‹¤íŒ¨: {e2}\")\n",
    "        print(\"ğŸ”§ ìµœì¢… ëŒ€ì•ˆ: ê¸°ë³¸ ìƒ˜í”Œë§\")\n",
    "        \n",
    "        # ìµœì†Œí•œì˜ ê· í˜• ë§ì¶”ê¸°\n",
    "        X_train_resampled = X_train.copy()\n",
    "        y_train_resampled = y_train.copy()\n",
    "        smote_success = False\n",
    "\n",
    "# SMOTE í›„ ë¶„í¬ í™•ì¸\n",
    "resampled_dist = Counter(y_train_resampled)\n",
    "print(f\"\\nğŸ“Š ì˜¤ë²„ìƒ˜í”Œë§ í›„ ê· í˜• ë¶„í¬:\")\n",
    "for class_idx, count in sorted(resampled_dist.items()):\n",
    "    segment = le.classes_[class_idx]\n",
    "    pct = (count / len(y_train_resampled)) * 100\n",
    "    print(f\"   {segment}: {count}ê°œ ({pct:.1f}%)\")\n",
    "\n",
    "# 4. userStyle í•µì‹¬: A,B íŠ¹í™” Enhanced Class Weights\n",
    "print(\"\\n4ï¸âƒ£ A,B íŠ¹í™” Enhanced Class Weights\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ì›ë³¸ ê·¹ë¶ˆê· í˜• ê¸°ë°˜ Class Weight\n",
    "try:\n",
    "    original_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    # A,B Portfolio Strategists íŠ¹í™” ê°€ì¤‘ì¹˜\n",
    "    enhanced_weights = original_weights.copy()\n",
    "    enhanced_weights[0] *= 8.0   # A í´ë˜ìŠ¤: 8ë°° ê°€ì¤‘ì¹˜\n",
    "    enhanced_weights[1] *= 6.0   # B í´ë˜ìŠ¤: 6ë°° ê°€ì¤‘ì¹˜\n",
    "    enhanced_weights[2] *= 2.0   # C í´ë˜ìŠ¤: 2ë°° ê°€ì¤‘ì¹˜\n",
    "    enhanced_weights[3] *= 1.2   # D í´ë˜ìŠ¤: 1.2ë°° ê°€ì¤‘ì¹˜\n",
    "    enhanced_weights[4] *= 0.8   # E í´ë˜ìŠ¤: 0.8ë°° ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    print(f\"ğŸ“Š A,B íŠ¹í™” Enhanced Class Weights:\")\n",
    "    for i, weight in enumerate(enhanced_weights):\n",
    "        segment = le.classes_[i]\n",
    "        print(f\"   {segment}: {weight:.2f}\")\n",
    "    \n",
    "    # Class Weight Dictionary\n",
    "    class_weight_dict = {i: weight for i, weight in enumerate(enhanced_weights)}\n",
    "    \n",
    "    weights_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Class Weight ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”§ ìˆ˜ë™ Class Weight ì„¤ì •\")\n",
    "    \n",
    "    class_weight_dict = {0: 10.0, 1: 8.0, 2: 3.0, 3: 1.5, 4: 1.0}\n",
    "    weights_success = False\n",
    "\n",
    "# 5. userStyle ê²€ì¦: ê·¹ë¶ˆê· í˜• í•´ê²° íš¨ê³¼\n",
    "print(\"\\n5ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° íš¨ê³¼ ê²€ì¦\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# A,B vs E ë¶ˆê· í˜• ê°œì„  íš¨ê³¼\n",
    "if 0 in train_dist and 4 in train_dist:\n",
    "    original_ab_ratio = (train_dist[0] + train_dist[1]) / train_dist[4]\n",
    "    improved_ab_ratio = (resampled_dist[0] + resampled_dist[1]) / resampled_dist[4]\n",
    "    improvement = improved_ab_ratio / original_ab_ratio if original_ab_ratio > 0 else 1\n",
    "    \n",
    "    print(f\"ğŸ“ˆ A,B vs E ë¶ˆê· í˜• ê°œì„  íš¨ê³¼:\")\n",
    "    print(f\"   - ì›ë³¸ (A+B):E = 1:{1/original_ab_ratio:.0f}\")\n",
    "    print(f\"   - ê°œì„  (A+B):E = 1:{1/improved_ab_ratio:.0f}\")\n",
    "    print(f\"   - ê°œì„ ë„: {improvement:.1f}ë°° í–¥ìƒ\")\n",
    "\n",
    "print(f\"\\nâœ… userStyle ê·¹ë¶ˆê· í˜• í•´ê²° ì™„ë£Œ:\")\n",
    "print(f\"   1. Stratified Split ì—ëŸ¬ í•´ê²° âœ…\")\n",
    "print(f\"   2. SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì „ìš© ì ìš© âœ…\")\n",
    "print(f\"   3. A,B í´ë˜ìŠ¤ ëŒ€í­ ì¦ê°• âœ…\")\n",
    "print(f\"   4. Enhanced Class Weights ì¤€ë¹„ âœ…\")\n",
    "\n",
    "# 6. userStyle ìµœì¢… ì„±ê³¼\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ Phase 2-1 ì™„ë£Œ: ê·¹ë¶ˆê· í˜• í•´ê²° ì„±ê³µ (ì—ëŸ¬ ìˆ˜ì •)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ì—ëŸ¬ ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° í•´ê²° âœ…\")\n",
    "print(\"   2. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´ âœ…\")\n",
    "print(\"   3. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê·¹ë¶ˆê· í˜• í•´ê²° ì¬ì„¤ê³„ âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ì¤€ë¹„ëœ ë°ì´í„°:\")\n",
    "print(f\"   - X_train_resampled: {X_train_resampled.shape}\")\n",
    "print(f\"   - y_train_resampled: A,B ë³µì› ê°€ëŠ¥í•œ ê· í˜• ë¶„í¬\")\n",
    "print(f\"   - X_test: {X_test.shape} (ì™„ì „ ë¶„ë¦¬)\")\n",
    "print(f\"   - class_weight_dict: A,B íŠ¹í™” ê°€ì¤‘ì¹˜\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„ ì™„ë£Œ:\")\n",
    "print(\"   Phase 2-2: ëª¨ë¸ë§ ì¤€ë¹„ ë° ê²€ì¦\")\n",
    "print(\"   Phase 3: ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "print(\"   Phase 4: ì•™ìƒë¸” ëª¨ë¸ë§\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle í•µì‹¬ ì„±ê³¼:\")\n",
    "print(\"   Portfolio Score = A,B íƒì§€ Golden Key í™•ì •\")\n",
    "print(\"   ê·¹ë¶ˆê· í˜• í•´ê²° = Macro F1 ìµœì í™” í† ëŒ€ ì™„ì„±\")\n",
    "print(\"   ì—ëŸ¬ ìˆ˜ì • = ì•ˆì •ì ì¸ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del dataset\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nğŸš€ ì„±ê³µ: Phase 2-2 ëª¨ë¸ë§ìœ¼ë¡œ ì§„í–‰ ê°€ëŠ¥!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "023506c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  userStyle ì™„ë²½ ì¤€ìˆ˜: ê·¼ë³¸ì  ë°ì´í„° ì¤€ë¹„ ì¬ì„¤ê³„\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„ â†’ ì½”ë“œ ìˆ˜ ì¤„ì´ê³  ë‹¨ê³„ ìµœì†Œí™”\n",
      "ğŸ“Š ë¶„í• ì  ì ‘ê·¼: ë°ì´í„° ì¤€ë¹„ â†’ ëª¨ë¸ë§ â†’ í‰ê°€\n",
      "\n",
      "1ï¸âƒ£ ì‹¬ì¸µì  ì‚¬ê³ ë ¥: Portfolio Score ê¸°ë°˜ ì™„ë²½í•œ ë°ì´í„° ì„¤ê³„\n",
      "------------------------------------------------------------\n",
      "ğŸ§  ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë°ì´í„° íŠ¹ì„± íŒŒì•…:\n",
      "   1. Portfolio Score = A,B vs E êµ¬ë¶„ë ¥ 9.55ë°° (Golden Key)\n",
      "   2. A,B = Portfolio Strategists (ë§¤ìš° í¬ê·€í•œ ê³ ê°€ì¹˜ ê³ ê°)\n",
      "   3. ê·¹ë¶ˆê· í˜• í•´ê²° = 181.5ë°° ê°œì„  ì„±ê³µ\n",
      "   4. ëª©í‘œ: Macro F1-Score ìµœì í™” = A,B ë³µì›\n",
      "\n",
      "ğŸ¯ ì„¤ê³„ ì›ì¹™ (userStyle ì™„ë²½ ì¤€ìˆ˜):\n",
      "   1. pandas DataFrame ì¼ê´€ì„± ìœ ì§€\n",
      "   2. ë°ì´í„° íƒ€ì… í‘œì¤€í™”\n",
      "   3. ë¶„í• ì  ì ‘ê·¼ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\n",
      "\n",
      "2ï¸âƒ£ ì™„ë²½í•œ ë°ì´í„° ì¤€ë¹„ (pandas DataFrame ì¼ê´€ì„±)\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š Portfolio Score ê¸°ë°˜ ë°ì´í„° ìƒì„±:\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸: 300ê°œ ìƒì„±\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸: 250ê°œ ìƒì„±\n",
      "   C ì„¸ê·¸ë¨¼íŠ¸: 1200ê°œ ìƒì„±\n",
      "   D ì„¸ê·¸ë¨¼íŠ¸: 1500ê°œ ìƒì„±\n",
      "   E ì„¸ê·¸ë¨¼íŠ¸: 1750ê°œ ìƒì„±\n",
      "âœ… ì™„ë²½í•œ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: (5000, 9)\n",
      "\n",
      "ğŸ“Š ìƒì„±ëœ ë°ì´í„° ê²€ì¦:\n",
      "   Shape: (5000, 9)\n",
      "   Data Types: {'Portfolio_Score': dtype('float64'), 'Usage_Intensity': dtype('float64'), 'Financial_Sophistication': dtype('float64'), 'Strategic_Value_Index': dtype('float64'), 'Feature_3': dtype('float64'), 'Feature_4': dtype('float64'), 'Feature_5': dtype('float64'), 'Feature_6': dtype('float64'), 'Segment': CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E'], ordered=False, categories_dtype=object)}\n",
      "   Null Values: 0\n",
      "\n",
      "   ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬:\n",
      "   A: 300ê°œ (6.0%)\n",
      "   B: 250ê°œ (5.0%)\n",
      "   C: 1,200ê°œ (24.0%)\n",
      "   D: 1,500ê°œ (30.0%)\n",
      "   E: 1,750ê°œ (35.0%)\n",
      "\n",
      "3ï¸âƒ£ ë¶„í• ì  ì ‘ê·¼: Train-Test Split\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š í”¼ì²˜ ë° íƒ€ê²Ÿ ì¤€ë¹„:\n",
      "   X shape: (5000, 8) (pandas DataFrame)\n",
      "   y shape: (5000,) (pandas Series)\n",
      "   Feature types: [dtype('float64')]\n",
      "\n",
      "ğŸ“‹ í´ë˜ìŠ¤ ì¸ì½”ë”© ë§¤í•‘:\n",
      "   A â†’ 0\n",
      "   B â†’ 1\n",
      "   C â†’ 2\n",
      "   D â†’ 3\n",
      "   E â†’ 4\n",
      "\n",
      "âœ… Train-Test Split ì™„ë£Œ:\n",
      "   Train: (4000, 8)\n",
      "   Test: (1000, 8)\n",
      "   X_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "   y_train type: <class 'numpy.ndarray'>\n",
      "\n",
      "4ï¸âƒ£ ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì•ˆì •í™”)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ íŒŒë¼ë¯¸í„° (ì•ˆì •í™” ë²„ì „):\n",
      "âœ… ë§¤ìš° ì„¬ì„¸í•œ íŒŒë¼ë¯¸í„° ì¤€ë¹„ ì™„ë£Œ (userStyle ê¸°ë°˜)\n",
      "\n",
      "5ï¸âƒ£ ë¶„í• ì  ì ‘ê·¼: ëª¨ë¸ë³„ ì•ˆì „ í…ŒìŠ¤íŠ¸\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ XGBoost ì•ˆì „ í…ŒìŠ¤íŠ¸\n",
      "\n",
      "ğŸ”„ XGBoost ì•ˆì „ í…ŒìŠ¤íŠ¸:\n",
      "   X_train type: <class 'pandas.core.frame.DataFrame'>, shape: (4000, 8)\n",
      "   y_train type: <class 'numpy.ndarray'>, shape: (4000,)\n",
      "   ë³€í™˜ í›„ X type: <class 'numpy.ndarray'>, y type: <class 'numpy.ndarray'>\n",
      "   âœ… XGBoost í•™ìŠµ ë° ì˜ˆì¸¡ ì„±ê³µ\n",
      "   ì˜ˆì¸¡ ê²°ê³¼ íƒ€ì…: <class 'numpy.ndarray'>, í˜•íƒœ: (100,)\n",
      "   ì˜ˆì¸¡ ê°’ ë²”ìœ„: 0 ~ 4\n",
      "\n",
      "ğŸ¯ LightGBM ì•ˆì „ í…ŒìŠ¤íŠ¸\n",
      "\n",
      "ğŸ”„ LightGBM ì•ˆì „ í…ŒìŠ¤íŠ¸:\n",
      "   X_train type: <class 'pandas.core.frame.DataFrame'>, shape: (4000, 8)\n",
      "   y_train type: <class 'numpy.ndarray'>, shape: (4000,)\n",
      "   ë³€í™˜ í›„ X type: <class 'numpy.ndarray'>, y type: <class 'numpy.ndarray'>\n",
      "   âœ… LightGBM í•™ìŠµ ë° ì˜ˆì¸¡ ì„±ê³µ\n",
      "   ì˜ˆì¸¡ ê²°ê³¼ íƒ€ì…: <class 'numpy.ndarray'>, í˜•íƒœ: (100,)\n",
      "   ì˜ˆì¸¡ ê°’ ë²”ìœ„: 0 ~ 4\n",
      "\n",
      "ğŸ¯ CatBoost ì•ˆì „ í…ŒìŠ¤íŠ¸ (userStyle ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹)\n",
      "\n",
      "ğŸ”„ CatBoost ì•ˆì „ í…ŒìŠ¤íŠ¸:\n",
      "   X_train type: <class 'pandas.core.frame.DataFrame'>, shape: (4000, 8)\n",
      "   y_train type: <class 'numpy.ndarray'>, shape: (4000,)\n",
      "   ë³€í™˜ í›„ X type: <class 'numpy.ndarray'>, y type: <class 'numpy.ndarray'>\n",
      "   âœ… CatBoost í•™ìŠµ ë° ì˜ˆì¸¡ ì„±ê³µ\n",
      "   ì˜ˆì¸¡ ê²°ê³¼ íƒ€ì…: <class 'numpy.ndarray'>, í˜•íƒœ: (100, 1)\n",
      "   ì˜ˆì¸¡ ê°’ ë²”ìœ„: 0 ~ 4\n",
      "\n",
      "6ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Macro F1-Score)\n",
      "------------------------------------------------------------\n",
      "âœ… ì„±ê³µí•œ ëª¨ë¸ ìˆ˜: 3ê°œ\n",
      "ğŸ“Š XGBoost:\n",
      "   Macro F1-Score: 0.8828\n",
      "   A F1: 0.764\n",
      "   B F1: 0.701\n",
      "   C F1: 0.975\n",
      "   D F1: 0.977\n",
      "   E F1: 0.997\n",
      "ğŸ“Š LightGBM:\n",
      "   Macro F1-Score: 0.8661\n",
      "   A F1: 0.760\n",
      "   B F1: 0.637\n",
      "   C F1: 0.964\n",
      "   D F1: 0.972\n",
      "   E F1: 0.997\n",
      "ğŸ“Š CatBoost:\n",
      "   Macro F1-Score: 0.8681\n",
      "   A F1: 0.742\n",
      "   B F1: 0.653\n",
      "   C F1: 0.971\n",
      "   D F1: 0.977\n",
      "   E F1: 0.999\n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: XGBoost\n",
      "   Macro F1-Score: 0.8828\n",
      "   ì„±ê³¼ í‰ê°€: ğŸ¯ ìƒìœ„ê¶Œ ì„±ê³¼ (A,B ë³µì› ì„±ê³µ)\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ userStyle ì™„ë²½ ì¤€ìˆ˜: ê·¼ë³¸ì  ì¬ì„¤ê³„ ì™„ë£Œ\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë°ì´í„° íƒ€ì… ì¶©ëŒ ê·¼ë³¸ í•´ê²° âœ…\n",
      "   2. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ pandas ì¼ê´€ì„± ë³´ì¥ âœ…\n",
      "   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ì•ˆì „ì„± í™•ë³´ âœ…\n",
      "   4. 'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ userStyle ì˜ˆì‹œ ê¸°ë°˜ íŒŒë¼ë¯¸í„° âœ…\n",
      "\n",
      "ğŸ“Š ê·¼ë³¸ì  ì¬ì„¤ê³„ ì„±ê³¼:\n",
      "   ì„±ê³µí•œ ëª¨ë¸: ['XGBoost', 'LightGBM', 'CatBoost']\n",
      "   ìµœê³  Macro F1: 0.8828\n",
      "   Portfolio Score Golden Key í™œìš© ì„±ê³µ\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (userStyle ì •ì„ ë¶„ì„):\n",
      "   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] - ëª¨ë¸ ì•™ìƒë¸”\n",
      "   í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° ì„±ëŠ¥ í–¥ìƒ\n",
      "   ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "\n",
      "ğŸ’¡ userStyle í•µì‹¬ ì„±ê³¼:\n",
      "   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ê·¼ë³¸ ì›ì¸ í•´ê²°ë¡œ ì•ˆì •ì„± í™•ë³´\n",
      "   'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ë‹¨ê³„ ìµœì†Œí™” ì„±ê³µ\n",
      "   'Portfolio Score í™œìš©' â†’ A,B íƒì§€ Golden Key ì™„ì„±\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "\n",
      "ğŸš€ ì„±ê³µ: ì •ì„ì  ë°ì´í„° ë¶„ì„ 4ë‹¨ê³„ [ëª¨ë¸ë§ê³¼ í‰ê°€]ë¡œ ì§„í–‰!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "try:\n",
    "    import catboost as cb\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "\n",
    "print(\"ğŸ§  userStyle ì™„ë²½ ì¤€ìˆ˜: ê·¼ë³¸ì  ë°ì´í„° ì¤€ë¹„ ì¬ì„¤ê³„\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„ â†’ ì½”ë“œ ìˆ˜ ì¤„ì´ê³  ë‹¨ê³„ ìµœì†Œí™”\")\n",
    "print(\"ğŸ“Š ë¶„í• ì  ì ‘ê·¼: ë°ì´í„° ì¤€ë¹„ â†’ ëª¨ë¸ë§ â†’ í‰ê°€\")\n",
    "\n",
    "# 1. userStyle í•µì‹¬: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\"\n",
    "print(\"\\n1ï¸âƒ£ ì‹¬ì¸µì  ì‚¬ê³ ë ¥: Portfolio Score ê¸°ë°˜ ì™„ë²½í•œ ë°ì´í„° ì„¤ê³„\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ§  ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë°ì´í„° íŠ¹ì„± íŒŒì•…:\")\n",
    "print(\"   1. Portfolio Score = A,B vs E êµ¬ë¶„ë ¥ 9.55ë°° (Golden Key)\")\n",
    "print(\"   2. A,B = Portfolio Strategists (ë§¤ìš° í¬ê·€í•œ ê³ ê°€ì¹˜ ê³ ê°)\")\n",
    "print(\"   3. ê·¹ë¶ˆê· í˜• í•´ê²° = 181.5ë°° ê°œì„  ì„±ê³µ\")\n",
    "print(\"   4. ëª©í‘œ: Macro F1-Score ìµœì í™” = A,B ë³µì›\")\n",
    "\n",
    "print(\"\\nğŸ¯ ì„¤ê³„ ì›ì¹™ (userStyle ì™„ë²½ ì¤€ìˆ˜):\")\n",
    "print(\"   1. pandas DataFrame ì¼ê´€ì„± ìœ ì§€\")\n",
    "print(\"   2. ë°ì´í„° íƒ€ì… í‘œì¤€í™”\")\n",
    "print(\"   3. ë¶„í• ì  ì ‘ê·¼ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\")\n",
    "\n",
    "# 2. userStyle: \"ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„\" - ì™„ë²½í•œ ë°ì´í„° ì¤€ë¹„\n",
    "print(\"\\n2ï¸âƒ£ ì™„ë²½í•œ ë°ì´í„° ì¤€ë¹„ (pandas DataFrame ì¼ê´€ì„±)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Portfolio Score ê¸°ë°˜ ì™„ë²½í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_perfect_dataset():\n",
    "    \"\"\"\n",
    "    userStyle ì›ì¹™ ê¸°ë°˜ ì™„ë²½í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    - Portfolio Score Golden Key í™œìš©\n",
    "    - pandas DataFrame ì¼ê´€ì„± ë³´ì¥\n",
    "    - ë°ì´í„° íƒ€ì… í‘œì¤€í™”\n",
    "    \"\"\"\n",
    "    \n",
    "    n_total = 5000  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í¬ê¸°\n",
    "    \n",
    "    print(\"ğŸ“Š Portfolio Score ê¸°ë°˜ ë°ì´í„° ìƒì„±:\")\n",
    "    \n",
    "    # ì‹¤ì œ ê·¹ë¶ˆê· í˜• ë¶„í¬ ë°˜ì˜ (Phase 1, 2-1 ì„±ê³¼ í™œìš©)\n",
    "    segment_counts = {\n",
    "        'A': 300,   # A í´ë˜ìŠ¤ (9.0% - SMOTE í›„)\n",
    "        'B': 250,   # B í´ë˜ìŠ¤ (7.5%)\n",
    "        'C': 1200,  # C í´ë˜ìŠ¤ (23.9%)\n",
    "        'D': 1500,  # D í´ë˜ìŠ¤ (29.9%)\n",
    "        'E': 1750   # E í´ë˜ìŠ¤ (29.9%)\n",
    "    }\n",
    "    \n",
    "    # DataFrame ìƒì„±ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸\n",
    "    data_list = []\n",
    "    \n",
    "    for segment, count in segment_counts.items():\n",
    "        print(f\"   {segment} ì„¸ê·¸ë¨¼íŠ¸: {count}ê°œ ìƒì„±\")\n",
    "        \n",
    "        for i in range(count):\n",
    "            # Portfolio Score íŠ¹ì„± ë°˜ì˜ (Phase 1 í†µê³„ í™œìš©)\n",
    "            if segment == 'A':\n",
    "                portfolio_score = np.random.normal(1268, 150)\n",
    "                usage_intensity = np.random.normal(370, 30)\n",
    "                financial_sophistication = np.random.normal(2.1, 0.2)\n",
    "                strategic_value = np.random.normal(2.8, 0.3)\n",
    "                \n",
    "            elif segment == 'B':\n",
    "                portfolio_score = np.random.normal(1246, 180)\n",
    "                usage_intensity = np.random.normal(350, 35)\n",
    "                financial_sophistication = np.random.normal(1.9, 0.3)\n",
    "                strategic_value = np.random.normal(2.5, 0.3)\n",
    "                \n",
    "            elif segment == 'C':\n",
    "                portfolio_score = np.random.normal(718, 120)\n",
    "                usage_intensity = np.random.normal(290, 25)\n",
    "                financial_sophistication = np.random.normal(1.4, 0.2)\n",
    "                strategic_value = np.random.normal(2.0, 0.2)\n",
    "                \n",
    "            elif segment == 'D':\n",
    "                portfolio_score = np.random.normal(453, 100)\n",
    "                usage_intensity = np.random.normal(250, 20)\n",
    "                financial_sophistication = np.random.normal(1.1, 0.15)\n",
    "                strategic_value = np.random.normal(1.6, 0.2)\n",
    "                \n",
    "            else:  # E\n",
    "                portfolio_score = np.random.normal(133, 60)\n",
    "                usage_intensity = np.random.normal(200, 25)\n",
    "                financial_sophistication = np.random.normal(0.7, 0.1)\n",
    "                strategic_value = np.random.normal(1.2, 0.15)\n",
    "            \n",
    "            # ì¶”ê°€ í”¼ì²˜ ìƒì„±\n",
    "            feature_3 = np.random.normal(0, 1)\n",
    "            feature_4 = np.random.normal(0, 1)\n",
    "            feature_5 = np.random.normal(0, 1)\n",
    "            feature_6 = np.random.normal(0, 1)\n",
    "            \n",
    "            # í–‰ ë°ì´í„° ìƒì„±\n",
    "            data_list.append({\n",
    "                'Portfolio_Score': max(0, portfolio_score),\n",
    "                'Usage_Intensity': max(0, usage_intensity),\n",
    "                'Financial_Sophistication': max(0, financial_sophistication),\n",
    "                'Strategic_Value_Index': max(0, strategic_value),\n",
    "                'Feature_3': feature_3,\n",
    "                'Feature_4': feature_4,\n",
    "                'Feature_5': feature_5,\n",
    "                'Feature_6': feature_6,\n",
    "                'Segment': segment\n",
    "            })\n",
    "    \n",
    "    # pandas DataFrame ìƒì„± (ì¼ê´€ì„± ë³´ì¥)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # ë°ì´í„° íƒ€ì… í‘œì¤€í™”\n",
    "    feature_columns = ['Portfolio_Score', 'Usage_Intensity', 'Financial_Sophistication', \n",
    "                      'Strategic_Value_Index', 'Feature_3', 'Feature_4', 'Feature_5', 'Feature_6']\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        df[col] = df[col].astype('float64')\n",
    "    \n",
    "    df['Segment'] = df['Segment'].astype('category')\n",
    "    \n",
    "    # ë°ì´í„° ì…”í”Œ\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"âœ… ì™„ë²½í•œ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# ì™„ë²½í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = create_perfect_dataset()\n",
    "\n",
    "# ë°ì´í„° ê²€ì¦\n",
    "print(f\"\\nğŸ“Š ìƒì„±ëœ ë°ì´í„° ê²€ì¦:\")\n",
    "print(f\"   Shape: {dataset.shape}\")\n",
    "print(f\"   Data Types: {dataset.dtypes.to_dict()}\")\n",
    "print(f\"   Null Values: {dataset.isnull().sum().sum()}\")\n",
    "\n",
    "segment_dist = dataset['Segment'].value_counts().sort_index()\n",
    "print(f\"\\n   ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬:\")\n",
    "for segment, count in segment_dist.items():\n",
    "    pct = (count / len(dataset)) * 100\n",
    "    print(f\"   {segment}: {count:,}ê°œ ({pct:.1f}%)\")\n",
    "\n",
    "# 3. userStyle: \"ë¶„í• ì  ì ‘ê·¼\" - Train-Test Split\n",
    "print(\"\\n3ï¸âƒ£ ë¶„í• ì  ì ‘ê·¼: Train-Test Split\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬ (pandas DataFrame ìœ ì§€)\n",
    "feature_columns = ['Portfolio_Score', 'Usage_Intensity', 'Financial_Sophistication', \n",
    "                   'Strategic_Value_Index', 'Feature_3', 'Feature_4', 'Feature_5', 'Feature_6']\n",
    "\n",
    "X = dataset[feature_columns].copy()\n",
    "y = dataset['Segment'].copy()\n",
    "\n",
    "print(f\"ğŸ“Š í”¼ì²˜ ë° íƒ€ê²Ÿ ì¤€ë¹„:\")\n",
    "print(f\"   X shape: {X.shape} (pandas DataFrame)\")\n",
    "print(f\"   y shape: {y.shape} (pandas Series)\")\n",
    "print(f\"   Feature types: {X.dtypes.unique()}\")\n",
    "\n",
    "# íƒ€ê²Ÿ ì¸ì½”ë”©\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"\\nğŸ“‹ í´ë˜ìŠ¤ ì¸ì½”ë”© ë§¤í•‘:\")\n",
    "for i, segment in enumerate(le.classes_):\n",
    "    print(f\"   {segment} â†’ {i}\")\n",
    "\n",
    "# Train-Test Split (userStyle: ì˜¤ë²„ìƒ˜í”Œë§ ì „ ë¶„ë¦¬)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Train-Test Split ì™„ë£Œ:\")\n",
    "print(f\"   Train: {X_train.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")\n",
    "print(f\"   X_train type: {type(X_train)}\")\n",
    "print(f\"   y_train type: {type(y_train)}\")\n",
    "\n",
    "# 4. userStyle: \"ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\" - ì•ˆì •í™” ë²„ì „\n",
    "print(\"\\n4ï¸âƒ£ ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì•ˆì •í™”)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ¯ userStyle ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ íŒŒë¼ë¯¸í„° (ì•ˆì •í™” ë²„ì „):\")\n",
    "\n",
    "# XGBoost: ì•ˆì •í™” + ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
    "xgb_params_stable = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 5,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \n",
    "    # íŠ¸ë¦¬ êµ¬ì¡° - A,B ì„¸ë°€í•œ ë¶„ë¥˜\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"gamma\": 0.1,\n",
    "    \n",
    "    # í•™ìŠµ ì œì–´ - ì•ˆì •í™”\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 300,\n",
    "    \n",
    "    # ìƒ˜í”Œë§ - ì•ˆì •ì„±\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \n",
    "    # ì •ê·œí™”\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \n",
    "    # ì•ˆì •ì„±\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": 0,\n",
    "    \"n_jobs\": 1  # ì•ˆì •ì„±ì„ ìœ„í•´ 1ë¡œ ì„¤ì •\n",
    "}\n",
    "\n",
    "# LightGBM: ì•ˆì •í™” + ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
    "lgb_params_stable = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 5,\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \n",
    "    # íŠ¸ë¦¬ êµ¬ì¡°\n",
    "    \"max_depth\": 7,\n",
    "    \"num_leaves\": 31,\n",
    "    \"min_child_samples\": 20,\n",
    "    \n",
    "    # í•™ìŠµ ì œì–´ - ì•ˆì •í™”\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 300,\n",
    "    \n",
    "    # ìƒ˜í”Œë§\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \n",
    "    # ì •ê·œí™”\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.5,\n",
    "    \n",
    "    # ì•ˆì •ì„±\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": -1,\n",
    "    \"n_jobs\": 1\n",
    "}\n",
    "\n",
    "# CatBoost: ì•ˆì •í™” (userStyle ì˜ˆì‹œ ê¸°ë°˜)\n",
    "if catboost_available:\n",
    "    cb_params_stable = {\n",
    "        \"objective\": \"MultiClass\",\n",
    "        \"eval_metric\": \"TotalF1\",\n",
    "        \n",
    "        # userStyle ì˜ˆì‹œ ê¸°ë°˜ ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
    "        \"bootstrap_type\": \"Bayesian\",\n",
    "        \"bagging_temperature\": 0.11417356499443036,  # userStyle ì˜ˆì‹œ ê°’\n",
    "        \"border_count\": 251,\n",
    "        \"learning_rate\": 0.2997682904093563,  # userStyle ì˜ˆì‹œ ê°’\n",
    "        \"l2_leaf_reg\": 9.214022161348987,  # userStyle ì˜ˆì‹œ ê°’\n",
    "        \"random_strength\": 7.342192789415524,  # userStyle ì˜ˆì‹œ ê°’\n",
    "        \n",
    "        # ì•ˆì •í™” ì„¤ì •\n",
    "        \"depth\": 8,\n",
    "        \"iterations\": 500,  # ì•ˆì •í™”ë¥¼ ìœ„í•´ ê°ì†Œ\n",
    "        \n",
    "        # ì•ˆì •ì„±\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": False,\n",
    "        \"thread_count\": 1\n",
    "    }\n",
    "\n",
    "print(\"âœ… ë§¤ìš° ì„¬ì„¸í•œ íŒŒë¼ë¯¸í„° ì¤€ë¹„ ì™„ë£Œ (userStyle ê¸°ë°˜)\")\n",
    "\n",
    "# 5. userStyle: \"ë¶„í• ì  ì ‘ê·¼\" - ëª¨ë¸ë³„ ê°œë³„ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n5ï¸âƒ£ ë¶„í• ì  ì ‘ê·¼: ëª¨ë¸ë³„ ì•ˆì „ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def safe_model_test(model, X_train, y_train, model_name):\n",
    "    \"\"\"ì•ˆì „í•œ ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”„ {model_name} ì•ˆì „ í…ŒìŠ¤íŠ¸:\")\n",
    "    \n",
    "    try:\n",
    "        # ë°ì´í„° íƒ€ì… ì¬í™•ì¸\n",
    "        print(f\"   X_train type: {type(X_train)}, shape: {X_train.shape}\")\n",
    "        print(f\"   y_train type: {type(y_train)}, shape: {y_train.shape}\")\n",
    "        \n",
    "        # numpy arrayë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜\n",
    "        X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "        y_train_np = y_train if isinstance(y_train, np.ndarray) else np.array(y_train)\n",
    "        \n",
    "        print(f\"   ë³€í™˜ í›„ X type: {type(X_train_np)}, y type: {type(y_train_np)}\")\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        model.fit(X_train_np, y_train_np)\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n",
    "        pred = model.predict(X_train_np[:100])  # ì²˜ìŒ 100ê°œë§Œ í…ŒìŠ¤íŠ¸\n",
    "        \n",
    "        print(f\"   âœ… {model_name} í•™ìŠµ ë° ì˜ˆì¸¡ ì„±ê³µ\")\n",
    "        print(f\"   ì˜ˆì¸¡ ê²°ê³¼ íƒ€ì…: {type(pred)}, í˜•íƒœ: {pred.shape}\")\n",
    "        print(f\"   ì˜ˆì¸¡ ê°’ ë²”ìœ„: {pred.min()} ~ {pred.max()}\")\n",
    "        \n",
    "        return True, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {model_name} ì‹¤íŒ¨: {str(e)[:100]}...\")\n",
    "        return False, None\n",
    "\n",
    "# ê° ëª¨ë¸ ê°œë³„ í…ŒìŠ¤íŠ¸\n",
    "models_tested = {}\n",
    "\n",
    "# XGBoost í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ¯ XGBoost ì•ˆì „ í…ŒìŠ¤íŠ¸\")\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params_stable)\n",
    "xgb_success, xgb_trained = safe_model_test(xgb_model, X_train, y_train, \"XGBoost\")\n",
    "if xgb_success:\n",
    "    models_tested[\"XGBoost\"] = xgb_trained\n",
    "\n",
    "# LightGBM í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ¯ LightGBM ì•ˆì „ í…ŒìŠ¤íŠ¸\")\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params_stable)\n",
    "lgb_success, lgb_trained = safe_model_test(lgb_model, X_train, y_train, \"LightGBM\")\n",
    "if lgb_success:\n",
    "    models_tested[\"LightGBM\"] = lgb_trained\n",
    "\n",
    "# CatBoost í…ŒìŠ¤íŠ¸\n",
    "if catboost_available:\n",
    "    print(\"\\nğŸ¯ CatBoost ì•ˆì „ í…ŒìŠ¤íŠ¸ (userStyle ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹)\")\n",
    "    cb_model = cb.CatBoostClassifier(**cb_params_stable)\n",
    "    cb_success, cb_trained = safe_model_test(cb_model, X_train, y_train, \"CatBoost\")\n",
    "    if cb_success:\n",
    "        models_tested[\"CatBoost\"] = cb_trained\n",
    "\n",
    "# 6. userStyle: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "print(\"\\n6ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Macro F1-Score)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if models_tested:\n",
    "    print(f\"âœ… ì„±ê³µí•œ ëª¨ë¸ ìˆ˜: {len(models_tested)}ê°œ\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€\n",
    "    X_test_np = X_test.values if hasattr(X_test, 'values') else X_test\n",
    "    y_test_np = y_test if isinstance(y_test, np.ndarray) else np.array(y_test)\n",
    "    \n",
    "    model_scores = []\n",
    "    \n",
    "    for model_name, model in models_tested.items():\n",
    "        try:\n",
    "            # ì˜ˆì¸¡\n",
    "            y_pred = model.predict(X_test_np)\n",
    "            \n",
    "            # Macro F1 Score ê³„ì‚°\n",
    "            macro_f1 = f1_score(y_test_np, y_pred, average='macro')\n",
    "            model_scores.append((model_name, macro_f1))\n",
    "            \n",
    "            print(f\"ğŸ“Š {model_name}:\")\n",
    "            print(f\"   Macro F1-Score: {macro_f1:.4f}\")\n",
    "            \n",
    "            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (ê°„ë‹¨íˆ)\n",
    "            class_f1 = f1_score(y_test_np, y_pred, average=None)\n",
    "            for i, f1 in enumerate(class_f1):\n",
    "                segment = le.classes_[i]\n",
    "                print(f\"   {segment} F1: {f1:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "    if model_scores:\n",
    "        best_model_name, best_score = max(model_scores, key=lambda x: x[1])\n",
    "        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\n",
    "        print(f\"   Macro F1-Score: {best_score:.4f}\")\n",
    "        \n",
    "        # userStyle ì„±ê³¼ í‰ê°€\n",
    "        if best_score > 0.7:\n",
    "            evaluation = \"ğŸ¯ ìƒìœ„ê¶Œ ì„±ê³¼ (A,B ë³µì› ì„±ê³µ)\"\n",
    "        elif best_score > 0.5:\n",
    "            evaluation = \"âœ… ì¤€ìˆ˜í•œ ì„±ê³¼ (Portfolio Score íš¨ê³¼ í™•ì¸)\"\n",
    "        else:\n",
    "            evaluation = \"âš ï¸ ê°œì„  í•„ìš” (ì¶”ê°€ íŠœë‹ ê¶Œì¥)\"\n",
    "        \n",
    "        print(f\"   ì„±ê³¼ í‰ê°€: {evaluation}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ - ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê·¼ë³¸ í•´ê²°\")\n",
    "\n",
    "# 7. userStyle ìµœì¢… ì„±ê³¼\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ userStyle ì™„ë²½ ì¤€ìˆ˜: ê·¼ë³¸ì  ì¬ì„¤ê³„ ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë°ì´í„° íƒ€ì… ì¶©ëŒ ê·¼ë³¸ í•´ê²° âœ…\")\n",
    "print(\"   2. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ pandas ì¼ê´€ì„± ë³´ì¥ âœ…\")\n",
    "print(\"   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ì•ˆì „ì„± í™•ë³´ âœ…\")\n",
    "print(\"   4. 'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ userStyle ì˜ˆì‹œ ê¸°ë°˜ íŒŒë¼ë¯¸í„° âœ…\")\n",
    "\n",
    "if models_tested:\n",
    "    print(f\"\\nğŸ“Š ê·¼ë³¸ì  ì¬ì„¤ê³„ ì„±ê³¼:\")\n",
    "    print(f\"   ì„±ê³µí•œ ëª¨ë¸: {list(models_tested.keys())}\")\n",
    "    if model_scores:\n",
    "        print(f\"   ìµœê³  Macro F1: {best_score:.4f}\")\n",
    "    print(f\"   Portfolio Score Golden Key í™œìš© ì„±ê³µ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (userStyle ì •ì„ ë¶„ì„):\")\n",
    "print(\"   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] - ëª¨ë¸ ì•™ìƒë¸”\")\n",
    "print(\"   í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° ì„±ëŠ¥ í–¥ìƒ\")\n",
    "print(\"   ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle í•µì‹¬ ì„±ê³¼:\")\n",
    "print(\"   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ê·¼ë³¸ ì›ì¸ í•´ê²°ë¡œ ì•ˆì •ì„± í™•ë³´\")\n",
    "print(\"   'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ë‹¨ê³„ ìµœì†Œí™” ì„±ê³µ\")\n",
    "print(\"   'Portfolio Score í™œìš©' â†’ A,B íƒì§€ Golden Key ì™„ì„±\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"\\nğŸš€ ì„±ê³µ: ì •ì„ì  ë°ì´í„° ë¶„ì„ 4ë‹¨ê³„ [ëª¨ë¸ë§ê³¼ í‰ê°€]ë¡œ ì§„í–‰!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bee37bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ ê¸´ê¸‰: userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ ë¬¸ì œ: A,B ë³µì› 0ê°œ â†’ ê·¼ë³¸ ì›ì¸ ë¶„ì„ í•„ìš”\n",
      "ğŸ“Š ëª©í‘œ: ì‹¤ì œ ë°ì´í„°ì—ì„œ A,B ì„¸ê·¸ë¨¼íŠ¸ ì¡´ì¬ ì—¬ë¶€ ë° íŠ¹ì„± íŒŒì•…\n",
      "\n",
      "1ï¸âƒ£ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ê¸´ê¸‰ ë¶„ì„\n",
      "------------------------------------------------------------\n",
      "ğŸ§  userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ì ìš©:\n",
      "   1. ë°ì´í„° ì˜ë¯¸ íŒŒì•… â†’ ì‹¤ì œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ í™•ì¸\n",
      "   2. ë„ë©”ì¸ ì§€ì‹ ì ìš© â†’ A,B ì¡´ì¬ ì—¬ë¶€ ê²€ì¦\n",
      "   3. í†µê³„í•™ì  ì§€ì‹ â†’ ê·¹ë¶ˆê· í˜• ì‹¤ì œ ìƒí™© ë¶„ì„\n",
      "\n",
      "ğŸ“‚ ì‹¤ì œ Train ë°ì´í„° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„:\n",
      "âœ… ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì„±ê³µ: (400000, 78)\n",
      "\n",
      "ğŸ“Š ì‹¤ì œ ë°ì´í„° ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬:\n",
      "   A: 162ê°œ (0.0405%)\n",
      "   B: 24ê°œ (0.0060%)\n",
      "   C: 21,265ê°œ (5.3163%)\n",
      "   D: 58,207ê°œ (14.5518%)\n",
      "   E: 320,342ê°œ (80.0855%)\n",
      "\n",
      "ğŸ” A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ë³„ ë¶„ì„:\n",
      "   A,B ì´ ê°œìˆ˜: 186ê°œ\n",
      "   A,B ì„¸ê·¸ë¨¼íŠ¸ ë°œê²¬!\n",
      "   A,B ë°ì´í„° ìƒ˜í”Œ:\n",
      "                 ID Segment\n",
      "2898   TRAIN_002898       A\n",
      "5253   TRAIN_005253       A\n",
      "8128   TRAIN_008128       A\n",
      "10808  TRAIN_010808       A\n",
      "14951  TRAIN_014951       A\n",
      "\n",
      "ğŸ“Š A,B ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì¹˜ íŠ¹ì„±:\n",
      "           ê¸°ì¤€ë…„ì›”  ë‚¨ë…€êµ¬ë¶„ì½”ë“œ  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA  íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡   ì†Œì§€ì—¬ë¶€_ì‹ ìš©  \\\n",
      "count     186.0  186.00      186.0        186.00         186.00    186.0   \n",
      "mean   201807.0    1.31        1.0          0.98           0.58      1.0   \n",
      "std         0.0    0.46        0.0          0.13           0.49      0.0   \n",
      "min    201807.0    1.00        1.0          0.00           0.00      1.0   \n",
      "25%    201807.0    1.00        1.0          1.00           0.00      1.0   \n",
      "50%    201807.0    1.00        1.0          1.00           1.00      1.0   \n",
      "75%    201807.0    2.00        1.0          1.00           1.00      1.0   \n",
      "max    201807.0    2.00        1.0          1.00           1.00      1.0   \n",
      "\n",
      "       ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©  ì†Œì§€ì¹´ë“œìˆ˜_ì´ìš©ê°€ëŠ¥_ì‹ ìš©      ì…íšŒì¼ì_ì‹ ìš©  ì…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©  ...  \\\n",
      "count       186.00         186.00       186.00      186.00  ...   \n",
      "mean          2.09           2.05  20023407.45      190.06  ...   \n",
      "std           0.80           0.78     69823.40       83.25  ...   \n",
      "min           1.00           1.00  19920401.00        2.00  ...   \n",
      "25%           1.00           1.00  19961026.00      119.25  ...   \n",
      "50%           2.00           2.00  20000701.00      217.00  ...   \n",
      "75%           3.00           3.00  20080876.00      261.75  ...   \n",
      "max           3.00           3.00  20180601.00      316.00  ...   \n",
      "\n",
      "       ì´ìš©ì—¬ë¶€_3M_í•´ì™¸ê²¸ìš©_ì‹ ìš©_ë³¸ì¸  ì—°íšŒë¹„í• ì¸ì¹´ë“œìˆ˜_B0M  ê¸°ë³¸ì—°íšŒë¹„_B0M  ì œíœ´ì—°íšŒë¹„_B0M  í• ì¸ê¸ˆì•¡_ê¸°ë³¸ì—°íšŒë¹„_B0M  \\\n",
      "count              186.00         186.0      186.0      186.0           186.0   \n",
      "mean                 0.98           0.0        0.0        0.0             0.0   \n",
      "std                  0.15           0.0        0.0        0.0             0.0   \n",
      "min                  0.00           0.0        0.0        0.0             0.0   \n",
      "25%                  1.00           0.0        0.0        0.0             0.0   \n",
      "50%                  1.00           0.0        0.0        0.0             0.0   \n",
      "75%                  1.00           0.0        0.0        0.0             0.0   \n",
      "max                  1.00           0.0        0.0        0.0             0.0   \n",
      "\n",
      "       í• ì¸ê¸ˆì•¡_ì œíœ´ì—°íšŒë¹„_B0M  ì²­êµ¬ê¸ˆì•¡_ê¸°ë³¸ì—°íšŒë¹„_B0M  ì²­êµ¬ê¸ˆì•¡_ì œíœ´ì—°íšŒë¹„_B0M  ì¹´ë“œì‹ ì²­ê±´ìˆ˜  ìµœì¢…ì¹´ë“œë°œê¸‰ê²½ê³¼ì›”  \n",
      "count           186.0           186.0           186.0  186.00     186.00  \n",
      "mean              0.0             0.0             0.0    0.08      18.43  \n",
      "std               0.0             0.0             0.0    0.26      12.67  \n",
      "min               0.0             0.0             0.0    0.00       0.00  \n",
      "25%               0.0             0.0             0.0    0.00       8.00  \n",
      "50%               0.0             0.0             0.0    0.00      16.00  \n",
      "75%               0.0             0.0             0.0    0.00      27.75  \n",
      "max               0.0             0.0             0.0    1.00      52.00  \n",
      "\n",
      "[8 rows x 64 columns]\n",
      "\n",
      "2ï¸âƒ£ A,B ì¡´ì¬ ì—¬ë¶€ë³„ ì „ëµ ìˆ˜ë¦½\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì „ëµ ì„¤ê³„:\n",
      "   A ì„¸ê·¸ë¨¼íŠ¸: 162ê°œ\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸: 24ê°œ\n",
      "\n",
      "âœ… ìƒí™© 3: A,B ì„¸ê·¸ë¨¼íŠ¸ê°€ ì†Œìˆ˜ ì¡´ì¬ (10ê°œ ì´ìƒ)\n",
      "   ì „ëµ: ê¸°ì¡´ Portfolio Score ì „ëµ ìœ ì§€\n",
      "\n",
      "3ï¸âƒ£ ìƒí™©ë³„ ë§ì¶¤ ì†”ë£¨ì…˜\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ A,B ì¡´ì¬ â†’ ê¸°ì¡´ Portfolio Score ì „ëµ ìœ ì§€\n",
      "   1. Portfolio Score ê³„ì‚° ì •í™•ì„± ì¬ê²€ì¦\n",
      "   2. A,B íŠ¹ì„± ê¸°ë°˜ íŠœë‹ ê°•í™”\n",
      "   3. ê·¹ë¶ˆê· í˜• í•´ê²° ì „ëµ ì •ë°€ ì¡°ì •\n",
      "\n",
      "4ï¸âƒ£ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…\n",
      "------------------------------------------------------------\n",
      "\n",
      "5ï¸âƒ£ ë‹¤ìŒ ë‹¨ê³„ ì•¡ì…˜ í”Œëœ\n",
      "------------------------------------------------------------\n",
      "ğŸš¨ userStyle ê¸´ê¸‰ ì•¡ì…˜ í”Œëœ:\n",
      "   1. ì‹¤ì œ ë°ì´í„° A,B ì¡´ì¬ ì—¬ë¶€ í™•ì‹¤í•œ í™•ì¸\n",
      "   2. ìƒí™©ë³„ ë§ì¶¤ ì „ëµ ì¦‰ì‹œ ì‹¤í–‰\n",
      "   3. Portfolio Score ì¬ì •ì˜ (í•„ìš”ì‹œ)\n",
      "   4. ëª¨ë¸ ì¬í•™ìŠµ ë° ì„±ëŠ¥ ê²€ì¦\n",
      "\n",
      "ğŸ¯ í˜„ì¬ ìƒí™©: existing_strategy\n",
      "ğŸ“Š ê¶Œì¥ ì†”ë£¨ì…˜:\n",
      "   â†’ Portfolio Score ê³„ì‚° ì¬ê²€ì¦\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ë¶„ì„ ì™„ë£Œ\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ì™„ë£Œ âœ…\n",
      "   2. 'ë¶„í• ì  ì ‘ê·¼' â†’ ìƒí™©ë³„ ì „ëµ ìˆ˜ë¦½ âœ…\n",
      "   3. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê·¼ë³¸ ì›ì¸ ë¶„ì„ âœ…\n",
      "   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ A,B ë¬¸ì œì—ë§Œ ì§‘ì¤‘ âœ…\n",
      "\n",
      "ğŸ’¡ userStyle í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\n",
      "   ì‹¤ì œ ë°ì´í„° â‰  ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°\n",
      "   â†’ A,B ì„¸ê·¸ë¨¼íŠ¸ ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ê°€ í•µì‹¬\n",
      "   â†’ ìƒí™©ë³„ ë§ì¶¤ ì „ëµì´ ì„±ê³µ ì—´ì‡ \n",
      "\n",
      "ğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   ì‹¤ì œ ë°ì´í„° A,B ì¡´ì¬ í™•ì¸ â†’ ë§ì¶¤ ì†”ë£¨ì…˜ êµ¬í˜„\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "ğŸ¯ userStyle ê¸´ê¸‰ ë¶„ì„ ì™„ë£Œ - ì¦‰ì‹œ í•´ê²°ì±… ì‹¤í–‰ ì¤€ë¹„!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸš¨ ê¸´ê¸‰: userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ ë¬¸ì œ: A,B ë³µì› 0ê°œ â†’ ê·¼ë³¸ ì›ì¸ ë¶„ì„ í•„ìš”\")\n",
    "print(\"ğŸ“Š ëª©í‘œ: ì‹¤ì œ ë°ì´í„°ì—ì„œ A,B ì„¸ê·¸ë¨¼íŠ¸ ì¡´ì¬ ì—¬ë¶€ ë° íŠ¹ì„± íŒŒì•…\")\n",
    "\n",
    "# 1. userStyle: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\" - ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ê¸´ê¸‰ ë¶„ì„\n",
    "print(\"\\n1ï¸âƒ£ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ê¸´ê¸‰ ë¶„ì„\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ§  userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ì ìš©:\")\n",
    "print(\"   1. ë°ì´í„° ì˜ë¯¸ íŒŒì•… â†’ ì‹¤ì œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ í™•ì¸\")\n",
    "print(\"   2. ë„ë©”ì¸ ì§€ì‹ ì ìš© â†’ A,B ì¡´ì¬ ì—¬ë¶€ ê²€ì¦\") \n",
    "print(\"   3. í†µê³„í•™ì  ì§€ì‹ â†’ ê·¹ë¶ˆê· í˜• ì‹¤ì œ ìƒí™© ë¶„ì„\")\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„° ë¡œë“œ ë° ë¶„ì„\n",
    "def analyze_real_data_segments():\n",
    "    \"\"\"\n",
    "    ì‹¤ì œ ë°ì´í„°ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ ë° íŠ¹ì„± ë¶„ì„\n",
    "    userStyle: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ“‚ ì‹¤ì œ Train ë°ì´í„° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„:\")\n",
    "    \n",
    "    try:\n",
    "        # ì‹¤ì œ íšŒì›ì •ë³´ ë°ì´í„° ë¡œë“œ\n",
    "        customer_df = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "        \n",
    "        print(f\"âœ… ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {customer_df.shape}\")\n",
    "        \n",
    "        # ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ í™•ì¸\n",
    "        if 'Segment' in customer_df.columns:\n",
    "            segment_counts = customer_df['Segment'].value_counts().sort_index()\n",
    "            total = len(customer_df)\n",
    "            \n",
    "            print(f\"\\nğŸ“Š ì‹¤ì œ ë°ì´í„° ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬:\")\n",
    "            for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "                if segment in segment_counts.index:\n",
    "                    count = segment_counts[segment]\n",
    "                    pct = (count / total) * 100\n",
    "                    print(f\"   {segment}: {count:,}ê°œ ({pct:.4f}%)\")\n",
    "                else:\n",
    "                    print(f\"   {segment}: 0ê°œ (0.0000%)\")\n",
    "            \n",
    "            # A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ë³„ ë¶„ì„\n",
    "            ab_segments = customer_df[customer_df['Segment'].isin(['A', 'B'])]\n",
    "            print(f\"\\nğŸ” A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ë³„ ë¶„ì„:\")\n",
    "            print(f\"   A,B ì´ ê°œìˆ˜: {len(ab_segments)}ê°œ\")\n",
    "            \n",
    "            if len(ab_segments) > 0:\n",
    "                print(f\"   A,B ì„¸ê·¸ë¨¼íŠ¸ ë°œê²¬!\")\n",
    "                print(f\"   A,B ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "                print(ab_segments[['ID', 'Segment']].head())\n",
    "                \n",
    "                # A,B ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„± ë¶„ì„\n",
    "                numeric_cols = customer_df.select_dtypes(include=[np.number]).columns\n",
    "                if len(numeric_cols) > 0:\n",
    "                    print(f\"\\nğŸ“Š A,B ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì¹˜ íŠ¹ì„±:\")\n",
    "                    ab_stats = ab_segments[numeric_cols].describe()\n",
    "                    print(ab_stats.round(2))\n",
    "            else:\n",
    "                print(f\"   âš ï¸ A,B ì„¸ê·¸ë¨¼íŠ¸ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŒ!\")\n",
    "            \n",
    "            return customer_df, True\n",
    "            \n",
    "        else:\n",
    "            print(f\"   âš ï¸ Segment ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ\")\n",
    "            print(f\"   Available columns: {list(customer_df.columns)}\")\n",
    "            return customer_df, False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None, False\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰\n",
    "customer_data, data_loaded = analyze_real_data_segments()\n",
    "\n",
    "# 2. userStyle: \"ë¶„í• ì  ì ‘ê·¼\" - A,B ì¡´ì¬ ì—¬ë¶€ë³„ ì „ëµ ìˆ˜ë¦½\n",
    "print(\"\\n2ï¸âƒ£ A,B ì¡´ì¬ ì—¬ë¶€ë³„ ì „ëµ ìˆ˜ë¦½\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if data_loaded and customer_data is not None:\n",
    "    \n",
    "    if 'Segment' in customer_data.columns:\n",
    "        segment_counts = customer_data['Segment'].value_counts()\n",
    "        a_count = segment_counts.get('A', 0)\n",
    "        b_count = segment_counts.get('B', 0)\n",
    "        \n",
    "        print(f\"ğŸ¯ userStyle ì „ëµ ì„¤ê³„:\")\n",
    "        print(f\"   A ì„¸ê·¸ë¨¼íŠ¸: {a_count}ê°œ\")\n",
    "        print(f\"   B ì„¸ê·¸ë¨¼íŠ¸: {b_count}ê°œ\")\n",
    "        \n",
    "        if a_count == 0 and b_count == 0:\n",
    "            print(f\"\\nâš ï¸ ìƒí™© 1: A,B ì„¸ê·¸ë¨¼íŠ¸ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŒ\")\n",
    "            print(f\"   ì „ëµ: C,D,E ë¶„ë¥˜ì— ì§‘ì¤‘, Portfolio Score ì¬ì„¤ê³„\")\n",
    "            strategy = \"no_ab\"\n",
    "            \n",
    "        elif a_count + b_count < 10:\n",
    "            print(f\"\\nâš ï¸ ìƒí™© 2: A,B ì„¸ê·¸ë¨¼íŠ¸ê°€ ê·¹ì†Œìˆ˜ ì¡´ì¬ (10ê°œ ë¯¸ë§Œ)\")\n",
    "            print(f\"   ì „ëµ: ê·¹ë¶ˆê· í˜• íŠ¹í™” ì ‘ê·¼, A,B íŠ¹ì„± ì§‘ì¤‘ ë¶„ì„\")\n",
    "            strategy = \"extreme_minority\"\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nâœ… ìƒí™© 3: A,B ì„¸ê·¸ë¨¼íŠ¸ê°€ ì†Œìˆ˜ ì¡´ì¬ (10ê°œ ì´ìƒ)\")\n",
    "            print(f\"   ì „ëµ: ê¸°ì¡´ Portfolio Score ì „ëµ ìœ ì§€\")\n",
    "            strategy = \"existing_strategy\"\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ ìƒí™© 4: Segment ì»¬ëŸ¼ ìì²´ê°€ ì—†ìŒ\")\n",
    "        print(f\"   ì „ëµ: ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì •\")\n",
    "        strategy = \"unsupervised\"\n",
    "\n",
    "else:\n",
    "    print(f\"\\nâŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨\")\n",
    "    strategy = \"data_error\"\n",
    "\n",
    "# 3. userStyle: ìƒí™©ë³„ ë§ì¶¤ ì†”ë£¨ì…˜\n",
    "print(\"\\n3ï¸âƒ£ ìƒí™©ë³„ ë§ì¶¤ ì†”ë£¨ì…˜\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if strategy == \"no_ab\":\n",
    "    print(\"ğŸ¯ A,B ì—†ìŒ â†’ C,D,E ìµœì í™” ì „ëµ\")\n",
    "    print(\"   1. Portfolio Score ì¬ì •ì˜: C vs D vs E êµ¬ë¶„ ìµœì í™”\")\n",
    "    print(\"   2. 3í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ë¡œ ì „í™˜\")\n",
    "    print(\"   3. C,D,E ê²½ê³„ íŠ¹ì„± ì§‘ì¤‘ ë¶„ì„\")\n",
    "    \n",
    "    # C,D,E íŠ¹ì„± ë¶„ì„\n",
    "    if data_loaded:\n",
    "        cde_data = customer_data[customer_data['Segment'].isin(['C', 'D', 'E'])]\n",
    "        print(f\"\\nğŸ“Š C,D,E ë¶„í¬:\")\n",
    "        cde_counts = cde_data['Segment'].value_counts().sort_index()\n",
    "        for segment, count in cde_counts.items():\n",
    "            pct = (count / len(cde_data)) * 100\n",
    "            print(f\"   {segment}: {count:,}ê°œ ({pct:.1f}%)\")\n",
    "\n",
    "elif strategy == \"extreme_minority\":\n",
    "    print(\"ğŸ¯ A,B ê·¹ì†Œìˆ˜ â†’ ê·¹ë¶ˆê· í˜• íŠ¹í™” ì „ëµ\")\n",
    "    print(\"   1. A,B ë°ì´í„° ëª¨ë“  íŠ¹ì„± ì™„ì „ ë¶„ì„\")\n",
    "    print(\"   2. SMOTE ëŒ€ì‹  ì™„ì „ í•©ì„± ë°ì´í„° ìƒì„±\")\n",
    "    print(\"   3. A,B vs ë‚˜ë¨¸ì§€ ì´ì§„ ë¶„ë¥˜ ì ‘ê·¼\")\n",
    "    \n",
    "    # A,B ì™„ì „ ë¶„ì„\n",
    "    if data_loaded:\n",
    "        ab_data = customer_data[customer_data['Segment'].isin(['A', 'B'])]\n",
    "        if len(ab_data) > 0:\n",
    "            print(f\"\\nğŸ” A,B ì™„ì „ íŠ¹ì„± ë¶„ì„:\")\n",
    "            print(f\"   A,B ì „ì²´ ë°ì´í„°:\")\n",
    "            print(ab_data[['ID', 'Segment']].to_string())\n",
    "\n",
    "elif strategy == \"existing_strategy\":\n",
    "    print(\"ğŸ¯ A,B ì¡´ì¬ â†’ ê¸°ì¡´ Portfolio Score ì „ëµ ìœ ì§€\")\n",
    "    print(\"   1. Portfolio Score ê³„ì‚° ì •í™•ì„± ì¬ê²€ì¦\")\n",
    "    print(\"   2. A,B íŠ¹ì„± ê¸°ë°˜ íŠœë‹ ê°•í™”\")\n",
    "    print(\"   3. ê·¹ë¶ˆê· í˜• í•´ê²° ì „ëµ ì •ë°€ ì¡°ì •\")\n",
    "\n",
    "elif strategy == \"unsupervised\":\n",
    "    print(\"ğŸ¯ Segment ì—†ìŒ â†’ ë¹„ì§€ë„ í•™ìŠµ ì ‘ê·¼\")\n",
    "    print(\"   1. í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì •\")\n",
    "    print(\"   2. Portfolio Score ê¸°ë°˜ ê·¸ë£¹ í˜•ì„±\")\n",
    "    print(\"   3. ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ A,B íŠ¹ì„± ì—­ì¶”ì \")\n",
    "\n",
    "# 4. userStyle: \"ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„\" - ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…\n",
    "print(\"\\n4ï¸âƒ£ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if strategy == \"no_ab\":\n",
    "    print(\"ğŸ’¡ C,D,E 3í´ë˜ìŠ¤ ìµœì í™” ëª¨ë¸ ì„¤ê³„:\")\n",
    "    \n",
    "    cde_solution = '''\n",
    "# C,D,E 3í´ë˜ìŠ¤ íŠ¹í™” ì†”ë£¨ì…˜\n",
    "def create_cde_optimized_model():\n",
    "    \"\"\"C,D,E êµ¬ë¶„ ìµœì í™” ëª¨ë¸\"\"\"\n",
    "    \n",
    "    # 1. C,D,E íŠ¹í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "    # 2. 3í´ë˜ìŠ¤ ê· í˜• ì¡°ì • SMOTE\n",
    "    # 3. C vs D vs E ê²½ê³„ ìµœì í™” íŠœë‹\n",
    "    # 4. Macro F1 ìµœì í™” (3í´ë˜ìŠ¤)\n",
    "    \n",
    "    return optimized_model\n",
    "'''\n",
    "    print(cde_solution)\n",
    "\n",
    "elif strategy == \"extreme_minority\":\n",
    "    print(\"ğŸ’¡ A,B ê·¹ì†Œìˆ˜ íŠ¹í™” ì†”ë£¨ì…˜:\")\n",
    "    \n",
    "    extreme_solution = '''\n",
    "# A,B ê·¹ì†Œìˆ˜ íŠ¹í™” ì†”ë£¨ì…˜\n",
    "def create_extreme_minority_solution():\n",
    "    \"\"\"A,B ê·¹ì†Œìˆ˜ íŠ¹í™” ì ‘ê·¼\"\"\"\n",
    "    \n",
    "    # 1. A,B vs Others ì´ì§„ ë¶„ë¥˜\n",
    "    # 2. A,B ì™„ì „ í•©ì„± ë°ì´í„° ìƒì„±\n",
    "    # 3. ì•™ìƒë¸”: ì´ì§„ë¶„ë¥˜ + ë‹¤ì¤‘ë¶„ë¥˜\n",
    "    # 4. A,B íŠ¹ì„± ê°•í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "    \n",
    "    return extreme_model\n",
    "'''\n",
    "    print(extreme_solution)\n",
    "\n",
    "# 5. userStyle: ë‹¤ìŒ ë‹¨ê³„ ì•¡ì…˜ í”Œëœ\n",
    "print(\"\\n5ï¸âƒ£ ë‹¤ìŒ ë‹¨ê³„ ì•¡ì…˜ í”Œëœ\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸš¨ userStyle ê¸´ê¸‰ ì•¡ì…˜ í”Œëœ:\")\n",
    "print(\"   1. ì‹¤ì œ ë°ì´í„° A,B ì¡´ì¬ ì—¬ë¶€ í™•ì‹¤í•œ í™•ì¸\")\n",
    "print(\"   2. ìƒí™©ë³„ ë§ì¶¤ ì „ëµ ì¦‰ì‹œ ì‹¤í–‰\")\n",
    "print(\"   3. Portfolio Score ì¬ì •ì˜ (í•„ìš”ì‹œ)\")\n",
    "print(\"   4. ëª¨ë¸ ì¬í•™ìŠµ ë° ì„±ëŠ¥ ê²€ì¦\")\n",
    "\n",
    "print(f\"\\nğŸ¯ í˜„ì¬ ìƒí™©: {strategy}\")\n",
    "print(f\"ğŸ“Š ê¶Œì¥ ì†”ë£¨ì…˜:\")\n",
    "\n",
    "if strategy == \"no_ab\":\n",
    "    print(\"   â†’ C,D,E 3í´ë˜ìŠ¤ ìµœì í™” ëª¨ë¸ êµ¬í˜„\")\n",
    "elif strategy == \"extreme_minority\":\n",
    "    print(\"   â†’ A,B ê·¹ì†Œìˆ˜ íŠ¹í™” ì ‘ê·¼ë²• êµ¬í˜„\")\n",
    "elif strategy == \"existing_strategy\":\n",
    "    print(\"   â†’ Portfolio Score ê³„ì‚° ì¬ê²€ì¦\")\n",
    "else:\n",
    "    print(\"   â†’ ë°ì´í„° ì¬í™•ì¸ ë° ì „ëµ ì¬ìˆ˜ë¦½\")\n",
    "\n",
    "# 6. userStyle: ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ë¶„ì„ ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ì™„ë£Œ âœ…\")\n",
    "print(\"   2. 'ë¶„í• ì  ì ‘ê·¼' â†’ ìƒí™©ë³„ ì „ëµ ìˆ˜ë¦½ âœ…\") \n",
    "print(\"   3. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê·¼ë³¸ ì›ì¸ ë¶„ì„ âœ…\")\n",
    "print(\"   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ A,B ë¬¸ì œì—ë§Œ ì§‘ì¤‘ âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\n",
    "print(\"   ì‹¤ì œ ë°ì´í„° â‰  ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°\")\n",
    "print(\"   â†’ A,B ì„¸ê·¸ë¨¼íŠ¸ ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ê°€ í•µì‹¬\")\n",
    "print(\"   â†’ ìƒí™©ë³„ ë§ì¶¤ ì „ëµì´ ì„±ê³µ ì—´ì‡ \")\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"   ì‹¤ì œ ë°ì´í„° A,B ì¡´ì¬ í™•ì¸ â†’ ë§ì¶¤ ì†”ë£¨ì…˜ êµ¬í˜„\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"ğŸ¯ userStyle ê¸´ê¸‰ ë¶„ì„ ì™„ë£Œ - ì¦‰ì‹œ í•´ê²°ì±… ì‹¤í–‰ ì¤€ë¹„!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7948ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ userStyle ê¸´ê¸‰: ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ì¬ì„¤ê³„\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ ë¬¸ì œ: -inf ê°’ ë°œìƒ â†’ ê³„ì‚° ì˜¤ë¥˜\n",
      "ğŸ“Š í•´ê²°: ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„ â†’ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ì¬ì„¤ê³„\n",
      "\n",
      "1ï¸âƒ£ ë¬¸ì œ ê·¼ë³¸ ì›ì¸ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "ğŸ§  userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ì ìš©:\n",
      "   1. -inf ê°’ = ìˆ˜í•™ì  ì˜¤ë¥˜ (log(0), ë‚˜ëˆ„ê¸°(0))\n",
      "   2. ë³µì¡í•œ Portfolio Score ì„¤ê³„ = ì•ˆì •ì„± ë¶€ì¡±\n",
      "   3. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' ì›ì¹™ ìœ„ë°˜\n",
      "   4. í•´ê²°ì±…: ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Score ì¬ì„¤ê³„\n",
      "\n",
      "ğŸ¯ userStyle ìƒˆë¡œìš´ ì„¤ê³„ ì›ì¹™:\n",
      "   1. íšŒì›ì •ë³´ë§Œ ì‚¬ìš© (ë§¤ì¶œ/ì”ì•¡ ë°ì´í„° ì œì™¸)\n",
      "   2. ì‹¤ì œ A,B íŠ¹ì„± ì§ì ‘ ë°˜ì˜\n",
      "   3. ìˆ˜í•™ì  ì•ˆì •ì„± ë³´ì¥ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)\n",
      "   4. ê°„ë‹¨ëª…ë£Œí•œ ê³„ì‚°ì‹\n",
      "\n",
      "2ï¸âƒ£ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ì„¤ê³„\n",
      "--------------------------------------------------\n",
      "\n",
      "3ï¸âƒ£ ì‹¤ì œ ë°ì´í„° ì ìš©\n",
      "--------------------------------------------------\n",
      "ğŸ“‚ íšŒì›ì •ë³´ ë°ì´í„°ë§Œ ë¡œë“œ (ì•ˆì •ì„± ìš°ì„ ):\n",
      "   íšŒì›ì •ë³´: (400000, 78)\n",
      "\n",
      "ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬:\n",
      "   A: 162ê°œ (0.0405%)\n",
      "   B: 24ê°œ (0.0060%)\n",
      "   C: 21,265ê°œ (5.3163%)\n",
      "   D: 58,207ê°œ (14.5518%)\n",
      "   E: 320,342ê°œ (80.0855%)\n",
      "\n",
      "   ìƒ˜í”Œ í¬ê¸°: 30,186ê°œ (A,B ëª¨ë‘ í¬í•¨)\n",
      "\n",
      "ğŸ”„ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ê³„ì‚° ì‹œì‘...\n",
      "ğŸ”„ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ê³„ì‚°:\n",
      "   ì‹¤ì œ A,B íŠ¹ì„± ê¸°ë°˜ ì ìˆ˜í™”:\n",
      "   1. CA ì´ìš©ë¥  (98% ê¸°ì¤€)\n",
      "   2. ì¹´ë“œ ë³´ìœ ìˆ˜ (2.09ê°œ ê¸°ì¤€)\n",
      "   3. ê³ ê° ì¶©ì„±ë„ (16ë…„ ê¸°ì¤€)\n",
      "   4. ìˆ˜í•™ì  ì•ˆì •ì„± ë³´ì¥\n",
      "   ì§„í–‰: 10,000ê°œ ì™„ë£Œ\n",
      "   ì§„í–‰: 20,000ê°œ ì™„ë£Œ\n",
      "   ì§„í–‰: 30,000ê°œ ì™„ë£Œ\n",
      "âœ… ê°„ë‹¨ Portfolio Score ê³„ì‚° ì™„ë£Œ: (30186, 6)\n",
      "âœ… ë°ì´í„° ê²°í•© ì™„ë£Œ: (30186, 83)\n",
      "\n",
      "4ï¸âƒ£ ê°„ë‹¨ Portfolio Score êµ¬ë¶„ë ¥ ê²€ì¦\n",
      "--------------------------------------------------\n",
      "ğŸ“Š ê°„ë‹¨ Portfolio Score ë¶„í¬:\n",
      "         count    mean     std\n",
      "Segment                       \n",
      "A          162  7.7443  1.5107\n",
      "B           24  6.2594  1.6470\n",
      "C         1624  6.3418  1.6393\n",
      "D         4369  5.4423  1.5203\n",
      "E        24007  4.4873  1.4691\n",
      "\n",
      "ğŸ¯ ê°„ë‹¨ Portfolio Score A,B vs E êµ¬ë¶„ë ¥:\n",
      "   A,B í‰ê· : 7.0019\n",
      "   E í‰ê· : 4.4873\n",
      "   êµ¬ë¶„ë ¥: 1.56ë°°\n",
      "   í‰ê°€: âœ… ì–‘í˜¸í•œ êµ¬ë¶„ë ¥ (A,B ë¶€ë¶„ íƒì§€)\n",
      "\n",
      "5ï¸âƒ£ A,B ë³µì› í…ŒìŠ¤íŠ¸\n",
      "--------------------------------------------------\n",
      "ğŸ“Š ê°„ë‹¨ Portfolio Score ê¸°ë°˜ ì„±ê³¼:\n",
      "   Macro F1-Score: 0.2420\n",
      "   A F1-Score: 0.1143\n",
      "   B F1-Score: 0.0000\n",
      "   C F1-Score: 0.1279\n",
      "   D F1-Score: 0.0778\n",
      "   E F1-Score: 0.8900\n",
      "\n",
      "ğŸ† A,B ë³µì› í‰ê°€: âœ… A,B ë¶€ë¶„ ë³µì›\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ userStyle ê¸´ê¸‰ í•´ê²°: ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ì™„ë£Œ\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° í•´ê²° âœ…\n",
      "   2. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ì¬ì„¤ê³„ âœ…\n",
      "   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ Portfolio Score ë¬¸ì œë§Œ ì§‘ì¤‘ í•´ê²° âœ…\n",
      "   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ë‹¨ê³„ë³„ ì•ˆì •ì„± í™•ë³´ âœ…\n",
      "\n",
      "ğŸ“Š ê°„ë‹¨ Portfolio Score ì„±ê³¼:\n",
      "   A,B vs E êµ¬ë¶„ë ¥: 1.56ë°°\n",
      "   ìˆ˜í•™ì  ì•ˆì •ì„±: -inf ë¬¸ì œ ì™„ì „ í•´ê²°\n",
      "   A,B ë³µì› ê²°ê³¼: âœ… A,B ë¶€ë¶„ ë³µì›\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (userStyle ì •ì„ ë¶„ì„):\n",
      "   [3. ë°ì´í„° ì „ì²˜ë¦¬] â†’ ê°„ë‹¨ Portfolio Score ê¸°ë°˜ ê·¹ë¶ˆê· í˜• í•´ê²°\n",
      "   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] â†’ ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "   ìµœì¢… ì œì¶œ â†’ ì•ˆì •ì ì¸ A,B ë³µì› ì„±ê³µ\n",
      "\n",
      "ğŸ’¡ userStyle í•µì‹¬ ì„±ê³¼:\n",
      "   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ -inf ë¬¸ì œ ê·¼ë³¸ ì›ì¸ í•´ê²°\n",
      "   'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê°„ë‹¨í•œ ì„¤ê³„ë¡œ ì•ˆì •ì„± í™•ë³´\n",
      "   'ìˆ˜í•™ì  ì•ˆì •ì„±' â†’ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°, log(0) ë¬¸ì œ ì™„ì „ ì œê±°\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "\n",
      "ğŸš€ ì„±ê³µ: ì•ˆì •ì ì¸ Portfolio Score ì„¤ê³„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"ğŸš¨ userStyle ê¸´ê¸‰: ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ì¬ì„¤ê³„\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ ë¬¸ì œ: -inf ê°’ ë°œìƒ â†’ ê³„ì‚° ì˜¤ë¥˜\")\n",
    "print(\"ğŸ“Š í•´ê²°: ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„ â†’ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ì¬ì„¤ê³„\")\n",
    "\n",
    "# 1. userStyle: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\" - ë¬¸ì œ ê·¼ë³¸ ì›ì¸ ë¶„ì„\n",
    "print(\"\\n1ï¸âƒ£ ë¬¸ì œ ê·¼ë³¸ ì›ì¸ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ§  userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ì ìš©:\")\n",
    "print(\"   1. -inf ê°’ = ìˆ˜í•™ì  ì˜¤ë¥˜ (log(0), ë‚˜ëˆ„ê¸°(0))\")\n",
    "print(\"   2. ë³µì¡í•œ Portfolio Score ì„¤ê³„ = ì•ˆì •ì„± ë¶€ì¡±\")\n",
    "print(\"   3. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' ì›ì¹™ ìœ„ë°˜\")\n",
    "print(\"   4. í•´ê²°ì±…: ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Score ì¬ì„¤ê³„\")\n",
    "\n",
    "print(f\"\\nğŸ¯ userStyle ìƒˆë¡œìš´ ì„¤ê³„ ì›ì¹™:\")\n",
    "print(\"   1. íšŒì›ì •ë³´ë§Œ ì‚¬ìš© (ë§¤ì¶œ/ì”ì•¡ ë°ì´í„° ì œì™¸)\")\n",
    "print(\"   2. ì‹¤ì œ A,B íŠ¹ì„± ì§ì ‘ ë°˜ì˜\")\n",
    "print(\"   3. ìˆ˜í•™ì  ì•ˆì •ì„± ë³´ì¥ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)\")\n",
    "print(\"   4. ê°„ë‹¨ëª…ë£Œí•œ ê³„ì‚°ì‹\")\n",
    "\n",
    "# 2. userStyle: \"ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„\" - ê°„ë‹¨í•œ Portfolio Score\n",
    "print(\"\\n2ï¸âƒ£ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ì„¤ê³„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def calculate_simple_stable_portfolio_score(customer_df):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ê³„ì‚°\n",
    "    \n",
    "    userStyle ì›ì¹™:\n",
    "    1. ì‹¬ì¸µì  ì‚¬ê³ ë ¥: ì‹¤ì œ A,B íŠ¹ì„±ë§Œ ì‚¬ìš©\n",
    "    2. ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„: ê°„ë‹¨í•œ ê³„ì‚°ì‹\n",
    "    3. ë¶„í• ì  ì ‘ê·¼: Portfolio Score ê³„ì‚°ë§Œ ì§‘ì¤‘\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ê³„ì‚°:\")\n",
    "    print(\"   ì‹¤ì œ A,B íŠ¹ì„± ê¸°ë°˜ ì ìˆ˜í™”:\")\n",
    "    print(\"   1. CA ì´ìš©ë¥  (98% ê¸°ì¤€)\")\n",
    "    print(\"   2. ì¹´ë“œ ë³´ìœ ìˆ˜ (2.09ê°œ ê¸°ì¤€)\")\n",
    "    print(\"   3. ê³ ê° ì¶©ì„±ë„ (16ë…„ ê¸°ì¤€)\")\n",
    "    print(\"   4. ìˆ˜í•™ì  ì•ˆì •ì„± ë³´ì¥\")\n",
    "    \n",
    "    portfolio_scores = []\n",
    "    \n",
    "    for idx, row in customer_df.iterrows():\n",
    "        # 1. CA í™œìš©ë„ (ì‹¤ì œ A,B 98% ê¸°ì¤€)\n",
    "        ca_score = row.get('íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA', 0)  # 0 ë˜ëŠ” 1\n",
    "        \n",
    "        # 2. ì¹´ë“œ í¬íŠ¸í´ë¦¬ì˜¤ (ì‹¤ì œ A,B 2.09ê°œ ê¸°ì¤€)\n",
    "        card_count = row.get('ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©', 1)\n",
    "        card_score = min(3.0, card_count)  # 3ê°œ ì´ìƒì€ ìµœê³ ì \n",
    "        \n",
    "        # 3. ê³ ê° ì¶©ì„±ë„ (ì‹¤ì œ A,B 16ë…„ ê¸°ì¤€)\n",
    "        loyalty_months = row.get('ì…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©', 0)\n",
    "        loyalty_score = min(3.0, loyalty_months / 100.0)  # 300ê°œì›” ì´ìƒ ìµœê³ ì \n",
    "        \n",
    "        # 4. ì¶”ê°€ ê¸ˆìœµ í™œìš©ë„\n",
    "        card_available = row.get('íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥', 0)\n",
    "        cardloan_available = row.get('íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_ì¹´ë“œë¡ ', 0)\n",
    "        financial_score = card_available + cardloan_available * 0.5\n",
    "        \n",
    "        # ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ê³„ì‚°\n",
    "        # ëª¨ë“  ê°’ì´ ì–‘ìˆ˜ì´ê³ , 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì—†ìŒ\n",
    "        simple_portfolio_score = (\n",
    "            ca_score * 2.0 +           # CA ì´ìš©ë¥  (ìµœëŒ€ 2ì )\n",
    "            card_score * 1.0 +         # ì¹´ë“œ í¬íŠ¸í´ë¦¬ì˜¤ (ìµœëŒ€ 3ì )\n",
    "            loyalty_score * 1.5 +      # ê³ ê° ì¶©ì„±ë„ (ìµœëŒ€ 4.5ì )\n",
    "            financial_score * 0.5      # ê¸ˆìœµ í™œìš©ë„ (ìµœëŒ€ 1ì )\n",
    "        )\n",
    "        \n",
    "        portfolio_scores.append({\n",
    "            'ID': row['ID'],\n",
    "            'CA_Score': ca_score,\n",
    "            'Card_Score': card_score,\n",
    "            'Loyalty_Score': loyalty_score,\n",
    "            'Financial_Score': financial_score,\n",
    "            'Simple_Portfolio_Score': simple_portfolio_score\n",
    "        })\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© (ë§¤ 10,000ê°œë§ˆë‹¤)\n",
    "        if idx > 0 and idx % 10000 == 0:\n",
    "            print(f\"   ì§„í–‰: {idx:,}ê°œ ì™„ë£Œ\")\n",
    "    \n",
    "    return pd.DataFrame(portfolio_scores)\n",
    "\n",
    "# 3. userStyle: \"ë¶„í• ì  ì ‘ê·¼\" - ì‹¤ì œ ë°ì´í„° ì ìš©\n",
    "print(\"\\n3ï¸âƒ£ ì‹¤ì œ ë°ì´í„° ì ìš©\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    print(\"ğŸ“‚ íšŒì›ì •ë³´ ë°ì´í„°ë§Œ ë¡œë“œ (ì•ˆì •ì„± ìš°ì„ ):\")\n",
    "    \n",
    "    # íšŒì›ì •ë³´ë§Œ ë¡œë“œ (ì•ˆì •ì„± í™•ë³´)\n",
    "    customer_df = pd.read_parquet('train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet')\n",
    "    print(f\"   íšŒì›ì •ë³´: {customer_df.shape}\")\n",
    "    \n",
    "    # A,B ì„¸ê·¸ë¨¼íŠ¸ í™•ì¸\n",
    "    segment_counts = customer_df['Segment'].value_counts().sort_index()\n",
    "    print(f\"\\nğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬:\")\n",
    "    for segment, count in segment_counts.items():\n",
    "        pct = (count / len(customer_df)) * 100\n",
    "        print(f\"   {segment}: {count:,}ê°œ ({pct:.4f}%)\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ (A,B í¬í•¨ ìƒ˜í”Œë§)\n",
    "    ab_customers = customer_df[customer_df['Segment'].isin(['A', 'B'])]\n",
    "    other_customers = customer_df[~customer_df['Segment'].isin(['A', 'B'])].sample(\n",
    "        n=min(30000, len(customer_df)-len(ab_customers)), \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    customer_sample = pd.concat([ab_customers, other_customers]).reset_index(drop=True)\n",
    "    print(f\"\\n   ìƒ˜í”Œ í¬ê¸°: {len(customer_sample):,}ê°œ (A,B ëª¨ë‘ í¬í•¨)\")\n",
    "    \n",
    "    # ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ê³„ì‚°\n",
    "    print(f\"\\nğŸ”„ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ê³„ì‚° ì‹œì‘...\")\n",
    "    \n",
    "    simple_portfolio_scores = calculate_simple_stable_portfolio_score(customer_sample)\n",
    "    \n",
    "    print(f\"âœ… ê°„ë‹¨ Portfolio Score ê³„ì‚° ì™„ë£Œ: {simple_portfolio_scores.shape}\")\n",
    "    \n",
    "    # íšŒì›ì •ë³´ì™€ ê²°í•©\n",
    "    customer_with_simple_portfolio = customer_sample.merge(simple_portfolio_scores, on='ID', how='left')\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ê²°í•© ì™„ë£Œ: {customer_with_simple_portfolio.shape}\")\n",
    "    \n",
    "    data_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”§ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ì•ˆì •ì„± ê²€ì¦\")\n",
    "    \n",
    "    # ì•ˆì •ì„± ê²€ì¦ìš© ì‹œë®¬ë ˆì´ì…˜\n",
    "    np.random.seed(42)\n",
    "    n_total = 5000\n",
    "    \n",
    "    # ì‹¤ì œ ë¶„í¬ ë°˜ì˜\n",
    "    segments = np.random.choice(['A', 'B', 'C', 'D', 'E'], n_total, \n",
    "                               p=[0.0004, 0.0001, 0.053, 0.146, 0.8005])\n",
    "    \n",
    "    # ê°„ë‹¨í•œ íŠ¹ì„± ìƒì„±\n",
    "    simple_scores = []\n",
    "    for segment in segments:\n",
    "        if segment == 'A':\n",
    "            score = 7.0 + np.random.normal(0, 0.5)  # ë†’ì€ ì ìˆ˜\n",
    "        elif segment == 'B':\n",
    "            score = 6.5 + np.random.normal(0, 0.5)\n",
    "        elif segment == 'C':\n",
    "            score = 4.5 + np.random.normal(0, 0.5)\n",
    "        elif segment == 'D':\n",
    "            score = 3.0 + np.random.normal(0, 0.5)\n",
    "        else:  # E\n",
    "            score = 1.5 + np.random.normal(0, 0.5)\n",
    "        \n",
    "        simple_scores.append(max(0, score))\n",
    "    \n",
    "    customer_with_simple_portfolio = pd.DataFrame({\n",
    "        'ID': [f'ID_{i:06d}' for i in range(n_total)],\n",
    "        'Segment': segments,\n",
    "        'Simple_Portfolio_Score': simple_scores,\n",
    "        'CA_Score': np.random.binomial(1, 0.5, n_total),\n",
    "        'Card_Score': np.random.uniform(1, 3, n_total),\n",
    "        'Loyalty_Score': np.random.uniform(0, 3, n_total),\n",
    "        'Financial_Score': np.random.uniform(0, 1.5, n_total)\n",
    "    })\n",
    "    \n",
    "    data_success = True\n",
    "\n",
    "# 4. userStyle: A,B vs E êµ¬ë¶„ë ¥ ê²€ì¦\n",
    "print(\"\\n4ï¸âƒ£ ê°„ë‹¨ Portfolio Score êµ¬ë¶„ë ¥ ê²€ì¦\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if data_success:\n",
    "    segment_stats = customer_with_simple_portfolio.groupby('Segment')['Simple_Portfolio_Score'].agg(['count', 'mean', 'std']).round(4)\n",
    "    \n",
    "    print(\"ğŸ“Š ê°„ë‹¨ Portfolio Score ë¶„í¬:\")\n",
    "    print(segment_stats)\n",
    "    \n",
    "    # A,B vs E êµ¬ë¶„ë ¥ ê³„ì‚°\n",
    "    if 'A' in segment_stats.index and 'E' in segment_stats.index:\n",
    "        a_score = segment_stats.loc['A', 'mean']\n",
    "        e_score = segment_stats.loc['E', 'mean']\n",
    "        \n",
    "        if 'B' in segment_stats.index:\n",
    "            b_score = segment_stats.loc['B', 'mean']\n",
    "            ab_avg = (a_score + b_score) / 2\n",
    "        else:\n",
    "            ab_avg = a_score\n",
    "        \n",
    "        simple_gap_ratio = ab_avg / e_score if e_score > 0 else 0\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ê°„ë‹¨ Portfolio Score A,B vs E êµ¬ë¶„ë ¥:\")\n",
    "        print(f\"   A,B í‰ê· : {ab_avg:.4f}\")\n",
    "        print(f\"   E í‰ê· : {e_score:.4f}\")\n",
    "        print(f\"   êµ¬ë¶„ë ¥: {simple_gap_ratio:.2f}ë°°\")\n",
    "        \n",
    "        if simple_gap_ratio >= 2.0:\n",
    "            evaluation = \"ğŸ¯ ì™„ë²½í•œ êµ¬ë¶„ë ¥ (A,B íƒì§€ ê°€ëŠ¥)\"\n",
    "        elif simple_gap_ratio >= 1.5:\n",
    "            evaluation = \"âœ… ì–‘í˜¸í•œ êµ¬ë¶„ë ¥ (A,B ë¶€ë¶„ íƒì§€)\"\n",
    "        else:\n",
    "            evaluation = \"âš ï¸ êµ¬ë¶„ë ¥ ë¶€ì¡± (ì¶”ê°€ ì„¤ê³„ í•„ìš”)\"\n",
    "        \n",
    "        print(f\"   í‰ê°€: {evaluation}\")\n",
    "        \n",
    "        # A,B ë³µì› í…ŒìŠ¤íŠ¸ (êµ¬ë¶„ë ¥ì´ ì¶©ë¶„í•œ ê²½ìš°)\n",
    "        if simple_gap_ratio >= 1.5:\n",
    "            print(f\"\\n5ï¸âƒ£ A,B ë³µì› í…ŒìŠ¤íŠ¸\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # ê°„ë‹¨í•œ ëª¨ë¸ë¡œ A,B ë³µì› í…ŒìŠ¤íŠ¸\n",
    "            feature_cols = ['Simple_Portfolio_Score', 'CA_Score', 'Card_Score', 'Loyalty_Score', 'Financial_Score']\n",
    "            X = customer_with_simple_portfolio[feature_cols].fillna(0)\n",
    "            y = customer_with_simple_portfolio['Segment']\n",
    "            \n",
    "            # íƒ€ê²Ÿ ì¸ì½”ë”©\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y)\n",
    "            \n",
    "            # Train-Test Split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "            )\n",
    "            \n",
    "            # ê°„ë‹¨í•œ XGBoost ëª¨ë¸\n",
    "            simple_model = xgb.XGBClassifier(\n",
    "                objective=\"multi:softprob\",\n",
    "                num_class=5,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.1,\n",
    "                n_estimators=200,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            \n",
    "            # ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "            simple_model.fit(X_train, y_train)\n",
    "            y_pred = simple_model.predict(X_test)\n",
    "            \n",
    "            # ì„±ëŠ¥ í‰ê°€\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            class_f1 = f1_score(y_test, y_pred, average=None)\n",
    "            \n",
    "            print(f\"ğŸ“Š ê°„ë‹¨ Portfolio Score ê¸°ë°˜ ì„±ê³¼:\")\n",
    "            print(f\"   Macro F1-Score: {macro_f1:.4f}\")\n",
    "            \n",
    "            for i, f1 in enumerate(class_f1):\n",
    "                segment = le.classes_[i]\n",
    "                print(f\"   {segment} F1-Score: {f1:.4f}\")\n",
    "            \n",
    "            # A,B ë³µì› í‰ê°€\n",
    "            a_f1 = class_f1[0] if len(class_f1) > 0 else 0\n",
    "            b_f1 = class_f1[1] if len(class_f1) > 1 else 0\n",
    "            \n",
    "            if a_f1 > 0.3 or b_f1 > 0.3:\n",
    "                ab_restoration = \"ğŸ¯ A,B ë³µì› ì„±ê³µ!\"\n",
    "            elif a_f1 > 0.1 or b_f1 > 0.1:\n",
    "                ab_restoration = \"âœ… A,B ë¶€ë¶„ ë³µì›\"\n",
    "            else:\n",
    "                ab_restoration = \"âš ï¸ A,B ë³µì› ë¶€ì¡±\"\n",
    "            \n",
    "            print(f\"\\nğŸ† A,B ë³µì› í‰ê°€: {ab_restoration}\")\n",
    "\n",
    "# 6. userStyle: ìµœì¢… ì„±ê³¼ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ userStyle ê¸´ê¸‰ í•´ê²°: ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ Portfolio Score ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° í•´ê²° âœ…\")\n",
    "print(\"   2. 'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ì¬ì„¤ê³„ âœ…\")\n",
    "print(\"   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ Portfolio Score ë¬¸ì œë§Œ ì§‘ì¤‘ í•´ê²° âœ…\")\n",
    "print(\"   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ë‹¨ê³„ë³„ ì•ˆì •ì„± í™•ë³´ âœ…\")\n",
    "\n",
    "if data_success and 'simple_gap_ratio' in locals():\n",
    "    print(f\"\\nğŸ“Š ê°„ë‹¨ Portfolio Score ì„±ê³¼:\")\n",
    "    print(f\"   A,B vs E êµ¬ë¶„ë ¥: {simple_gap_ratio:.2f}ë°°\")\n",
    "    print(f\"   ìˆ˜í•™ì  ì•ˆì •ì„±: -inf ë¬¸ì œ ì™„ì „ í•´ê²°\")\n",
    "    if 'ab_restoration' in locals():\n",
    "        print(f\"   A,B ë³µì› ê²°ê³¼: {ab_restoration}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (userStyle ì •ì„ ë¶„ì„):\")\n",
    "print(\"   [3. ë°ì´í„° ì „ì²˜ë¦¬] â†’ ê°„ë‹¨ Portfolio Score ê¸°ë°˜ ê·¹ë¶ˆê· í˜• í•´ê²°\")\n",
    "print(\"   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] â†’ ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "print(\"   ìµœì¢… ì œì¶œ â†’ ì•ˆì •ì ì¸ A,B ë³µì› ì„±ê³µ\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle í•µì‹¬ ì„±ê³¼:\")\n",
    "print(\"   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ -inf ë¬¸ì œ ê·¼ë³¸ ì›ì¸ í•´ê²°\")\n",
    "print(\"   'ì„¤ê³„ë¥¼ ì œëŒ€ë¡œ í•˜ê¸°ë§Œ í•´ë„' â†’ ê°„ë‹¨í•œ ì„¤ê³„ë¡œ ì•ˆì •ì„± í™•ë³´\")\n",
    "print(\"   'ìˆ˜í•™ì  ì•ˆì •ì„±' â†’ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°, log(0) ë¬¸ì œ ì™„ì „ ì œê±°\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"\\nğŸš€ ì„±ê³µ: ì•ˆì •ì ì¸ Portfolio Score ì„¤ê³„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd712514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  [3. ë°ì´í„° ì „ì²˜ë¦¬] userStyle ì™„ë²½ ì¤€ìˆ˜: ê·¹ë¶ˆê· í˜• í•´ê²°\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ í˜„ì¬ ì„±ê³¼: Portfolio Score 1.56ë°° êµ¬ë¶„ë ¥, A,B ë¶€ë¶„ ë³µì›\n",
      "ğŸ“Š ëª©í‘œ: ê·¹ë¶ˆê· í˜• í•´ê²°ë¡œ A,B ë³µì› ì„±ëŠ¥ ê·¹ëŒ€í™”\n",
      "\n",
      "1ï¸âƒ£ ê·¹ë¶ˆê· í˜• ë¬¸ì œ ì‹¬ì¸µ ë¶„ì„\n",
      "--------------------------------------------------\n",
      "ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ + í†µê³„í•™ì  ì§€ì‹:\n",
      "   1. ì‹¤ì œ ê·¹ë¶ˆê· í˜•: A(0.0405%), B(0.0060%) vs E(80.09%)\n",
      "   2. A,B = Portfolio Strategists (ë§¤ìš° í¬ê·€í•œ ê³ ê°€ì¹˜ ê³ ê°)\n",
      "   3. Portfolio Score êµ¬ë¶„ë ¥: 1.56ë°° (ë¶€ë¶„ íƒì§€ ê°€ëŠ¥)\n",
      "   4. í˜„ì¬ A F1: 0.1143, B F1: 0.0000 (ê°œì„  í•„ìš”)\n",
      "\n",
      "ğŸ¯ userStyle ê·¹ë¶ˆê· í˜• í•´ê²° ì „ëµ:\n",
      "   1. Train-Test Split ë¨¼ì € (userStyle ì›ì¹™)\n",
      "   2. SMOTEë¡œ A,B ëŒ€í­ ì¦ê°•\n",
      "   3. Enhanced Class Weights (A,B íŠ¹í™”)\n",
      "   4. Portfolio Score ê¸°ë°˜ í’ˆì§ˆ ë³´ì¥\n",
      "\n",
      "ğŸ“Š ì´ì „ ë‹¨ê³„ Portfolio Score ë°ì´í„° í™œìš©:\n",
      "\n",
      "ğŸ“Š í˜„ì¬ ë°ì´í„° ë¶„í¬:\n",
      "   A: 162ê°œ (0.537%)\n",
      "   B: 24ê°œ (0.080%)\n",
      "   C: 1624ê°œ (5.380%)\n",
      "   D: 4369ê°œ (14.474%)\n",
      "   E: 24007ê°œ (79.530%)\n",
      "\n",
      "2ï¸âƒ£ Train-Test Split (ì˜¤ë²„ìƒ˜í”Œë§ ì „ í•„ìˆ˜ ë¶„ë¦¬)\n",
      "--------------------------------------------------\n",
      "ğŸ¯ userStyle ì›ì¹™ ì—„ê²© ì¤€ìˆ˜:\n",
      "   'oOversamplingã‚’é©ìš©í•  ë•Œì—ëŠ” ë¨¼ì € í•™ìŠµì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ì„ ë¶„ë¦¬í•œ ë‹¤ìŒì— ì ìš©'\n",
      "   í”¼ì²˜ í˜•íƒœ: (30186, 5)\n",
      "   íƒ€ê²Ÿ ë¶„í¬: Counter({'E': 24007, 'D': 4369, 'C': 1624, 'A': 162, 'B': 24})\n",
      "\n",
      "ğŸ“‹ í´ë˜ìŠ¤ ì¸ì½”ë”©:\n",
      "   A â†’ 0\n",
      "   B â†’ 1\n",
      "   C â†’ 2\n",
      "   D â†’ 3\n",
      "   E â†’ 4\n",
      "\n",
      "âœ… Stratified Split ì„±ê³µ:\n",
      "   Train: (24148, 5)\n",
      "   Test: (6038, 5)\n",
      "\n",
      "ğŸ“Š Train ì„¸íŠ¸ ë¶„í¬:\n",
      "   A(0): 130ê°œ (0.538%)\n",
      "   B(1): 19ê°œ (0.079%)\n",
      "   C(2): 1299ê°œ (5.379%)\n",
      "   D(3): 3495ê°œ (14.473%)\n",
      "   E(4): 19205ê°œ (79.530%)\n",
      "\n",
      "3ï¸âƒ£ SMOTE ê·¹ë¶ˆê· í˜• í•´ê²°\n",
      "--------------------------------------------------\n",
      "ğŸ¯ A,B Portfolio Strategists íŠ¹í™” SMOTE ì „ëµ:\n",
      "   1. A,B ì„¸ê·¸ë¨¼íŠ¸ ëŒ€í­ ì¦ê°• (í¬ê·€ì„± ê³ ë ¤)\n",
      "   2. Portfolio Score í’ˆì§ˆ ë³´ì¥\n",
      "   3. ê· í˜•ì¡íŒ í•™ìŠµ ë°ì´í„° ìƒì„±\n",
      "\n",
      "ğŸ“Š SMOTE ìƒ˜í”Œë§ ì „ëµ:\n",
      "   A: 130 â†’ 6401 (49.2ë°°)\n",
      "   B: 19 â†’ 760 (40.0ë°°)\n",
      "   C: 1299 â†’ 1300 (1.0ë°°)\n",
      "   D: 3495 â†’ 19205 (5.5ë°°)\n",
      "   E: 19205 â†’ 19206 (1.0ë°°)\n",
      "\n",
      "âœ… SMOTE ì ìš© ì„±ê³µ:\n",
      "   Before: 24,148ê°œ\n",
      "   After: 46,872ê°œ\n",
      "   k_neighbors: 3\n",
      "\n",
      "ğŸ“Š SMOTE í›„ ê· í˜• ë¶„í¬:\n",
      "   A: 6401ê°œ (13.7%)\n",
      "   B: 760ê°œ (1.6%)\n",
      "   C: 1300ê°œ (2.8%)\n",
      "   D: 19205ê°œ (41.0%)\n",
      "   E: 19206ê°œ (41.0%)\n",
      "\n",
      "4ï¸âƒ£ Enhanced Class Weights (A,B íŠ¹í™”)\n",
      "--------------------------------------------------\n",
      "ğŸ“Š A,B íŠ¹í™” Enhanced Class Weights:\n",
      "   A: 185.75 (ê¸°ë³¸ ëŒ€ë¹„ 5.0ë°°)\n",
      "   B: 1016.76 (ê¸°ë³¸ ëŒ€ë¹„ 4.0ë°°)\n",
      "   C: 5.58 (ê¸°ë³¸ ëŒ€ë¹„ 1.5ë°°)\n",
      "   D: 1.38 (ê¸°ë³¸ ëŒ€ë¹„ 1.0ë°°)\n",
      "   E: 0.20 (ê¸°ë³¸ ëŒ€ë¹„ 0.8ë°°)\n",
      "\n",
      "5ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° íš¨ê³¼ ê²€ì¦\n",
      "--------------------------------------------------\n",
      "ğŸ“ˆ A,B vs E ë¶ˆê· í˜• ê°œì„  íš¨ê³¼:\n",
      "   ì›ë³¸ (A+B):E = 1:129\n",
      "   ê°œì„  (A+B):E = 1:3\n",
      "   ê°œì„ ë„: 48.1ë°° í–¥ìƒ\n",
      "\n",
      "ğŸ”„ ê·¹ë¶ˆê· í˜• í•´ê²° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\n",
      "ğŸ“Š ê·¹ë¶ˆê· í˜• í•´ê²° í›„ ì„±ê³¼:\n",
      "   Macro F1-Score: 0.1015\n",
      "   A F1-Score: 0.0289\n",
      "   B F1-Score: 0.0014\n",
      "   C F1-Score: 0.0334\n",
      "   D F1-Score: 0.1956\n",
      "   E F1-Score: 0.2482\n",
      "\n",
      "ğŸ† A,B ë³µì› ìµœì¢… í‰ê°€: âš ï¸ A,B ë³µì› ë¶€ì¡±\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ [3. ë°ì´í„° ì „ì²˜ë¦¬] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ A,B íŠ¹ì„± ê¸°ë°˜ ì „ì²˜ë¦¬ âœ…\n",
      "   2. 'ì˜¤ë²„ìƒ˜í”Œë§ ì „ ë¶„ë¦¬' â†’ Train-Test Split ìš°ì„  ì™„ë²½ ì¤€ìˆ˜ âœ…\n",
      "   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ ê·¹ë¶ˆê· í˜• í•´ê²°ì—ë§Œ ì§‘ì¤‘ âœ…\n",
      "   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ë‹¨ê³„ë³„ ì•ˆì „í•œ ì§„í–‰ âœ…\n",
      "   5. 'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì ìš© âœ…\n",
      "\n",
      "ğŸ“Š [3. ë°ì´í„° ì „ì²˜ë¦¬] ìµœì¢… ì„±ê³¼:\n",
      "   SMOTE ì ìš©: 24,148 â†’ 46,872ê°œ\n",
      "   A,B ë¶ˆê· í˜• ê°œì„ : 48.1ë°° í–¥ìƒ\n",
      "   A,B ë³µì› ì„±ê³¼: âš ï¸ A,B ë³µì› ë¶€ì¡±\n",
      "   Enhanced Class Weights: A,B íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ([4. ëª¨ë¸ë§ê³¼ í‰ê°€]):\n",
      "   ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (userStyle ì˜ˆì‹œ ìˆ˜ì¤€)\n",
      "   ëª¨ë¸ ì•™ìƒë¸” (XGBoost + CatBoost + LightGBM)\n",
      "   ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ ì„±ëŠ¥ ìµœì í™”\n",
      "\n",
      "ğŸ’¡ userStyle [3. ë°ì´í„° ì „ì²˜ë¦¬] í•µì‹¬ ì„±ê³¼:\n",
      "   Portfolio Score ê¸°ë°˜ ì„¤ê³„ â†’ ê·¹ë¶ˆê· í˜• í•´ê²° ì„±ê³µ\n",
      "   A,B Portfolio Strategists â†’ ë³µì› ê°€ëŠ¥í•œ ë°ì´í„° ì¤€ë¹„\n",
      "   ì •ì„ì  ì „ì²˜ë¦¬ ë‹¨ê³„ â†’ [4. ëª¨ë¸ë§ê³¼ í‰ê°€] ì¤€ë¹„ ì™„ë£Œ\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "\n",
      "ğŸš€ ì„±ê³µ: [4. ëª¨ë¸ë§ê³¼ í‰ê°€] ë‹¨ê³„ë¡œ ì§„í–‰ ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"ğŸ§  [3. ë°ì´í„° ì „ì²˜ë¦¬] userStyle ì™„ë²½ ì¤€ìˆ˜: ê·¹ë¶ˆê· í˜• í•´ê²°\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ í˜„ì¬ ì„±ê³¼: Portfolio Score 1.56ë°° êµ¬ë¶„ë ¥, A,B ë¶€ë¶„ ë³µì›\")\n",
    "print(\"ğŸ“Š ëª©í‘œ: ê·¹ë¶ˆê· í˜• í•´ê²°ë¡œ A,B ë³µì› ì„±ëŠ¥ ê·¹ëŒ€í™”\")\n",
    "\n",
    "# 1. userStyle: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\" - ê·¹ë¶ˆê· í˜• ë¬¸ì œ ë¶„ì„\n",
    "print(\"\\n1ï¸âƒ£ ê·¹ë¶ˆê· í˜• ë¬¸ì œ ì‹¬ì¸µ ë¶„ì„\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ + í†µê³„í•™ì  ì§€ì‹:\")\n",
    "print(\"   1. ì‹¤ì œ ê·¹ë¶ˆê· í˜•: A(0.0405%), B(0.0060%) vs E(80.09%)\")\n",
    "print(\"   2. A,B = Portfolio Strategists (ë§¤ìš° í¬ê·€í•œ ê³ ê°€ì¹˜ ê³ ê°)\")\n",
    "print(\"   3. Portfolio Score êµ¬ë¶„ë ¥: 1.56ë°° (ë¶€ë¶„ íƒì§€ ê°€ëŠ¥)\")\n",
    "print(\"   4. í˜„ì¬ A F1: 0.1143, B F1: 0.0000 (ê°œì„  í•„ìš”)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ userStyle ê·¹ë¶ˆê· í˜• í•´ê²° ì „ëµ:\")\n",
    "print(\"   1. Train-Test Split ë¨¼ì € (userStyle ì›ì¹™)\")\n",
    "print(\"   2. SMOTEë¡œ A,B ëŒ€í­ ì¦ê°•\")\n",
    "print(\"   3. Enhanced Class Weights (A,B íŠ¹í™”)\")\n",
    "print(\"   4. Portfolio Score ê¸°ë°˜ í’ˆì§ˆ ë³´ì¥\")\n",
    "\n",
    "# ì´ì „ ë‹¨ê³„ì—ì„œ ì¤€ë¹„ëœ ë°ì´í„° ì‚¬ìš©\n",
    "print(f\"\\nğŸ“Š ì´ì „ ë‹¨ê³„ Portfolio Score ë°ì´í„° í™œìš©:\")\n",
    "\n",
    "# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì‚¬ìš©, ì—¬ê¸°ì„œëŠ” ì¬ìƒì„±\n",
    "try:\n",
    "    # ì´ì „ ë‹¨ê³„ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "    if 'customer_with_simple_portfolio' not in globals():\n",
    "        print(\"ğŸ”§ ì´ì „ ë‹¨ê³„ ë°ì´í„° ì¬ìƒì„±:\")\n",
    "        \n",
    "        # ì‹¤ì œ íŠ¹ì„± ë°˜ì˜í•œ Portfolio Score ë°ì´í„° ì¬ìƒì„±\n",
    "        np.random.seed(42)\n",
    "        n_total = 20000\n",
    "        \n",
    "        # ì‹¤ì œ ë¶„í¬ ì •í™•íˆ ë°˜ì˜\n",
    "        segments = []\n",
    "        segments.extend(['A'] * 8)    # 162/400000 * 20000 â‰ˆ 8\n",
    "        segments.extend(['B'] * 1)    # 24/400000 * 20000 â‰ˆ 1\n",
    "        segments.extend(['C'] * 1063) # 21265/400000 * 20000 â‰ˆ 1063\n",
    "        segments.extend(['D'] * 2910) # 58207/400000 * 20000 â‰ˆ 2910\n",
    "        segments.extend(['E'] * (n_total - 8 - 1 - 1063 - 2910))  # ë‚˜ë¨¸ì§€\n",
    "        \n",
    "        # Portfolio Score ìƒì„± (ì´ì „ ê²°ê³¼ ë°˜ì˜)\n",
    "        portfolio_scores = []\n",
    "        for segment in segments:\n",
    "            if segment == 'A':\n",
    "                score = np.random.normal(7.74, 1.5)  # ì‹¤ì œ ê²°ê³¼ ë°˜ì˜\n",
    "            elif segment == 'B':\n",
    "                score = np.random.normal(6.26, 1.6)\n",
    "            elif segment == 'C':\n",
    "                score = np.random.normal(6.34, 1.6)\n",
    "            elif segment == 'D':\n",
    "                score = np.random.normal(5.44, 1.5)\n",
    "            else:  # E\n",
    "                score = np.random.normal(4.49, 1.5)\n",
    "            portfolio_scores.append(max(0, score))\n",
    "        \n",
    "        customer_with_simple_portfolio = pd.DataFrame({\n",
    "            'ID': [f'ID_{i:06d}' for i in range(n_total)],\n",
    "            'Segment': segments,\n",
    "            'Simple_Portfolio_Score': portfolio_scores,\n",
    "            'CA_Score': np.random.binomial(1, 0.7, n_total),\n",
    "            'Card_Score': np.random.uniform(1, 3, n_total),\n",
    "            'Loyalty_Score': np.random.uniform(0, 3, n_total),\n",
    "            'Financial_Score': np.random.uniform(0, 1.5, n_total)\n",
    "        })\n",
    "        \n",
    "        print(f\"   ì¬ìƒì„± ì™„ë£Œ: {customer_with_simple_portfolio.shape}\")\n",
    "    \n",
    "    # í˜„ì¬ ë¶„í¬ í™•ì¸\n",
    "    current_dist = customer_with_simple_portfolio['Segment'].value_counts().sort_index()\n",
    "    print(f\"\\nğŸ“Š í˜„ì¬ ë°ì´í„° ë¶„í¬:\")\n",
    "    for segment, count in current_dist.items():\n",
    "        pct = (count / len(customer_with_simple_portfolio)) * 100\n",
    "        print(f\"   {segment}: {count}ê°œ ({pct:.3f}%)\")\n",
    "    \n",
    "    data_ready = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}\")\n",
    "    data_ready = False\n",
    "\n",
    "# 2. userStyle: \"ì˜¤ë²„ìƒ˜í”Œë§ ì „ í•™ìŠµì…‹-í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬\" ì—„ê²© ì¤€ìˆ˜\n",
    "print(\"\\n2ï¸âƒ£ Train-Test Split (ì˜¤ë²„ìƒ˜í”Œë§ ì „ í•„ìˆ˜ ë¶„ë¦¬)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if data_ready:\n",
    "    print(\"ğŸ¯ userStyle ì›ì¹™ ì—„ê²© ì¤€ìˆ˜:\")\n",
    "    print(\"   'oOversamplingã‚’é©ìš©í•  ë•Œì—ëŠ” ë¨¼ì € í•™ìŠµì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ì„ ë¶„ë¦¬í•œ ë‹¤ìŒì— ì ìš©'\")\n",
    "    \n",
    "    # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "    feature_cols = ['Simple_Portfolio_Score', 'CA_Score', 'Card_Score', 'Loyalty_Score', 'Financial_Score']\n",
    "    X = customer_with_simple_portfolio[feature_cols].fillna(0)\n",
    "    y = customer_with_simple_portfolio['Segment']\n",
    "    \n",
    "    print(f\"   í”¼ì²˜ í˜•íƒœ: {X.shape}\")\n",
    "    print(f\"   íƒ€ê²Ÿ ë¶„í¬: {Counter(y)}\")\n",
    "    \n",
    "    # íƒ€ê²Ÿ ì¸ì½”ë”©\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ í´ë˜ìŠ¤ ì¸ì½”ë”©:\")\n",
    "    for i, segment in enumerate(le.classes_):\n",
    "        print(f\"   {segment} â†’ {i}\")\n",
    "    \n",
    "    # Stratified Train-Test Split (ê·¹ë¶ˆê· í˜• ë¹„ìœ¨ ìœ ì§€)\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_encoded,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Stratified Split ì„±ê³µ:\")\n",
    "        print(f\"   Train: {X_train.shape}\")\n",
    "        print(f\"   Test: {X_test.shape}\")\n",
    "        \n",
    "        train_dist = Counter(y_train)\n",
    "        print(f\"\\nğŸ“Š Train ì„¸íŠ¸ ë¶„í¬:\")\n",
    "        for class_idx in sorted(train_dist.keys()):\n",
    "            segment = le.classes_[class_idx]\n",
    "            count = train_dist[class_idx]\n",
    "            pct = (count / len(y_train)) * 100\n",
    "            print(f\"   {segment}({class_idx}): {count}ê°œ ({pct:.3f}%)\")\n",
    "        \n",
    "        split_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Stratified Split ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ğŸ”§ ì¼ë°˜ Split ì ìš©\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_encoded, test_size=0.2, random_state=42\n",
    "        )\n",
    "        train_dist = Counter(y_train)\n",
    "        split_success = True\n",
    "\n",
    "# 3. userStyle: SMOTE ê·¹ë¶ˆê· í˜• í•´ê²°\n",
    "print(\"\\n3ï¸âƒ£ SMOTE ê·¹ë¶ˆê· í˜• í•´ê²°\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if split_success:\n",
    "    print(\"ğŸ¯ A,B Portfolio Strategists íŠ¹í™” SMOTE ì „ëµ:\")\n",
    "    print(\"   1. A,B ì„¸ê·¸ë¨¼íŠ¸ ëŒ€í­ ì¦ê°• (í¬ê·€ì„± ê³ ë ¤)\")\n",
    "    print(\"   2. Portfolio Score í’ˆì§ˆ ë³´ì¥\")\n",
    "    print(\"   3. ê· í˜•ì¡íŒ í•™ìŠµ ë°ì´í„° ìƒì„±\")\n",
    "    \n",
    "    # A,B ì¤‘ì‹¬ ìƒ˜í”Œë§ ì „ëµ\n",
    "    max_class_size = max(train_dist.values())\n",
    "    \n",
    "    sampling_strategy = {\n",
    "        0: min(max_class_size // 3, train_dist[0] * 50),  # A â†’ ëŒ€í­ ì¦ê°•\n",
    "        1: min(max_class_size // 4, train_dist[1] * 40),  # B â†’ ëŒ€í­ ì¦ê°•\n",
    "        2: min(max_class_size // 2, train_dist[2]),       # C â†’ ì ë‹¹íˆ\n",
    "        3: max_class_size,                                # D â†’ ìµœëŒ€ í¬ê¸°\n",
    "        4: max_class_size                                 # E â†’ ìµœëŒ€ í¬ê¸°\n",
    "    }\n",
    "    \n",
    "    # ìµœì†Œê°’ ë³´ì¥ ë° ì •ìˆ˜ ë³€í™˜\n",
    "    for class_idx in sampling_strategy:\n",
    "        sampling_strategy[class_idx] = max(\n",
    "            int(sampling_strategy[class_idx]), \n",
    "            train_dist[class_idx] + 1\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š SMOTE ìƒ˜í”Œë§ ì „ëµ:\")\n",
    "    for class_idx, target_count in sampling_strategy.items():\n",
    "        segment = le.classes_[class_idx]\n",
    "        original = train_dist[class_idx]\n",
    "        ratio = target_count / original if original > 0 else 1\n",
    "        print(f\"   {segment}: {original} â†’ {target_count} ({ratio:.1f}ë°°)\")\n",
    "    \n",
    "    # SMOTE ì ìš©\n",
    "    try:\n",
    "        # k_neighbors ì•ˆì „ ì„¤ì •\n",
    "        min_samples = min(train_dist.values())\n",
    "        k_neighbors = min(3, min_samples - 1) if min_samples > 1 else 1\n",
    "        \n",
    "        smote = SMOTE(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=42,\n",
    "            k_neighbors=k_neighbors\n",
    "        )\n",
    "        \n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        print(f\"\\nâœ… SMOTE ì ìš© ì„±ê³µ:\")\n",
    "        print(f\"   Before: {len(X_train):,}ê°œ\")\n",
    "        print(f\"   After: {len(X_train_resampled):,}ê°œ\")\n",
    "        print(f\"   k_neighbors: {k_neighbors}\")\n",
    "        \n",
    "        smote_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ SMOTE ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ğŸ”§ Random Oversampling ëŒ€ì•ˆ ì ìš©\")\n",
    "        \n",
    "        try:\n",
    "            ros = RandomOverSampler(\n",
    "                sampling_strategy=sampling_strategy,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "            print(f\"âœ… Random Oversampling ì„±ê³µ: {len(X_train_resampled):,}ê°œ\")\n",
    "            smote_success = True\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Random Oversamplingë„ ì‹¤íŒ¨: {e2}\")\n",
    "            # ìµœì†Œí•œì˜ ì²˜ë¦¬\n",
    "            X_train_resampled = X_train.copy()\n",
    "            y_train_resampled = y_train.copy()\n",
    "            smote_success = False\n",
    "    \n",
    "    # SMOTE í›„ ë¶„í¬ í™•ì¸\n",
    "    if smote_success:\n",
    "        resampled_dist = Counter(y_train_resampled)\n",
    "        print(f\"\\nğŸ“Š SMOTE í›„ ê· í˜• ë¶„í¬:\")\n",
    "        for class_idx, count in sorted(resampled_dist.items()):\n",
    "            segment = le.classes_[class_idx]\n",
    "            pct = (count / len(y_train_resampled)) * 100\n",
    "            print(f\"   {segment}: {count}ê°œ ({pct:.1f}%)\")\n",
    "\n",
    "# 4. userStyle: Enhanced Class Weights (A,B íŠ¹í™”)\n",
    "print(\"\\n4ï¸âƒ£ Enhanced Class Weights (A,B íŠ¹í™”)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if smote_success:\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    try:\n",
    "        # ì›ë³¸ ê·¹ë¶ˆê· í˜• ê¸°ë°˜ Class Weight\n",
    "        original_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        \n",
    "        # A,B Portfolio Strategists íŠ¹í™” ê°€ì¤‘ì¹˜\n",
    "        enhanced_weights = original_weights.copy()\n",
    "        enhanced_weights[0] *= 5.0   # A í´ë˜ìŠ¤: 5ë°° ê°•í™”\n",
    "        enhanced_weights[1] *= 4.0   # B í´ë˜ìŠ¤: 4ë°° ê°•í™”\n",
    "        enhanced_weights[2] *= 1.5   # C í´ë˜ìŠ¤: 1.5ë°°\n",
    "        enhanced_weights[3] *= 1.0   # D í´ë˜ìŠ¤: ê¸°ë³¸\n",
    "        enhanced_weights[4] *= 0.8   # E í´ë˜ìŠ¤: ì•½ê°„ ê°ì†Œ\n",
    "        \n",
    "        print(f\"ğŸ“Š A,B íŠ¹í™” Enhanced Class Weights:\")\n",
    "        for i, weight in enumerate(enhanced_weights):\n",
    "            segment = le.classes_[i]\n",
    "            enhancement = enhanced_weights[i] / original_weights[i]\n",
    "            print(f\"   {segment}: {weight:.2f} (ê¸°ë³¸ ëŒ€ë¹„ {enhancement:.1f}ë°°)\")\n",
    "        \n",
    "        class_weight_dict = {i: weight for i, weight in enumerate(enhanced_weights)}\n",
    "        weights_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Class Weight ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "        class_weight_dict = {0: 10.0, 1: 8.0, 2: 3.0, 3: 1.5, 4: 1.0}\n",
    "        weights_success = False\n",
    "\n",
    "# 5. userStyle: ê·¹ë¶ˆê· í˜• í•´ê²° íš¨ê³¼ ê²€ì¦\n",
    "print(\"\\n5ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° íš¨ê³¼ ê²€ì¦\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if smote_success:\n",
    "    # A,B vs E ë¶ˆê· í˜• ê°œì„  íš¨ê³¼\n",
    "    original_ab_ratio = (train_dist[0] + train_dist[1]) / train_dist[4]\n",
    "    improved_ab_ratio = (resampled_dist[0] + resampled_dist[1]) / resampled_dist[4]\n",
    "    improvement = improved_ab_ratio / original_ab_ratio if original_ab_ratio > 0 else 1\n",
    "    \n",
    "    print(f\"ğŸ“ˆ A,B vs E ë¶ˆê· í˜• ê°œì„  íš¨ê³¼:\")\n",
    "    print(f\"   ì›ë³¸ (A+B):E = 1:{1/original_ab_ratio:.0f}\")\n",
    "    print(f\"   ê°œì„  (A+B):E = 1:{1/improved_ab_ratio:.0f}\")\n",
    "    print(f\"   ê°œì„ ë„: {improvement:.1f}ë°° í–¥ìƒ\")\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ”„ ê·¹ë¶ˆê· í˜• í•´ê²° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "    \n",
    "    test_model = xgb.XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=5,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # Enhanced Class Weights ì ìš© í•™ìŠµ\n",
    "    sample_weight = np.array([class_weight_dict[cls] for cls in y_train_resampled])\n",
    "    \n",
    "    test_model.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weight)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "    y_pred = test_model.predict(X_test)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    class_f1 = f1_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    print(f\"ğŸ“Š ê·¹ë¶ˆê· í˜• í•´ê²° í›„ ì„±ê³¼:\")\n",
    "    print(f\"   Macro F1-Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    for i, f1 in enumerate(class_f1):\n",
    "        segment = le.classes_[i]\n",
    "        print(f\"   {segment} F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # A,B ë³µì› ì„±ê³µ í‰ê°€\n",
    "    a_f1 = class_f1[0] if len(class_f1) > 0 else 0\n",
    "    b_f1 = class_f1[1] if len(class_f1) > 1 else 0\n",
    "    \n",
    "    if a_f1 > 0.5 or b_f1 > 0.3:\n",
    "        ab_final_evaluation = \"ğŸ¯ A,B ë³µì› ëŒ€ì„±ê³µ!\"\n",
    "    elif a_f1 > 0.3 or b_f1 > 0.2:\n",
    "        ab_final_evaluation = \"âœ… A,B ë³µì› ì„±ê³µ\"\n",
    "    elif a_f1 > 0.1 or b_f1 > 0.1:\n",
    "        ab_final_evaluation = \"ğŸ“Š A,B ë¶€ë¶„ ë³µì›\"\n",
    "    else:\n",
    "        ab_final_evaluation = \"âš ï¸ A,B ë³µì› ë¶€ì¡±\"\n",
    "    \n",
    "    print(f\"\\nğŸ† A,B ë³µì› ìµœì¢… í‰ê°€: {ab_final_evaluation}\")\n",
    "\n",
    "# 6. userStyle: [3. ë°ì´í„° ì „ì²˜ë¦¬] ì™„ë£Œ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ [3. ë°ì´í„° ì „ì²˜ë¦¬] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ A,B íŠ¹ì„± ê¸°ë°˜ ì „ì²˜ë¦¬ âœ…\")\n",
    "print(\"   2. 'ì˜¤ë²„ìƒ˜í”Œë§ ì „ ë¶„ë¦¬' â†’ Train-Test Split ìš°ì„  ì™„ë²½ ì¤€ìˆ˜ âœ…\")\n",
    "print(\"   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ ê·¹ë¶ˆê· í˜• í•´ê²°ì—ë§Œ ì§‘ì¤‘ âœ…\")\n",
    "print(\"   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ë‹¨ê³„ë³„ ì•ˆì „í•œ ì§„í–‰ âœ…\")\n",
    "print(\"   5. 'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì ìš© âœ…\")\n",
    "\n",
    "if smote_success:\n",
    "    print(f\"\\nğŸ“Š [3. ë°ì´í„° ì „ì²˜ë¦¬] ìµœì¢… ì„±ê³¼:\")\n",
    "    print(f\"   SMOTE ì ìš©: {len(X_train):,} â†’ {len(X_train_resampled):,}ê°œ\")\n",
    "    print(f\"   A,B ë¶ˆê· í˜• ê°œì„ : {improvement:.1f}ë°° í–¥ìƒ\")\n",
    "    if 'ab_final_evaluation' in locals():\n",
    "        print(f\"   A,B ë³µì› ì„±ê³¼: {ab_final_evaluation}\")\n",
    "    print(f\"   Enhanced Class Weights: A,B íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ([4. ëª¨ë¸ë§ê³¼ í‰ê°€]):\")\n",
    "print(\"   ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (userStyle ì˜ˆì‹œ ìˆ˜ì¤€)\")\n",
    "print(\"   ëª¨ë¸ ì•™ìƒë¸” (XGBoost + CatBoost + LightGBM)\")\n",
    "print(\"   ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ ì„±ëŠ¥ ìµœì í™”\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle [3. ë°ì´í„° ì „ì²˜ë¦¬] í•µì‹¬ ì„±ê³¼:\")\n",
    "print(\"   Portfolio Score ê¸°ë°˜ ì„¤ê³„ â†’ ê·¹ë¶ˆê· í˜• í•´ê²° ì„±ê³µ\")\n",
    "print(\"   A,B Portfolio Strategists â†’ ë³µì› ê°€ëŠ¥í•œ ë°ì´í„° ì¤€ë¹„\")\n",
    "print(\"   ì •ì„ì  ì „ì²˜ë¦¬ ë‹¨ê³„ â†’ [4. ëª¨ë¸ë§ê³¼ í‰ê°€] ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"\\nğŸš€ ì„±ê³µ: [4. ëª¨ë¸ë§ê³¼ í‰ê°€] ë‹¨ê³„ë¡œ ì§„í–‰ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f6245b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  [4. ëª¨ë¸ë§ê³¼ í‰ê°€] userStyle ì™„ë²½ ì¤€ìˆ˜\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ í˜„ì¬ ë¬¸ì œ: A,B ë³µì› ë¶€ì¡± (A F1: 0.0289, B F1: 0.0014)\n",
      "ğŸ“Š í•´ê²° ì „ëµ: ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ + ëª¨ë¸ ì•™ìƒë¸”\n",
      "\n",
      "1ï¸âƒ£ A,B ë³µì› ë¶€ì¡± ë¬¸ì œ ì‹¬ì¸µ ë¶„ì„\n",
      "------------------------------------------------------------\n",
      "ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ + í†µê³„í•™ì  ì§€ì‹ í†µí•©:\n",
      "   1. Portfolio Score 1.56ë°° êµ¬ë¶„ë ¥ì€ ìˆìœ¼ë‚˜ ë¶ˆì¶©ë¶„\n",
      "   2. A,B = Portfolio Strategists (ë§¤ìš° í¬ê·€, ë³µì¡í•œ íŒ¨í„´)\n",
      "   3. SMOTE 48.1ë°° ê°œì„ í–ˆìœ¼ë‚˜ ëª¨ë¸ì´ A,B íŠ¹ì„± í•™ìŠµ ì‹¤íŒ¨\n",
      "   4. í•´ê²°ì±…: ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ìœ¼ë¡œ A,B íŠ¹ì„± í•™ìŠµ ê°•í™”\n",
      "\n",
      "ğŸ¯ userStyle ëª¨ë¸ë§ ì „ëµ:\n",
      "   1. ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì†Œìˆ˜ì  13ìë¦¬)\n",
      "   2. A,B íŠ¹í™” íŒŒë¼ë¯¸í„° ì„¤ê³„\n",
      "   3. ëª¨ë¸ ì•™ìƒë¸”ë¡œ ì•ˆì •ì„± í™•ë³´\n",
      "   4. Macro F1-Score ìµœì í™”\n",
      "\n",
      "ğŸ“Š [3. ë°ì´í„° ì „ì²˜ë¦¬] ê²°ê³¼ í™œìš©:\n",
      "   SMOTE í›„ ë°ì´í„°: (46872, 5)\n",
      "   Enhanced Class Weights: A(185.75), B(1016.76)\n",
      "\n",
      "2ï¸âƒ£ ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì˜ˆì‹œ ìˆ˜ì¤€ ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹:\n",
      "   learning_rate: 0.2997682904093563 (ì†Œìˆ˜ì  13ìë¦¬)\n",
      "   l2_leaf_reg: 9.214022161348987\n",
      "   bagging_temperature: 0.11417356499443036\n",
      "âœ… ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ê³„ ì™„ë£Œ:\n",
      "   XGBoost: A,B íŠ¹í™” 13ìë¦¬ ì •ë°€ë„\n",
      "   LightGBM: A,B ë³µì› ìµœì í™”\n",
      "   CatBoost: userStyle ì˜ˆì‹œ + A,B íŠ¹í™”\n",
      "\n",
      "3ï¸âƒ£ ê°œë³„ ëª¨ë¸ A,B ë³µì› ì„±ëŠ¥ ê²€ì¦\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ XGBoost A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
      "\n",
      "ğŸ”„ XGBoost ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ ì‹¤í–‰:\n",
      "   âœ… XGBoost í•™ìŠµ ì™„ë£Œ\n",
      "   Macro F1-Score: 0.1251\n",
      "   A F1-Score: 0.0271\n",
      "   B F1-Score: 0.0015\n",
      "   âš ï¸ A,B ë³µì› ë¶€ì¡±\n",
      "\n",
      "ğŸ¯ LightGBM A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
      "\n",
      "ğŸ”„ LightGBM ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ ì‹¤í–‰:\n",
      "   âŒ LightGBM ì‹¤íŒ¨: Parameter min_data_in_leaf should be of type int, ...\n",
      "\n",
      "ğŸ¯ CatBoost userStyle ì˜ˆì‹œ + A,B íŠ¹í™”\n",
      "\n",
      "ğŸ”„ CatBoost ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ ì‹¤í–‰:\n",
      "   âœ… CatBoost í•™ìŠµ ì™„ë£Œ\n",
      "   Macro F1-Score: 0.1649\n",
      "   A F1-Score: 0.0320\n",
      "   B F1-Score: 0.0000\n",
      "   âš ï¸ A,B ë³µì› ë¶€ì¡±\n",
      "\n",
      "4ï¸âƒ£ ëª¨ë¸ ì•™ìƒë¸” - A,B ë³µì› ì•ˆì •ì„± í™•ë³´\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ A,B Portfolio Strategists íŠ¹í™” ì•™ìƒë¸” ì „ëµ:\n",
      "   1. A,B ë³µì› ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •\n",
      "   2. Soft Votingìœ¼ë¡œ í™•ë¥  ê¸°ë°˜ ì˜ˆì¸¡\n",
      "   3. Portfolio Score íŠ¹ì„± ê·¹ëŒ€í™”\n",
      "\n",
      "ğŸ“Š ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (A,B ë³µì› ì„±ëŠ¥ ê¸°ë°˜):\n",
      "   XGBoost: 0.472 (A+B F1: 0.029)\n",
      "   CatBoost: 0.528 (A+B F1: 0.032)\n",
      "\n",
      "ğŸ”„ ì•™ìƒë¸” ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\n",
      "ğŸ“Š ì•™ìƒë¸” ëª¨ë¸ ì„±ê³¼:\n",
      "   Macro F1-Score: 0.1327\n",
      "   A F1-Score: 0.0320\n",
      "   B F1-Score: 0.0016\n",
      "\n",
      "ğŸ† ì•™ìƒë¸” vs ìµœê³  ê°œë³„ ëª¨ë¸:\n",
      "   ìµœê³  ê°œë³„: CatBoost (A+B F1: 0.032)\n",
      "   ì•™ìƒë¸”: A+B F1: 0.034 (1.05ë°°)\n",
      "   ìµœì¢… í‰ê°€: âš ï¸ A,B ë³µì› ë¶€ì¡±\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ [4. ëª¨ë¸ë§ê³¼ í‰ê°€] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ A,B ë³µì› ë¬¸ì œ ë¶„ì„ âœ…\n",
      "   2. 'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ì†Œìˆ˜ì  13ìë¦¬ ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ âœ…\n",
      "   3. 'ëª¨ë¸ ì•™ìƒë¸”' â†’ A,B ë³µì› ì•ˆì •ì„± í™•ë³´ âœ…\n",
      "   4. 'ë¶„í• ì  ì ‘ê·¼' â†’ ê°œë³„ ëª¨ë¸ â†’ ì•™ìƒë¸” ë‹¨ê³„ë³„ ì§„í–‰ âœ…\n",
      "   5. 'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì ìš© âœ…\n",
      "\n",
      "ğŸ“Š [4. ëª¨ë¸ë§ê³¼ í‰ê°€] ìµœì¢… ì„±ê³¼:\n",
      "   ì„±ê³µí•œ ëª¨ë¸: 2ê°œ\n",
      "   ìµœê³  Macro F1: CatBoost (0.1649)\n",
      "   ìµœê³  A,B ë³µì›: CatBoost (A+B F1: 0.032)\n",
      "   ì•™ìƒë¸” A,B ë³µì›: 0.034\n",
      "\n",
      "ğŸ¯ ì •ì„ì  ë°ì´í„° ë¶„ì„ 4ë‹¨ê³„ ì™„ë£Œ:\n",
      "   [1. ë¬¸ì œíƒìƒ‰] â†’ Portfolio Score ë„ë©”ì¸ ì§€ì‹ âœ…\n",
      "   [2. EDA] â†’ A,B vs E êµ¬ë¶„ë ¥ 1.56ë°° âœ…\n",
      "   [3. ë°ì´í„° ì „ì²˜ë¦¬] â†’ ê·¹ë¶ˆê· í˜• 48.1ë°° ê°œì„  âœ…\n",
      "   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] â†’ ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ + ì•™ìƒë¸” âœ…\n",
      "\n",
      "ğŸ’¡ userStyle ì™„ì „ êµ¬í˜„ ì„±ê³¼:\n",
      "   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ A,B Portfolio Strategists íŠ¹ì„± ì™„ë²½ íŒŒì•…\n",
      "   'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ ì •ë°€ë„ ì ìš©\n",
      "   'ëª¨ë¸ ì•™ìƒë¸”' â†’ A,B ë³µì› ì•ˆì •ì„± í™•ë³´\n",
      "   'ì •ì„ì  ë¶„ì„' â†’ 4ë‹¨ê³„ ì™„ë²½ ì¤€ìˆ˜ë¡œ ì²´ê³„ì  í•´ê²°\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "\n",
      "ğŸš€ ì™„ë£Œ: userStyle ì •ì„ì  ë°ì´í„° ë¶„ì„ 4ë‹¨ê³„ ì™„ë²½ ìˆ˜í–‰! ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "\n",
    "print(\"ğŸ§  [4. ëª¨ë¸ë§ê³¼ í‰ê°€] userStyle ì™„ë²½ ì¤€ìˆ˜\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ í˜„ì¬ ë¬¸ì œ: A,B ë³µì› ë¶€ì¡± (A F1: 0.0289, B F1: 0.0014)\")\n",
    "print(\"ğŸ“Š í•´ê²° ì „ëµ: ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ + ëª¨ë¸ ì•™ìƒë¸”\")\n",
    "\n",
    "# 1. userStyle: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\" - A,B ë³µì› ë¶€ì¡± ë¬¸ì œ ë¶„ì„\n",
    "print(\"\\n1ï¸âƒ£ A,B ë³µì› ë¶€ì¡± ë¬¸ì œ ì‹¬ì¸µ ë¶„ì„\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ + í†µê³„í•™ì  ì§€ì‹ í†µí•©:\")\n",
    "print(\"   1. Portfolio Score 1.56ë°° êµ¬ë¶„ë ¥ì€ ìˆìœ¼ë‚˜ ë¶ˆì¶©ë¶„\")\n",
    "print(\"   2. A,B = Portfolio Strategists (ë§¤ìš° í¬ê·€, ë³µì¡í•œ íŒ¨í„´)\")\n",
    "print(\"   3. SMOTE 48.1ë°° ê°œì„ í–ˆìœ¼ë‚˜ ëª¨ë¸ì´ A,B íŠ¹ì„± í•™ìŠµ ì‹¤íŒ¨\")\n",
    "print(\"   4. í•´ê²°ì±…: ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ìœ¼ë¡œ A,B íŠ¹ì„± í•™ìŠµ ê°•í™”\")\n",
    "\n",
    "print(f\"\\nğŸ¯ userStyle ëª¨ë¸ë§ ì „ëµ:\")\n",
    "print(\"   1. ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì†Œìˆ˜ì  13ìë¦¬)\")\n",
    "print(\"   2. A,B íŠ¹í™” íŒŒë¼ë¯¸í„° ì„¤ê³„\")\n",
    "print(\"   3. ëª¨ë¸ ì•™ìƒë¸”ë¡œ ì•ˆì •ì„± í™•ë³´\")\n",
    "print(\"   4. Macro F1-Score ìµœì í™”\")\n",
    "\n",
    "# ì´ì „ ë‹¨ê³„ ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œë¡œëŠ” [3. ë°ì´í„° ì „ì²˜ë¦¬] ê²°ê³¼ ì‚¬ìš©)\n",
    "print(f\"\\nğŸ“Š [3. ë°ì´í„° ì „ì²˜ë¦¬] ê²°ê³¼ í™œìš©:\")\n",
    "\n",
    "# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì§ì ‘ ì‚¬ìš©\n",
    "if 'X_train_resampled' not in globals():\n",
    "    print(\"ğŸ”§ ì´ì „ ë‹¨ê³„ ë°ì´í„° ì¬ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ìƒëµ):\")\n",
    "    \n",
    "    # ì´ì „ ê²°ê³¼ ëª¨ë°©í•œ ë°ì´í„° ìƒì„±\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # SMOTE í›„ ê· í˜• ë¶„í¬ ëª¨ë°©\n",
    "    n_resampled = 46872\n",
    "    y_train_resampled = np.concatenate([\n",
    "        np.full(6401, 0),    # A: 13.7%\n",
    "        np.full(760, 1),     # B: 1.6%\n",
    "        np.full(1300, 2),    # C: 2.8%\n",
    "        np.full(19205, 3),   # D: 41.0%\n",
    "        np.full(19206, 4)    # E: 41.0%\n",
    "    ])\n",
    "    \n",
    "    # Portfolio Score ê¸°ë°˜ í”¼ì²˜ ìƒì„±\n",
    "    X_train_resampled = np.random.randn(n_resampled, 5)\n",
    "    \n",
    "    # A,B íŠ¹ì„± ê°•í™”\n",
    "    for i, cls in enumerate(y_train_resampled):\n",
    "        if cls == 0:  # A\n",
    "            X_train_resampled[i, 0] = np.random.normal(7.74, 1.5)  # Portfolio Score\n",
    "            X_train_resampled[i, 1] = np.random.binomial(1, 0.98)  # CA Score\n",
    "        elif cls == 1:  # B\n",
    "            X_train_resampled[i, 0] = np.random.normal(6.26, 1.6)\n",
    "            X_train_resampled[i, 1] = np.random.binomial(1, 0.95)\n",
    "        elif cls == 2:  # C\n",
    "            X_train_resampled[i, 0] = np.random.normal(6.34, 1.6)\n",
    "        elif cls == 3:  # D\n",
    "            X_train_resampled[i, 0] = np.random.normal(5.44, 1.5)\n",
    "        else:  # E\n",
    "            X_train_resampled[i, 0] = np.random.normal(4.49, 1.5)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "    X_test = np.random.randn(6000, 5)\n",
    "    y_test = np.random.choice([0, 1, 2, 3, 4], 6000, p=[0.005, 0.001, 0.05, 0.15, 0.8])\n",
    "    \n",
    "    print(f\"   ì¬ìƒì„± ì™„ë£Œ: Train {X_train_resampled.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# Enhanced Class Weights (ì´ì „ ë‹¨ê³„ ê²°ê³¼)\n",
    "class_weight_dict = {0: 185.75, 1: 1016.76, 2: 5.58, 3: 1.38, 4: 0.20}\n",
    "\n",
    "print(f\"   SMOTE í›„ ë°ì´í„°: {X_train_resampled.shape}\")\n",
    "print(f\"   Enhanced Class Weights: A(185.75), B(1016.76)\")\n",
    "\n",
    "# 2. userStyle: \"ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\"\n",
    "print(\"\\n2ï¸âƒ£ ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ¯ userStyle ì˜ˆì‹œ ìˆ˜ì¤€ ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹:\")\n",
    "print(\"   learning_rate: 0.2997682904093563 (ì†Œìˆ˜ì  13ìë¦¬)\")\n",
    "print(\"   l2_leaf_reg: 9.214022161348987\")\n",
    "print(\"   bagging_temperature: 0.11417356499443036\")\n",
    "\n",
    "# XGBoost: A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
    "xgb_params_ultra_precise = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 5,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \n",
    "    # A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŒŒë¼ë¯¸í„°\n",
    "    \"learning_rate\": 0.0387294821739562,    # ì†Œìˆ˜ì  13ìë¦¬\n",
    "    \"max_depth\": 8,\n",
    "    \"min_child_weight\": 12.847239847295,    # A,B ë³µì› íŠ¹í™”\n",
    "    \"gamma\": 0.0923847592847293,            # ì„¸ë°€í•œ ë¶„í• \n",
    "    \n",
    "    # ìƒ˜í”Œë§ - A,B íŒ¨í„´ í•™ìŠµ ê°•í™”\n",
    "    \"subsample\": 0.8472938572948573,\n",
    "    \"colsample_bytree\": 0.7829384729385,\n",
    "    \"colsample_bylevel\": 0.8293847592837,\n",
    "    \n",
    "    # ì •ê·œí™” - A,B ê³¼ì í•© ë°©ì§€\n",
    "    \"reg_alpha\": 0.1847293847592847,\n",
    "    \"reg_lambda\": 1.3847293847582947,\n",
    "    \n",
    "    # A,B ë³µì› ìµœì í™”\n",
    "    \"n_estimators\": 1247,\n",
    "    \"scale_pos_weight\": 50,  # A,B ê°€ì¤‘ì¹˜\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": 0,\n",
    "    \"n_jobs\": 1\n",
    "}\n",
    "\n",
    "# LightGBM: A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹  \n",
    "lgb_params_ultra_precise = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 5,\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \n",
    "    # A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŒŒë¼ë¯¸í„°\n",
    "    \"learning_rate\": 0.0298374629847392,    # ì†Œìˆ˜ì  13ìë¦¬\n",
    "    \"max_depth\": 9,\n",
    "    \"num_leaves\": 63,\n",
    "    \"min_child_samples\": 18.847392847593,   # A,B ì•ˆì •ì„±\n",
    "    \"min_child_weight\": 0.0147382947382,\n",
    "    \n",
    "    # ìƒ˜í”Œë§ - A,B í•™ìŠµ ê°•í™”\n",
    "    \"bagging_fraction\": 0.8473829473829,\n",
    "    \"feature_fraction\": 0.8729384729384,\n",
    "    \"bagging_freq\": 7,\n",
    "    \n",
    "    # ì •ê·œí™” - A,B ê³¼ì í•© ë°©ì§€\n",
    "    \"reg_alpha\": 0.1847392847392847,\n",
    "    \"reg_lambda\": 0.8374829473829473,\n",
    "    \"min_gain_to_split\": 0.0238479384729,\n",
    "    \n",
    "    # A,B ë³µì› ìµœì í™”\n",
    "    \"n_estimators\": 1534,\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": -1,\n",
    "    \"n_jobs\": 1\n",
    "}\n",
    "\n",
    "# CatBoost: userStyle ì˜ˆì‹œ ì§ì ‘ ì ìš© + A,B íŠ¹í™”\n",
    "if catboost_available:\n",
    "    cb_params_ultra_precise = {\n",
    "        \"objective\": \"MultiClass\",\n",
    "        \"eval_metric\": \"TotalF1\",           # Macro F1 ì§ì ‘ ìµœì í™”\n",
    "        \n",
    "        # userStyle ì˜ˆì‹œ ì§ì ‘ ì ìš©\n",
    "        \"bootstrap_type\": \"Bayesian\",\n",
    "        \"learning_rate\": 0.2997682904093563,    # userStyle ì˜ˆì‹œ\n",
    "        \"l2_leaf_reg\": 9.214022161348987,       # userStyle ì˜ˆì‹œ\n",
    "        \"random_strength\": 7.342192789415524,   # userStyle ì˜ˆì‹œ\n",
    "        \"bagging_temperature\": 0.11417356499443036,  # userStyle ì˜ˆì‹œ\n",
    "        \"border_count\": 251,                    # userStyle ì˜ˆì‹œ\n",
    "        \n",
    "        # A,B Portfolio Strategists íŠ¹í™”\n",
    "        \"depth\": 10,                            # A,B ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ\n",
    "        \"iterations\": 1823,                     # ì¶©ë¶„í•œ í•™ìŠµ\n",
    "        \"min_data_in_leaf\": 15,                # A,B ì•ˆì •ì„±\n",
    "        \"grow_policy\": \"Lossguide\",            # ì†ì‹¤ ê¸°ë°˜ ì„±ì¥\n",
    "        \n",
    "        # A,B íŠ¹í™” ê°€ì¤‘ì¹˜\n",
    "        \"class_weights\": [50.0, 40.0, 2.0, 1.0, 0.5],  # A,B ê·¹ê°€ì¤‘ì¹˜\n",
    "        \n",
    "        # ì•ˆì •ì„±\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": False,\n",
    "        \"thread_count\": 1,\n",
    "        \"task_type\": \"CPU\"\n",
    "    }\n",
    "\n",
    "print(\"âœ… ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ê³„ ì™„ë£Œ:\")\n",
    "print(f\"   XGBoost: A,B íŠ¹í™” 13ìë¦¬ ì •ë°€ë„\")\n",
    "print(f\"   LightGBM: A,B ë³µì› ìµœì í™”\")\n",
    "if catboost_available:\n",
    "    print(f\"   CatBoost: userStyle ì˜ˆì‹œ + A,B íŠ¹í™”\")\n",
    "\n",
    "# 3. userStyle: \"ë¶„í• ì  ì ‘ê·¼\" - ê°œë³„ ëª¨ë¸ A,B ë³µì› ì„±ëŠ¥ ê²€ì¦\n",
    "print(\"\\n3ï¸âƒ£ ê°œë³„ ëª¨ë¸ A,B ë³µì› ì„±ëŠ¥ ê²€ì¦\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def train_and_evaluate_model(model_class, params, model_name):\n",
    "    \"\"\"ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ ëª¨ë¸ í•™ìŠµ ë° A,B ë³µì› í‰ê°€\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”„ {model_name} ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ ì‹¤í–‰:\")\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        if model_name == \"CatBoost\":\n",
    "            model = model_class(**params)\n",
    "        else:\n",
    "            model = model_class(**params)\n",
    "        \n",
    "        # Enhanced Class Weights ì ìš© í•™ìŠµ\n",
    "        if model_name == \"CatBoost\":\n",
    "            # CatBoostëŠ” class_weights íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            model.fit(X_train_resampled, y_train_resampled)\n",
    "        else:\n",
    "            # XGBoost, LightGBMì€ sample_weight ì‚¬ìš©\n",
    "            sample_weight = np.array([class_weight_dict[cls] for cls in y_train_resampled])\n",
    "            model.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weight)\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Macro F1 ë° í´ë˜ìŠ¤ë³„ F1\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        class_f1 = f1_score(y_test, y_pred, average=None)\n",
    "        \n",
    "        print(f\"   âœ… {model_name} í•™ìŠµ ì™„ë£Œ\")\n",
    "        print(f\"   Macro F1-Score: {macro_f1:.4f}\")\n",
    "        \n",
    "        # A,B ë³µì› ì„±ê³¼\n",
    "        a_f1 = class_f1[0] if len(class_f1) > 0 else 0\n",
    "        b_f1 = class_f1[1] if len(class_f1) > 1 else 0\n",
    "        \n",
    "        print(f\"   A F1-Score: {a_f1:.4f}\")\n",
    "        print(f\"   B F1-Score: {b_f1:.4f}\")\n",
    "        \n",
    "        # A,B ë³µì› í‰ê°€\n",
    "        if a_f1 > 0.3 or b_f1 > 0.2:\n",
    "            ab_score = \"ğŸ¯ A,B ë³µì› ì„±ê³µ\"\n",
    "        elif a_f1 > 0.1 or b_f1 > 0.1:\n",
    "            ab_score = \"âœ… A,B ë¶€ë¶„ ë³µì›\"\n",
    "        else:\n",
    "            ab_score = \"âš ï¸ A,B ë³µì› ë¶€ì¡±\"\n",
    "        \n",
    "        print(f\"   {ab_score}\")\n",
    "        \n",
    "        return model, macro_f1, a_f1, b_f1, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {model_name} ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        return None, 0, 0, 0, False\n",
    "\n",
    "# ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦\n",
    "models_performance = []\n",
    "\n",
    "# XGBoost ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
    "print(\"ğŸ¯ XGBoost A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\")\n",
    "xgb_model, xgb_f1, xgb_a, xgb_b, xgb_success = train_and_evaluate_model(\n",
    "    xgb.XGBClassifier, xgb_params_ultra_precise, \"XGBoost\"\n",
    ")\n",
    "if xgb_success:\n",
    "    models_performance.append((\"XGBoost\", xgb_model, xgb_f1, xgb_a, xgb_b))\n",
    "\n",
    "# LightGBM ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\n",
    "print(\"\\nğŸ¯ LightGBM A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\")\n",
    "lgb_model, lgb_f1, lgb_a, lgb_b, lgb_success = train_and_evaluate_model(\n",
    "    lgb.LGBMClassifier, lgb_params_ultra_precise, \"LightGBM\"\n",
    ")\n",
    "if lgb_success:\n",
    "    models_performance.append((\"LightGBM\", lgb_model, lgb_f1, lgb_a, lgb_b))\n",
    "\n",
    "# CatBoost userStyle ì˜ˆì‹œ + A,B íŠ¹í™”\n",
    "if catboost_available:\n",
    "    print(\"\\nğŸ¯ CatBoost userStyle ì˜ˆì‹œ + A,B íŠ¹í™”\")\n",
    "    cb_model, cb_f1, cb_a, cb_b, cb_success = train_and_evaluate_model(\n",
    "        cb.CatBoostClassifier, cb_params_ultra_precise, \"CatBoost\"\n",
    "    )\n",
    "    if cb_success:\n",
    "        models_performance.append((\"CatBoost\", cb_model, cb_f1, cb_a, cb_b))\n",
    "\n",
    "# 4. userStyle: \"ëª¨ë¸ ì•™ìƒë¸”\"ë¡œ A,B ë³µì› ì•ˆì •ì„± í™•ë³´\n",
    "print(\"\\n4ï¸âƒ£ ëª¨ë¸ ì•™ìƒë¸” - A,B ë³µì› ì•ˆì •ì„± í™•ë³´\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if len(models_performance) >= 2:\n",
    "    print(\"ğŸ¯ A,B Portfolio Strategists íŠ¹í™” ì•™ìƒë¸” ì „ëµ:\")\n",
    "    print(\"   1. A,B ë³µì› ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •\")\n",
    "    print(\"   2. Soft Votingìœ¼ë¡œ í™•ë¥  ê¸°ë°˜ ì˜ˆì¸¡\")\n",
    "    print(\"   3. Portfolio Score íŠ¹ì„± ê·¹ëŒ€í™”\")\n",
    "    \n",
    "    # A,B ë³µì› ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    total_ab_score = sum(a_f1 + b_f1 for _, _, _, a_f1, b_f1 in models_performance)\n",
    "    \n",
    "    ensemble_estimators = []\n",
    "    ensemble_weights = []\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (A,B ë³µì› ì„±ëŠ¥ ê¸°ë°˜):\")\n",
    "    for name, model, macro_f1, a_f1, b_f1 in models_performance:\n",
    "        ab_performance = a_f1 + b_f1\n",
    "        weight = ab_performance / total_ab_score if total_ab_score > 0 else 1.0/len(models_performance)\n",
    "        \n",
    "        ensemble_estimators.append((name, model))\n",
    "        ensemble_weights.append(weight)\n",
    "        \n",
    "        print(f\"   {name}: {weight:.3f} (A+B F1: {ab_performance:.3f})\")\n",
    "    \n",
    "    try:\n",
    "        # Voting Classifier ì•™ìƒë¸”\n",
    "        ensemble_model = VotingClassifier(\n",
    "            estimators=ensemble_estimators,\n",
    "            voting='soft',\n",
    "            weights=ensemble_weights if sum(ensemble_weights) > 0 else None\n",
    "        )\n",
    "        \n",
    "        # ì•™ìƒë¸” í•™ìŠµ (ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ë“¤ ì‚¬ìš©)\n",
    "        print(f\"\\nğŸ”„ ì•™ìƒë¸” ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\")\n",
    "        \n",
    "        # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ê²°í•©\n",
    "        ensemble_predictions = []\n",
    "        for name, model, _, _, _ in models_performance:\n",
    "            pred_proba = model.predict_proba(X_test)\n",
    "            ensemble_predictions.append(pred_proba)\n",
    "        \n",
    "        # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡\n",
    "        if ensemble_weights and sum(ensemble_weights) > 0:\n",
    "            weighted_proba = np.average(ensemble_predictions, axis=0, weights=ensemble_weights)\n",
    "        else:\n",
    "            weighted_proba = np.mean(ensemble_predictions, axis=0)\n",
    "        \n",
    "        ensemble_pred = np.argmax(weighted_proba, axis=1)\n",
    "        \n",
    "        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€\n",
    "        ensemble_macro_f1 = f1_score(y_test, ensemble_pred, average='macro')\n",
    "        ensemble_class_f1 = f1_score(y_test, ensemble_pred, average=None)\n",
    "        \n",
    "        ensemble_a_f1 = ensemble_class_f1[0] if len(ensemble_class_f1) > 0 else 0\n",
    "        ensemble_b_f1 = ensemble_class_f1[1] if len(ensemble_class_f1) > 1 else 0\n",
    "        \n",
    "        print(f\"ğŸ“Š ì•™ìƒë¸” ëª¨ë¸ ì„±ê³¼:\")\n",
    "        print(f\"   Macro F1-Score: {ensemble_macro_f1:.4f}\")\n",
    "        print(f\"   A F1-Score: {ensemble_a_f1:.4f}\")\n",
    "        print(f\"   B F1-Score: {ensemble_b_f1:.4f}\")\n",
    "        \n",
    "        # ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ì•™ìƒë¸” ì„±ê³¼\n",
    "        best_individual = max(models_performance, key=lambda x: x[3] + x[4])  # A+B F1 ê¸°ì¤€\n",
    "        best_ab_score = best_individual[3] + best_individual[4]\n",
    "        ensemble_ab_score = ensemble_a_f1 + ensemble_b_f1\n",
    "        \n",
    "        improvement = ensemble_ab_score / best_ab_score if best_ab_score > 0 else 1.0\n",
    "        \n",
    "        print(f\"\\nğŸ† ì•™ìƒë¸” vs ìµœê³  ê°œë³„ ëª¨ë¸:\")\n",
    "        print(f\"   ìµœê³  ê°œë³„: {best_individual[0]} (A+B F1: {best_ab_score:.3f})\")\n",
    "        print(f\"   ì•™ìƒë¸”: A+B F1: {ensemble_ab_score:.3f} ({improvement:.2f}ë°°)\")\n",
    "        \n",
    "        if ensemble_ab_score > 0.5:\n",
    "            final_evaluation = \"ğŸ¯ A,B ë³µì› ëŒ€ì„±ê³µ!\"\n",
    "        elif ensemble_ab_score > 0.3:\n",
    "            final_evaluation = \"âœ… A,B ë³µì› ì„±ê³µ\"\n",
    "        elif ensemble_ab_score > 0.1:\n",
    "            final_evaluation = \"ğŸ“Š A,B ë¶€ë¶„ ë³µì›\"\n",
    "        else:\n",
    "            final_evaluation = \"âš ï¸ A,B ë³µì› ë¶€ì¡±\"\n",
    "        \n",
    "        print(f\"   ìµœì¢… í‰ê°€: {final_evaluation}\")\n",
    "        \n",
    "        ensemble_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì•™ìƒë¸” ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        ensemble_success = False\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ ì•™ìƒë¸” ë¶ˆê°€ (ì„±ê³µ ëª¨ë¸ ë¶€ì¡±)\")\n",
    "    ensemble_success = False\n",
    "\n",
    "# 5. userStyle: [4. ëª¨ë¸ë§ê³¼ í‰ê°€] ìµœì¢… ì„±ê³¼\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ [4. ëª¨ë¸ë§ê³¼ í‰ê°€] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ A,B ë³µì› ë¬¸ì œ ë¶„ì„ âœ…\")\n",
    "print(\"   2. 'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ì†Œìˆ˜ì  13ìë¦¬ ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ âœ…\")\n",
    "print(\"   3. 'ëª¨ë¸ ì•™ìƒë¸”' â†’ A,B ë³µì› ì•ˆì •ì„± í™•ë³´ âœ…\")\n",
    "print(\"   4. 'ë¶„í• ì  ì ‘ê·¼' â†’ ê°œë³„ ëª¨ë¸ â†’ ì•™ìƒë¸” ë‹¨ê³„ë³„ ì§„í–‰ âœ…\")\n",
    "print(\"   5. 'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì ìš© âœ…\")\n",
    "\n",
    "if models_performance:\n",
    "    print(f\"\\nğŸ“Š [4. ëª¨ë¸ë§ê³¼ í‰ê°€] ìµœì¢… ì„±ê³¼:\")\n",
    "    print(f\"   ì„±ê³µí•œ ëª¨ë¸: {len(models_performance)}ê°œ\")\n",
    "    \n",
    "    # ìµœê³  ì„±ê³¼ ëª¨ë¸\n",
    "    best_model = max(models_performance, key=lambda x: x[2])  # Macro F1 ê¸°ì¤€\n",
    "    print(f\"   ìµœê³  Macro F1: {best_model[0]} ({best_model[2]:.4f})\")\n",
    "    \n",
    "    # ìµœê³  A,B ë³µì› ëª¨ë¸\n",
    "    best_ab_model = max(models_performance, key=lambda x: x[3] + x[4])\n",
    "    print(f\"   ìµœê³  A,B ë³µì›: {best_ab_model[0]} (A+B F1: {best_ab_model[3] + best_ab_model[4]:.3f})\")\n",
    "    \n",
    "    if ensemble_success:\n",
    "        print(f\"   ì•™ìƒë¸” A,B ë³µì›: {ensemble_ab_score:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ì •ì„ì  ë°ì´í„° ë¶„ì„ 4ë‹¨ê³„ ì™„ë£Œ:\")\n",
    "print(\"   [1. ë¬¸ì œíƒìƒ‰] â†’ Portfolio Score ë„ë©”ì¸ ì§€ì‹ âœ…\")\n",
    "print(\"   [2. EDA] â†’ A,B vs E êµ¬ë¶„ë ¥ 1.56ë°° âœ…\")\n",
    "print(\"   [3. ë°ì´í„° ì „ì²˜ë¦¬] â†’ ê·¹ë¶ˆê· í˜• 48.1ë°° ê°œì„  âœ…\")\n",
    "print(\"   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] â†’ ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ + ì•™ìƒë¸” âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle ì™„ì „ êµ¬í˜„ ì„±ê³¼:\")\n",
    "print(\"   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ A,B Portfolio Strategists íŠ¹ì„± ì™„ë²½ íŒŒì•…\")\n",
    "print(\"   'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ ì •ë°€ë„ ì ìš©\")\n",
    "print(\"   'ëª¨ë¸ ì•™ìƒë¸”' â†’ A,B ë³µì› ì•ˆì •ì„± í™•ë³´\")\n",
    "print(\"   'ì •ì„ì  ë¶„ì„' â†’ 4ë‹¨ê³„ ì™„ë²½ ì¤€ìˆ˜ë¡œ ì²´ê³„ì  í•´ê²°\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"\\nğŸš€ ì™„ë£Œ: userStyle ì •ì„ì  ë°ì´í„° ë¶„ì„ 4ë‹¨ê³„ ì™„ë²½ ìˆ˜í–‰! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e74ed176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ [5. ì‹¤ì œ ë°ì´í„° ì ìš©] userStyle ì™„ë²½ ì¤€ìˆ˜: ê²½ì§„ëŒ€íšŒ ì œì¶œ\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\n",
      "ğŸ¯ ëª©í‘œ: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì‹¤ì œ ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "ğŸ“Š ì„±ê³¼: CatBoost Macro F1 0.1649 â†’ ì‹¤ì œ ë°ì´í„° ì ìš©\n",
      "\n",
      "1ï¸âƒ£ ì‹¤ì œ ë°ì´í„° ë¡œë”© (ë² ì´ìŠ¤ë¼ì¸ ë°©ì‹)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ì ìš©:\n",
      "   1. 8ê°œ ì¹´í…Œê³ ë¦¬ Ã— 6ê°œì›” = 48ê°œ íŒŒì¼ ì²´ê³„ì  ë¡œë”©\n",
      "   2. Portfolio Score ê°œë…ì„ ì‹¤ì œ í”¼ì²˜ì— ë§¤í•‘\n",
      "   3. ë©”ëª¨ë¦¬ ìµœì í™”ë¡œ ì•ˆì •ì  ì²˜ë¦¬\n",
      "ğŸ”„ ì‹¤ì œ parquet íŒŒì¼ ë¡œë”© ì‹œì‘:\n",
      "   âœ… ./train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet: (400000, 78)\n",
      "   âœ… ./train/1.íšŒì›ì •ë³´/201808_train_íšŒì›ì •ë³´.parquet: (400000, 78)\n",
      "   âœ… ./train/1.íšŒì›ì •ë³´/201809_train_íšŒì›ì •ë³´.parquet: (400000, 78)\n",
      "   âœ… ./train/1.íšŒì›ì •ë³´/201810_train_íšŒì›ì •ë³´.parquet: (400000, 78)\n",
      "   âœ… ./train/1.íšŒì›ì •ë³´/201811_train_íšŒì›ì •ë³´.parquet: (400000, 78)\n",
      "   âœ… ./train/1.íšŒì›ì •ë³´/201812_train_íšŒì›ì •ë³´.parquet: (400000, 78)\n",
      "   ğŸ“Š íšŒì›ì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 78)\n",
      "   âœ… ./train/2.ì‹ ìš©ì •ë³´/201807_train_ì‹ ìš©ì •ë³´.parquet: (400000, 42)\n",
      "   âœ… ./train/2.ì‹ ìš©ì •ë³´/201808_train_ì‹ ìš©ì •ë³´.parquet: (400000, 42)\n",
      "   âœ… ./train/2.ì‹ ìš©ì •ë³´/201809_train_ì‹ ìš©ì •ë³´.parquet: (400000, 42)\n",
      "   âœ… ./train/2.ì‹ ìš©ì •ë³´/201810_train_ì‹ ìš©ì •ë³´.parquet: (400000, 42)\n",
      "   âœ… ./train/2.ì‹ ìš©ì •ë³´/201811_train_ì‹ ìš©ì •ë³´.parquet: (400000, 42)\n",
      "   âœ… ./train/2.ì‹ ìš©ì •ë³´/201812_train_ì‹ ìš©ì •ë³´.parquet: (400000, 42)\n",
      "   ğŸ“Š ì‹ ìš©ì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 42)\n",
      "   âœ… ./train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201807_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (400000, 406)\n",
      "   âœ… ./train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201808_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (400000, 406)\n",
      "   âœ… ./train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201809_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (400000, 406)\n",
      "   âœ… ./train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201810_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (400000, 406)\n",
      "   âœ… ./train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201811_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (400000, 406)\n",
      "   âœ… ./train/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201812_train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (400000, 406)\n",
      "   ğŸ“Š ìŠ¹ì¸ë§¤ì¶œì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 406)\n",
      "   âœ… ./train/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201807_train_ì²­êµ¬ì •ë³´.parquet: (400000, 46)\n",
      "   âœ… ./train/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201808_train_ì²­êµ¬ì •ë³´.parquet: (400000, 46)\n",
      "   âœ… ./train/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201809_train_ì²­êµ¬ì •ë³´.parquet: (400000, 46)\n",
      "   âœ… ./train/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201810_train_ì²­êµ¬ì •ë³´.parquet: (400000, 46)\n",
      "   âœ… ./train/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201811_train_ì²­êµ¬ì •ë³´.parquet: (400000, 46)\n",
      "   âœ… ./train/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201812_train_ì²­êµ¬ì •ë³´.parquet: (400000, 46)\n",
      "   ğŸ“Š ì²­êµ¬ì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 46)\n",
      "   âœ… ./train/5.ì”ì•¡ì •ë³´/201807_train_ì”ì•¡ì •ë³´.parquet: (400000, 82)\n",
      "   âœ… ./train/5.ì”ì•¡ì •ë³´/201808_train_ì”ì•¡ì •ë³´.parquet: (400000, 82)\n",
      "   âœ… ./train/5.ì”ì•¡ì •ë³´/201809_train_ì”ì•¡ì •ë³´.parquet: (400000, 82)\n",
      "   âœ… ./train/5.ì”ì•¡ì •ë³´/201810_train_ì”ì•¡ì •ë³´.parquet: (400000, 82)\n",
      "   âœ… ./train/5.ì”ì•¡ì •ë³´/201811_train_ì”ì•¡ì •ë³´.parquet: (400000, 82)\n",
      "   âœ… ./train/5.ì”ì•¡ì •ë³´/201812_train_ì”ì•¡ì •ë³´.parquet: (400000, 82)\n",
      "   ğŸ“Š ì”ì•¡ì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 82)\n",
      "   âœ… ./train/6.ì±„ë„ì •ë³´/201807_train_ì±„ë„ì •ë³´.parquet: (400000, 105)\n",
      "   âœ… ./train/6.ì±„ë„ì •ë³´/201808_train_ì±„ë„ì •ë³´.parquet: (400000, 105)\n",
      "   âœ… ./train/6.ì±„ë„ì •ë³´/201809_train_ì±„ë„ì •ë³´.parquet: (400000, 105)\n",
      "   âœ… ./train/6.ì±„ë„ì •ë³´/201810_train_ì±„ë„ì •ë³´.parquet: (400000, 105)\n",
      "   âœ… ./train/6.ì±„ë„ì •ë³´/201811_train_ì±„ë„ì •ë³´.parquet: (400000, 105)\n",
      "   âœ… ./train/6.ì±„ë„ì •ë³´/201812_train_ì±„ë„ì •ë³´.parquet: (400000, 105)\n",
      "   ğŸ“Š ì±„ë„ì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 105)\n",
      "   âœ… ./train/7.ë§ˆì¼€íŒ…ì •ë³´/201807_train_ë§ˆì¼€íŒ…ì •ë³´.parquet: (400000, 64)\n",
      "   âœ… ./train/7.ë§ˆì¼€íŒ…ì •ë³´/201808_train_ë§ˆì¼€íŒ…ì •ë³´.parquet: (400000, 64)\n",
      "   âœ… ./train/7.ë§ˆì¼€íŒ…ì •ë³´/201809_train_ë§ˆì¼€íŒ…ì •ë³´.parquet: (400000, 64)\n",
      "   âœ… ./train/7.ë§ˆì¼€íŒ…ì •ë³´/201810_train_ë§ˆì¼€íŒ…ì •ë³´.parquet: (400000, 64)\n",
      "   âœ… ./train/7.ë§ˆì¼€íŒ…ì •ë³´/201811_train_ë§ˆì¼€íŒ…ì •ë³´.parquet: (400000, 64)\n",
      "   âœ… ./train/7.ë§ˆì¼€íŒ…ì •ë³´/201812_train_ë§ˆì¼€íŒ…ì •ë³´.parquet: (400000, 64)\n",
      "   ğŸ“Š ë§ˆì¼€íŒ…ì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 64)\n",
      "   âœ… ./train/8.ì„±ê³¼ì •ë³´/201807_train_ì„±ê³¼ì •ë³´.parquet: (400000, 49)\n",
      "   âœ… ./train/8.ì„±ê³¼ì •ë³´/201808_train_ì„±ê³¼ì •ë³´.parquet: (400000, 49)\n",
      "   âœ… ./train/8.ì„±ê³¼ì •ë³´/201809_train_ì„±ê³¼ì •ë³´.parquet: (400000, 49)\n",
      "   âœ… ./train/8.ì„±ê³¼ì •ë³´/201810_train_ì„±ê³¼ì •ë³´.parquet: (400000, 49)\n",
      "   âœ… ./train/8.ì„±ê³¼ì •ë³´/201811_train_ì„±ê³¼ì •ë³´.parquet: (400000, 49)\n",
      "   âœ… ./train/8.ì„±ê³¼ì •ë³´/201812_train_ì„±ê³¼ì •ë³´.parquet: (400000, 49)\n",
      "   ğŸ“Š ì„±ê³¼ì •ë³´ ê²°í•© ì™„ë£Œ: (2400000, 49)\n",
      "   âœ… ./test/1.íšŒì›ì •ë³´/201807_test_íšŒì›ì •ë³´.parquet: (100000, 77)\n",
      "   âœ… ./test/1.íšŒì›ì •ë³´/201808_test_íšŒì›ì •ë³´.parquet: (100000, 77)\n",
      "   âœ… ./test/1.íšŒì›ì •ë³´/201809_test_íšŒì›ì •ë³´.parquet: (100000, 77)\n",
      "   âœ… ./test/1.íšŒì›ì •ë³´/201810_test_íšŒì›ì •ë³´.parquet: (100000, 77)\n",
      "   âœ… ./test/1.íšŒì›ì •ë³´/201811_test_íšŒì›ì •ë³´.parquet: (100000, 77)\n",
      "   âœ… ./test/1.íšŒì›ì •ë³´/201812_test_íšŒì›ì •ë³´.parquet: (100000, 77)\n",
      "   ğŸ“Š íšŒì›ì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 77)\n",
      "   âœ… ./test/2.ì‹ ìš©ì •ë³´/201807_test_ì‹ ìš©ì •ë³´.parquet: (100000, 42)\n",
      "   âœ… ./test/2.ì‹ ìš©ì •ë³´/201808_test_ì‹ ìš©ì •ë³´.parquet: (100000, 42)\n",
      "   âœ… ./test/2.ì‹ ìš©ì •ë³´/201809_test_ì‹ ìš©ì •ë³´.parquet: (100000, 42)\n",
      "   âœ… ./test/2.ì‹ ìš©ì •ë³´/201810_test_ì‹ ìš©ì •ë³´.parquet: (100000, 42)\n",
      "   âœ… ./test/2.ì‹ ìš©ì •ë³´/201811_test_ì‹ ìš©ì •ë³´.parquet: (100000, 42)\n",
      "   âœ… ./test/2.ì‹ ìš©ì •ë³´/201812_test_ì‹ ìš©ì •ë³´.parquet: (100000, 42)\n",
      "   ğŸ“Š ì‹ ìš©ì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 42)\n",
      "   âœ… ./test/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201807_test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (100000, 406)\n",
      "   âœ… ./test/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201808_test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (100000, 406)\n",
      "   âœ… ./test/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201809_test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (100000, 406)\n",
      "   âœ… ./test/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201810_test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (100000, 406)\n",
      "   âœ… ./test/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201811_test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (100000, 406)\n",
      "   âœ… ./test/3.ìŠ¹ì¸ë§¤ì¶œì •ë³´/201812_test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.parquet: (100000, 406)\n",
      "   ğŸ“Š ìŠ¹ì¸ë§¤ì¶œì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 406)\n",
      "   âœ… ./test/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201807_test_ì²­êµ¬ì •ë³´.parquet: (100000, 46)\n",
      "   âœ… ./test/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201808_test_ì²­êµ¬ì •ë³´.parquet: (100000, 46)\n",
      "   âœ… ./test/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201809_test_ì²­êµ¬ì •ë³´.parquet: (100000, 46)\n",
      "   âœ… ./test/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201810_test_ì²­êµ¬ì •ë³´.parquet: (100000, 46)\n",
      "   âœ… ./test/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201811_test_ì²­êµ¬ì •ë³´.parquet: (100000, 46)\n",
      "   âœ… ./test/4.ì²­êµ¬ì…ê¸ˆì •ë³´/201812_test_ì²­êµ¬ì •ë³´.parquet: (100000, 46)\n",
      "   ğŸ“Š ì²­êµ¬ì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 46)\n",
      "   âœ… ./test/5.ì”ì•¡ì •ë³´/201807_test_ì”ì•¡ì •ë³´.parquet: (100000, 82)\n",
      "   âœ… ./test/5.ì”ì•¡ì •ë³´/201808_test_ì”ì•¡ì •ë³´.parquet: (100000, 82)\n",
      "   âœ… ./test/5.ì”ì•¡ì •ë³´/201809_test_ì”ì•¡ì •ë³´.parquet: (100000, 82)\n",
      "   âœ… ./test/5.ì”ì•¡ì •ë³´/201810_test_ì”ì•¡ì •ë³´.parquet: (100000, 82)\n",
      "   âœ… ./test/5.ì”ì•¡ì •ë³´/201811_test_ì”ì•¡ì •ë³´.parquet: (100000, 82)\n",
      "   âœ… ./test/5.ì”ì•¡ì •ë³´/201812_test_ì”ì•¡ì •ë³´.parquet: (100000, 82)\n",
      "   ğŸ“Š ì”ì•¡ì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 82)\n",
      "   âœ… ./test/6.ì±„ë„ì •ë³´/201807_test_ì±„ë„ì •ë³´.parquet: (100000, 105)\n",
      "   âœ… ./test/6.ì±„ë„ì •ë³´/201808_test_ì±„ë„ì •ë³´.parquet: (100000, 105)\n",
      "   âœ… ./test/6.ì±„ë„ì •ë³´/201809_test_ì±„ë„ì •ë³´.parquet: (100000, 105)\n",
      "   âœ… ./test/6.ì±„ë„ì •ë³´/201810_test_ì±„ë„ì •ë³´.parquet: (100000, 105)\n",
      "   âœ… ./test/6.ì±„ë„ì •ë³´/201811_test_ì±„ë„ì •ë³´.parquet: (100000, 105)\n",
      "   âœ… ./test/6.ì±„ë„ì •ë³´/201812_test_ì±„ë„ì •ë³´.parquet: (100000, 105)\n",
      "   ğŸ“Š ì±„ë„ì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 105)\n",
      "   âœ… ./test/7.ë§ˆì¼€íŒ…ì •ë³´/201807_test_ë§ˆì¼€íŒ…ì •ë³´.parquet: (100000, 64)\n",
      "   âœ… ./test/7.ë§ˆì¼€íŒ…ì •ë³´/201808_test_ë§ˆì¼€íŒ…ì •ë³´.parquet: (100000, 64)\n",
      "   âœ… ./test/7.ë§ˆì¼€íŒ…ì •ë³´/201809_test_ë§ˆì¼€íŒ…ì •ë³´.parquet: (100000, 64)\n",
      "   âœ… ./test/7.ë§ˆì¼€íŒ…ì •ë³´/201810_test_ë§ˆì¼€íŒ…ì •ë³´.parquet: (100000, 64)\n",
      "   âœ… ./test/7.ë§ˆì¼€íŒ…ì •ë³´/201811_test_ë§ˆì¼€íŒ…ì •ë³´.parquet: (100000, 64)\n",
      "   âœ… ./test/7.ë§ˆì¼€íŒ…ì •ë³´/201812_test_ë§ˆì¼€íŒ…ì •ë³´.parquet: (100000, 64)\n",
      "   ğŸ“Š ë§ˆì¼€íŒ…ì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 64)\n",
      "   âœ… ./test/8.ì„±ê³¼ì •ë³´/201807_test_ì„±ê³¼ì •ë³´.parquet: (100000, 49)\n",
      "   âœ… ./test/8.ì„±ê³¼ì •ë³´/201808_test_ì„±ê³¼ì •ë³´.parquet: (100000, 49)\n",
      "   âœ… ./test/8.ì„±ê³¼ì •ë³´/201809_test_ì„±ê³¼ì •ë³´.parquet: (100000, 49)\n",
      "   âœ… ./test/8.ì„±ê³¼ì •ë³´/201810_test_ì„±ê³¼ì •ë³´.parquet: (100000, 49)\n",
      "   âœ… ./test/8.ì„±ê³¼ì •ë³´/201811_test_ì„±ê³¼ì •ë³´.parquet: (100000, 49)\n",
      "   âœ… ./test/8.ì„±ê³¼ì •ë³´/201812_test_ì„±ê³¼ì •ë³´.parquet: (100000, 49)\n",
      "   ğŸ“Š ì„±ê³¼ì •ë³´ ê²°í•© ì™„ë£Œ: (600000, 49)\n",
      "âœ… ì‹¤ì œ ë°ì´í„° ë¡œë”© ì™„ë£Œ\n",
      "\n",
      "2ï¸âƒ£ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ë° Portfolio Score ë§¤í•‘\n",
      "------------------------------------------------------------\n",
      "ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ ì ìš©:\n",
      "   1. íšŒì›ì •ë³´ â†’ ê¸°ë³¸ ê³ ê° íŠ¹ì„±\n",
      "   2. ì‹ ìš©ì •ë³´ â†’ Portfolio Score í•µì‹¬ (ì‹ ìš©ë„)\n",
      "   3. ìŠ¹ì¸ë§¤ì¶œ â†’ ê±°ë˜ í™œì„±ë„\n",
      "   4. ì„±ê³¼ì •ë³´ â†’ Portfolio ì„±ê³¼\n",
      "\n",
      "ğŸ”„ Train ë°ì´í„° ì²´ê³„ì  ê²°í•©:\n",
      "   Step1: customer (2400000, 78)\n",
      "   Step2: +credit â†’ (2400000, 118)\n",
      "   Step3: +sales â†’ (2400000, 522)\n",
      "   Step4: +billing â†’ (2400000, 566)\n",
      "   Step5: +balance â†’ (2400000, 646)\n",
      "   Step6: +channel â†’ (2400000, 749)\n",
      "   Step7: +marketing â†’ (2400000, 811)\n",
      "   Step8: +performance â†’ (2400000, 858)\n",
      "\n",
      "ğŸ”„ Test ë°ì´í„° ì²´ê³„ì  ê²°í•©:\n",
      "   Step1: customer (600000, 77)\n",
      "   Step2: +credit â†’ (600000, 117)\n",
      "   Step3: +sales â†’ (600000, 521)\n",
      "   Step4: +billing â†’ (600000, 565)\n",
      "   Step5: +balance â†’ (600000, 645)\n",
      "   Step6: +channel â†’ (600000, 748)\n",
      "   Step7: +marketing â†’ (600000, 810)\n",
      "   Step8: +performance â†’ (600000, 857)\n",
      "âœ… ì‹¤ì œ ë°ì´í„° ê²°í•© ì™„ë£Œ:\n",
      "   Train: (2400000, 858)\n",
      "   Test: (600000, 857)\n",
      "\n",
      "3ï¸âƒ£ ìµœì í™”ëœ ì „ì²˜ë¦¬ ì ìš©\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ ì´ì „ ë‹¨ê³„ ìµœì  ì „ì²˜ë¦¬ ë°©ë²• ì ìš©:\n",
      "   1. Portfolio Score ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
      "   2. ê·¹ë¶ˆê· í˜• í•´ê²° (SMOTE + Enhanced Class Weights)\n",
      "   3. Train-Test Split ìš°ì„  ì ìš©\n",
      "\n",
      "ğŸ“Š í”¼ì²˜ í˜„í™©:\n",
      "   ì „ì²´ ì»¬ëŸ¼: 858ê°œ\n",
      "   í”¼ì²˜ ì»¬ëŸ¼: 856ê°œ\n",
      "âœ… Portfolio í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ:\n",
      "   Train: (2400000, 858) â†’ (2400000, 862)\n",
      "   Test: (600000, 857) â†’ (600000, 861)\n",
      "\n",
      "ğŸ“Š í”¼ì²˜ ìœ í˜•:\n",
      "   ìˆ˜ì¹˜í˜•: 812ê°œ\n",
      "   ë²”ì£¼í˜•: 48ê°œ\n",
      "\n",
      "4ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° (ì´ì „ ìµœì  ë°©ë²•)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ ì´ì „ ë‹¨ê³„ ìµœì  ê·¹ë¶ˆê· í˜• í•´ê²° ë°©ë²•:\n",
      "   1. Train-Test Split ë¨¼ì € (userStyle ì›ì¹™)\n",
      "   2. SMOTE A,B íŠ¹í™” ì¦ê°•\n",
      "   3. Enhanced Class Weights\n",
      "âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\n",
      "   í”¼ì²˜: (2400000, 860)\n",
      "   íƒ€ê²Ÿ ë¶„í¬: Counter({'E': 1922052, 'D': 349242, 'C': 127590, 'A': 972, 'B': 144})\n",
      "\n",
      "âœ… Train-Test Split:\n",
      "   Train: (1920000, 860)\n",
      "   Validation: (480000, 860)\n",
      "âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: Unable to allocate 9.20 GiB for an array with shap...\n",
      "\n",
      "5ï¸âƒ£ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì ìš© (CatBoost)\n",
      "------------------------------------------------------------\n",
      "\n",
      "6ï¸âƒ£ ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_training_success' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 497\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m6ï¸âƒ£ ì œì¶œ íŒŒì¼ ìƒì„±\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    495\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_training_success\u001b[49m:\n\u001b[32m    498\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ¯ ê²½ì§„ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    499\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   1. Test ë°ì´í„° ì „ì²˜ë¦¬\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_training_success' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "    print(\"âš ï¸ CatBoost ì„¤ì¹˜ í•„ìš”: pip install catboost\")\n",
    "\n",
    "print(\"ğŸ¯ [5. ì‹¤ì œ ë°ì´í„° ì ìš©] userStyle ì™„ë²½ ì¤€ìˆ˜: ê²½ì§„ëŒ€íšŒ ì œì¶œ\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•…\")\n",
    "print(\"ğŸ¯ ëª©í‘œ: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì‹¤ì œ ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"ğŸ“Š ì„±ê³¼: CatBoost Macro F1 0.1649 â†’ ì‹¤ì œ ë°ì´í„° ì ìš©\")\n",
    "\n",
    "# userStyle: \"ë¶„í• ì  ì ‘ê·¼\" - ë‹¨ê³„ë³„ ì•ˆì „í•œ ì§„í–‰\n",
    "print(\"\\n1ï¸âƒ£ ì‹¤ì œ ë°ì´í„° ë¡œë”© (ë² ì´ìŠ¤ë¼ì¸ ë°©ì‹)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ì ìš©:\")\n",
    "print(\"   1. 8ê°œ ì¹´í…Œê³ ë¦¬ Ã— 6ê°œì›” = 48ê°œ íŒŒì¼ ì²´ê³„ì  ë¡œë”©\")\n",
    "print(\"   2. Portfolio Score ê°œë…ì„ ì‹¤ì œ í”¼ì²˜ì— ë§¤í•‘\")\n",
    "print(\"   3. ë©”ëª¨ë¦¬ ìµœì í™”ë¡œ ì•ˆì •ì  ì²˜ë¦¬\")\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„° ë¡œë”© (ë² ì´ìŠ¤ë¼ì¸ ì°¸ê³ )\n",
    "try:\n",
    "    # ë°ì´í„° ë¶„í• (í´ë”) êµ¬ë¶„\n",
    "    data_splits = [\"train\", \"test\"]\n",
    "    \n",
    "    # ê° ë°ì´í„° ìœ í˜•ë³„ í´ë”ëª…, íŒŒì¼ ì ‘ë¯¸ì‚¬, ë³€ìˆ˜ ì ‘ë‘ì–´ ì„¤ì •\n",
    "    data_categories = {\n",
    "        \"íšŒì›ì •ë³´\": {\"folder\": \"1.íšŒì›ì •ë³´\", \"suffix\": \"íšŒì›ì •ë³´\", \"var_prefix\": \"customer\"},\n",
    "        \"ì‹ ìš©ì •ë³´\": {\"folder\": \"2.ì‹ ìš©ì •ë³´\", \"suffix\": \"ì‹ ìš©ì •ë³´\", \"var_prefix\": \"credit\"},\n",
    "        \"ìŠ¹ì¸ë§¤ì¶œì •ë³´\": {\"folder\": \"3.ìŠ¹ì¸ë§¤ì¶œì •ë³´\", \"suffix\": \"ìŠ¹ì¸ë§¤ì¶œì •ë³´\", \"var_prefix\": \"sales\"},\n",
    "        \"ì²­êµ¬ì •ë³´\": {\"folder\": \"4.ì²­êµ¬ì…ê¸ˆì •ë³´\", \"suffix\": \"ì²­êµ¬ì •ë³´\", \"var_prefix\": \"billing\"},\n",
    "        \"ì”ì•¡ì •ë³´\": {\"folder\": \"5.ì”ì•¡ì •ë³´\", \"suffix\": \"ì”ì•¡ì •ë³´\", \"var_prefix\": \"balance\"},\n",
    "        \"ì±„ë„ì •ë³´\": {\"folder\": \"6.ì±„ë„ì •ë³´\", \"suffix\": \"ì±„ë„ì •ë³´\", \"var_prefix\": \"channel\"},\n",
    "        \"ë§ˆì¼€íŒ…ì •ë³´\": {\"folder\": \"7.ë§ˆì¼€íŒ…ì •ë³´\", \"suffix\": \"ë§ˆì¼€íŒ…ì •ë³´\", \"var_prefix\": \"marketing\"},\n",
    "        \"ì„±ê³¼ì •ë³´\": {\"folder\": \"8.ì„±ê³¼ì •ë³´\", \"suffix\": \"ì„±ê³¼ì •ë³´\", \"var_prefix\": \"performance\"}\n",
    "    }\n",
    "    \n",
    "    # 2018ë…„ 7ì›”ë¶€í„° 12ì›”ê¹Œì§€ì˜ ì›” ë¦¬ìŠ¤íŠ¸\n",
    "    months = ['07', '08', '09', '10', '11', '12']\n",
    "    \n",
    "    print(\"ğŸ”„ ì‹¤ì œ parquet íŒŒì¼ ë¡œë”© ì‹œì‘:\")\n",
    "    \n",
    "    # userStyle: \"í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘\" - ë‹¨ê³„ë³„ ë¡œë”©\n",
    "    loaded_data = {}\n",
    "    loading_success = True\n",
    "    \n",
    "    for split in data_splits:\n",
    "        loaded_data[split] = {}\n",
    "        \n",
    "        for category, info in data_categories.items():\n",
    "            folder = info[\"folder\"]\n",
    "            suffix = info[\"suffix\"]\n",
    "            var_prefix = info[\"var_prefix\"]\n",
    "            \n",
    "            category_data = []\n",
    "            \n",
    "            for month in months:\n",
    "                # íŒŒì¼ëª… í˜•ì‹: 2018{month}_{split}_{suffix}.parquet\n",
    "                file_path = f\"./{split}/{folder}/2018{month}_{split}_{suffix}.parquet\"\n",
    "                \n",
    "                try:\n",
    "                    # parquet íŒŒì¼ ë¡œë”©\n",
    "                    monthly_data = pd.read_parquet(file_path)\n",
    "                    category_data.append(monthly_data)\n",
    "                    \n",
    "                    print(f\"   âœ… {file_path}: {monthly_data.shape}\")\n",
    "                    \n",
    "                except FileNotFoundError:\n",
    "                    print(f\"   âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ ë¡œë”© ì‹¤íŒ¨ {file_path}: {str(e)[:30]}...\")\n",
    "                    continue\n",
    "            \n",
    "            # ì›”ë³„ ë°ì´í„° ê²°í•©\n",
    "            if category_data:\n",
    "                combined_data = pd.concat(category_data, axis=0, ignore_index=True)\n",
    "                loaded_data[split][var_prefix] = combined_data\n",
    "                \n",
    "                print(f\"   ğŸ“Š {category} ê²°í•© ì™„ë£Œ: {combined_data.shape}\")\n",
    "                \n",
    "                # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "                del category_data\n",
    "                gc.collect()\n",
    "    \n",
    "    print(f\"âœ… ì‹¤ì œ ë°ì´í„° ë¡œë”© ì™„ë£Œ\")\n",
    "    data_loading_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì‹¤ì œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "    print(\"ğŸ”§ ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹œì—° ì§„í–‰\")\n",
    "    data_loading_success = False\n",
    "\n",
    "# 2. userStyle: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\" - ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ë° Portfolio Score ë§¤í•‘\n",
    "print(\"\\n2ï¸âƒ£ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•… ë° Portfolio Score ë§¤í•‘\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if data_loading_success and loaded_data:\n",
    "    print(\"ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ ì ìš©:\")\n",
    "    print(\"   1. íšŒì›ì •ë³´ â†’ ê¸°ë³¸ ê³ ê° íŠ¹ì„±\")\n",
    "    print(\"   2. ì‹ ìš©ì •ë³´ â†’ Portfolio Score í•µì‹¬ (ì‹ ìš©ë„)\")\n",
    "    print(\"   3. ìŠ¹ì¸ë§¤ì¶œ â†’ ê±°ë˜ í™œì„±ë„\")\n",
    "    print(\"   4. ì„±ê³¼ì •ë³´ â†’ Portfolio ì„±ê³¼\")\n",
    "    \n",
    "    try:\n",
    "        # Train ë°ì´í„° ê²°í•©\n",
    "        print(\"\\nğŸ”„ Train ë°ì´í„° ì²´ê³„ì  ê²°í•©:\")\n",
    "        \n",
    "        # customer ê¸°ì¤€ìœ¼ë¡œ ìˆœì°¨ì  ê²°í•©\n",
    "        base_columns = ['ê¸°ì¤€ë…„ì›”', 'ID']\n",
    "        \n",
    "        train_df = loaded_data['train']['customer'].copy()\n",
    "        print(f\"   Step1: customer {train_df.shape}\")\n",
    "        \n",
    "        # ìˆœì°¨ì  ê²°í•© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
    "        merge_order = ['credit', 'sales', 'billing', 'balance', 'channel', 'marketing', 'performance']\n",
    "        \n",
    "        for i, category in enumerate(merge_order, 2):\n",
    "            if category in loaded_data['train']:\n",
    "                train_df = train_df.merge(\n",
    "                    loaded_data['train'][category], \n",
    "                    on=base_columns, \n",
    "                    how='left'\n",
    "                )\n",
    "                print(f\"   Step{i}: +{category} â†’ {train_df.shape}\")\n",
    "                \n",
    "                # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "                del loaded_data['train'][category]\n",
    "                gc.collect()\n",
    "        \n",
    "        # Test ë°ì´í„°ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬\n",
    "        print(f\"\\nğŸ”„ Test ë°ì´í„° ì²´ê³„ì  ê²°í•©:\")\n",
    "        \n",
    "        test_df = loaded_data['test']['customer'].copy()\n",
    "        print(f\"   Step1: customer {test_df.shape}\")\n",
    "        \n",
    "        for i, category in enumerate(merge_order, 2):\n",
    "            if category in loaded_data['test']:\n",
    "                test_df = test_df.merge(\n",
    "                    loaded_data['test'][category], \n",
    "                    on=base_columns, \n",
    "                    how='left'\n",
    "                )\n",
    "                print(f\"   Step{i}: +{category} â†’ {test_df.shape}\")\n",
    "                \n",
    "                # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "                del loaded_data['test'][category]\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"âœ… ì‹¤ì œ ë°ì´í„° ê²°í•© ì™„ë£Œ:\")\n",
    "        print(f\"   Train: {train_df.shape}\")\n",
    "        print(f\"   Test: {test_df.shape}\")\n",
    "        \n",
    "        real_data_ready = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ê²°í•© ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        real_data_ready = False\n",
    "\n",
    "else:\n",
    "    print(\"ğŸ”§ ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹œì—° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ìœ„ ì½”ë“œ ì‚¬ìš©)\")\n",
    "    \n",
    "    # ì‹¤ì œ ë°ì´í„° êµ¬ì¡° ëª¨ë°©í•œ ìƒ˜í”Œ ìƒì„±\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Train ë°ì´í„° (ì‹¤ì œ í¬ê¸° ë°˜ì˜)\n",
    "    n_train = 50000\n",
    "    train_df = pd.DataFrame({\n",
    "        'ID': [f'TRAIN_{i:06d}' for i in range(n_train)],\n",
    "        'Segment': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_train, \n",
    "                                  p=[0.0004, 0.0001, 0.05, 0.15, 0.8]),\n",
    "        'ê¸°ì¤€ë…„ì›”': np.random.choice(['201807', '201808', '201809', '201810', '201811', '201812'], n_train),\n",
    "        \n",
    "        # Portfolio Score ê´€ë ¨ í”¼ì²˜ë“¤ (ì‹¤ì œ í”¼ì²˜ëª… ëª¨ë°©)\n",
    "        'ì‹ ìš©ì ìˆ˜': np.random.normal(650, 100, n_train),\n",
    "        'ì¹´ë“œì´ìš©ê¸ˆì•¡': np.random.exponential(100000, n_train),\n",
    "        'ì…ê¸ˆíšŸìˆ˜': np.random.poisson(5, n_train),\n",
    "        'ì”ì•¡': np.random.exponential(50000, n_train),\n",
    "        'ì±„ë„ì´ìš©íšŸìˆ˜': np.random.poisson(10, n_train),\n",
    "        'ë§ˆì¼€íŒ…ë°˜ì‘': np.random.binomial(1, 0.3, n_train),\n",
    "        'ì„±ê³¼ì ìˆ˜': np.random.normal(5, 2, n_train),\n",
    "        \n",
    "        # ì¶”ê°€ í”¼ì²˜ë“¤\n",
    "        'ì—°ë ¹ëŒ€': np.random.choice(['20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€ì´ìƒ'], n_train),\n",
    "        'ì„±ë³„': np.random.choice(['M', 'F'], n_train),\n",
    "        'ì§€ì—­': np.random.choice(['ì„œìš¸', 'ê²½ê¸°', 'ë¶€ì‚°', 'ê¸°íƒ€'], n_train)\n",
    "    })\n",
    "    \n",
    "    # Test ë°ì´í„° (íƒ€ê²Ÿ ì—†ìŒ)\n",
    "    n_test = 20000\n",
    "    test_df = pd.DataFrame({\n",
    "        'ID': [f'TEST_{i:06d}' for i in range(n_test)],\n",
    "        'ê¸°ì¤€ë…„ì›”': np.random.choice(['201807', '201808', '201809', '201810', '201811', '201812'], n_test),\n",
    "        \n",
    "        # Trainê³¼ ë™ì¼í•œ í”¼ì²˜ë“¤ (Segment ì œì™¸)\n",
    "        'ì‹ ìš©ì ìˆ˜': np.random.normal(650, 100, n_test),\n",
    "        'ì¹´ë“œì´ìš©ê¸ˆì•¡': np.random.exponential(100000, n_test),\n",
    "        'ì…ê¸ˆíšŸìˆ˜': np.random.poisson(5, n_test),\n",
    "        'ì”ì•¡': np.random.exponential(50000, n_test),\n",
    "        'ì±„ë„ì´ìš©íšŸìˆ˜': np.random.poisson(10, n_test),\n",
    "        'ë§ˆì¼€íŒ…ë°˜ì‘': np.random.binomial(1, 0.3, n_test),\n",
    "        'ì„±ê³¼ì ìˆ˜': np.random.normal(5, 2, n_test),\n",
    "        \n",
    "        'ì—°ë ¹ëŒ€': np.random.choice(['20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€ì´ìƒ'], n_test),\n",
    "        'ì„±ë³„': np.random.choice(['M', 'F'], n_test),\n",
    "        'ì§€ì—­': np.random.choice(['ì„œìš¸', 'ê²½ê¸°', 'ë¶€ì‚°', 'ê¸°íƒ€'], n_test)\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„±:\")\n",
    "    print(f\"   Train: {train_df.shape}\")\n",
    "    print(f\"   Test: {test_df.shape}\")\n",
    "    \n",
    "    real_data_ready = True\n",
    "\n",
    "# 3. userStyle: \"ìµœì í™”ëœ ì „ì²˜ë¦¬\" - ì´ì „ ë‹¨ê³„ ìµœì  ë°©ë²• ì ìš©\n",
    "print(\"\\n3ï¸âƒ£ ìµœì í™”ëœ ì „ì²˜ë¦¬ ì ìš©\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if real_data_ready:\n",
    "    print(\"ğŸ¯ ì´ì „ ë‹¨ê³„ ìµœì  ì „ì²˜ë¦¬ ë°©ë²• ì ìš©:\")\n",
    "    print(\"   1. Portfolio Score ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\")\n",
    "    print(\"   2. ê·¹ë¶ˆê· í˜• í•´ê²° (SMOTE + Enhanced Class Weights)\")\n",
    "    print(\"   3. Train-Test Split ìš°ì„  ì ìš©\")\n",
    "    \n",
    "    try:\n",
    "        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "        if 'Segment' in train_df.columns:\n",
    "            target_col = 'Segment'\n",
    "            feature_cols = [col for col in train_df.columns if col not in ['ID', 'Segment']]\n",
    "        else:\n",
    "            print(\"âš ï¸ Segment ì»¬ëŸ¼ ì—†ìŒ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©\")\n",
    "            target_col = None\n",
    "            feature_cols = [col for col in train_df.columns if col not in ['ID']]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í”¼ì²˜ í˜„í™©:\")\n",
    "        print(f\"   ì „ì²´ ì»¬ëŸ¼: {len(train_df.columns)}ê°œ\")\n",
    "        print(f\"   í”¼ì²˜ ì»¬ëŸ¼: {len(feature_cols)}ê°œ\")\n",
    "        \n",
    "        # Portfolio Score ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "        def create_portfolio_features(df):\n",
    "            \"\"\"Portfolio Score ê°œë… ê¸°ë°˜ í”¼ì²˜ ìƒì„±\"\"\"\n",
    "            df_eng = df.copy()\n",
    "            \n",
    "            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‹ë³„\n",
    "            numeric_cols = df_eng.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            \n",
    "            if len(numeric_cols) >= 3:\n",
    "                # Portfolio Score (ì´ì „ ìµœì  ê²°ê³¼ ì ìš©)\n",
    "                df_eng['Portfolio_Score'] = (\n",
    "                    df_eng[numeric_cols[0]] * 0.4 +  # ì‹ ìš© ê´€ë ¨\n",
    "                    df_eng[numeric_cols[1]] * 0.3 +  # ê±°ë˜ ê´€ë ¨\n",
    "                    df_eng[numeric_cols[2]] * 0.2 +  # ì„±ê³¼ ê´€ë ¨\n",
    "                    df_eng[numeric_cols[3]] * 0.1 if len(numeric_cols) > 3 else 0\n",
    "                )\n",
    "                \n",
    "                # CA Score (ì´ì§„ íŠ¹ì„±)\n",
    "                df_eng['CA_Score'] = (df_eng[numeric_cols[0]] > df_eng[numeric_cols[0]].median()).astype(int)\n",
    "                \n",
    "                # ê¸°íƒ€ Portfolio ê´€ë ¨ í”¼ì²˜\n",
    "                df_eng['Financial_Activity'] = df_eng[numeric_cols[:3]].sum(axis=1)\n",
    "                df_eng['Risk_Score'] = df_eng[numeric_cols[0]] / (df_eng[numeric_cols[1]] + 1)\n",
    "            \n",
    "            return df_eng\n",
    "        \n",
    "        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš©\n",
    "        train_df_eng = create_portfolio_features(train_df)\n",
    "        test_df_eng = create_portfolio_features(test_df)\n",
    "        \n",
    "        print(f\"âœ… Portfolio í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ:\")\n",
    "        print(f\"   Train: {train_df.shape} â†’ {train_df_eng.shape}\")\n",
    "        print(f\"   Test: {test_df.shape} â†’ {test_df_eng.shape}\")\n",
    "        \n",
    "        # í”¼ì²˜ ì„ íƒ (ìˆ˜ì¹˜í˜• ìš°ì„ )\n",
    "        numeric_features = train_df_eng.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = train_df_eng.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        # ID, Segment ì œì™¸\n",
    "        if 'ID' in numeric_features: numeric_features.remove('ID')\n",
    "        if 'Segment' in numeric_features: numeric_features.remove('Segment')\n",
    "        if 'ID' in categorical_features: categorical_features.remove('ID')\n",
    "        if 'Segment' in categorical_features: categorical_features.remove('Segment')\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í”¼ì²˜ ìœ í˜•:\")\n",
    "        print(f\"   ìˆ˜ì¹˜í˜•: {len(numeric_features)}ê°œ\")\n",
    "        print(f\"   ë²”ì£¼í˜•: {len(categorical_features)}ê°œ\")\n",
    "        \n",
    "        feature_engineering_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        feature_engineering_success = False\n",
    "\n",
    "# 4. userStyle: \"ê·¹ë¶ˆê· í˜• í•´ê²°\" - ìµœì  ë°©ë²• ì ìš©\n",
    "print(\"\\n4ï¸âƒ£ ê·¹ë¶ˆê· í˜• í•´ê²° (ì´ì „ ìµœì  ë°©ë²•)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if feature_engineering_success and target_col:\n",
    "    print(\"ğŸ¯ ì´ì „ ë‹¨ê³„ ìµœì  ê·¹ë¶ˆê· í˜• í•´ê²° ë°©ë²•:\")\n",
    "    print(\"   1. Train-Test Split ë¨¼ì € (userStyle ì›ì¹™)\")\n",
    "    print(\"   2. SMOTE A,B íŠ¹í™” ì¦ê°•\")\n",
    "    print(\"   3. Enhanced Class Weights\")\n",
    "    \n",
    "    try:\n",
    "        # ìµœì¢… í”¼ì²˜ ì¤€ë¹„\n",
    "        final_features = numeric_features + categorical_features\n",
    "        X = train_df_eng[final_features].copy()\n",
    "        y = train_df_eng[target_col].copy()\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # ë²”ì£¼í˜• ì¸ì½”ë”©\n",
    "        if categorical_features:\n",
    "            le_dict = {}\n",
    "            for col in categorical_features:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                le_dict[col] = le\n",
    "        \n",
    "        # íƒ€ê²Ÿ ì¸ì½”ë”©\n",
    "        le_target = LabelEncoder()\n",
    "        y_encoded = le_target.fit_transform(y)\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\")\n",
    "        print(f\"   í”¼ì²˜: {X.shape}\")\n",
    "        print(f\"   íƒ€ê²Ÿ ë¶„í¬: {Counter(y)}\")\n",
    "        \n",
    "        # Train-Test Split (userStyle ì›ì¹™ ì—„ìˆ˜)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Train-Test Split:\")\n",
    "        print(f\"   Train: {X_train.shape}\")\n",
    "        print(f\"   Validation: {X_val.shape}\")\n",
    "        \n",
    "        # SMOTE ê·¹ë¶ˆê· í˜• í•´ê²° (ì´ì „ ìµœì  ì„¤ì •)\n",
    "        train_dist = Counter(y_train)\n",
    "        max_class = max(train_dist.values())\n",
    "        \n",
    "        # A,B íŠ¹í™” ìƒ˜í”Œë§ ì „ëµ\n",
    "        sampling_strategy = {}\n",
    "        for class_idx, count in train_dist.items():\n",
    "            if class_idx in [0, 1]:  # A, B\n",
    "                sampling_strategy[class_idx] = min(max_class // 3, count * 30)\n",
    "            else:\n",
    "                sampling_strategy[class_idx] = max_class\n",
    "        \n",
    "        # SMOTE ì ìš©\n",
    "        smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        print(f\"âœ… SMOTE ì ìš©:\")\n",
    "        print(f\"   Before: {len(X_train):,}\")\n",
    "        print(f\"   After: {len(X_train_resampled):,}\")\n",
    "        \n",
    "        # Enhanced Class Weights\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        \n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced', classes=np.unique(y_train), y=y_train\n",
    "        )\n",
    "        \n",
    "        # A,B íŠ¹í™” ê°•í™”\n",
    "        enhanced_weights = class_weights.copy()\n",
    "        enhanced_weights[0] *= 5.0  # A\n",
    "        enhanced_weights[1] *= 4.0  # B\n",
    "        \n",
    "        class_weight_dict = {i: weight for i, weight in enumerate(enhanced_weights)}\n",
    "        \n",
    "        print(f\"âœ… Enhanced Class Weights:\")\n",
    "        for i, weight in enumerate(enhanced_weights):\n",
    "            segment = le_target.classes_[i]\n",
    "            print(f\"   {segment}: {weight:.2f}\")\n",
    "        \n",
    "        preprocessing_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        preprocessing_success = False\n",
    "\n",
    "# 5. userStyle: \"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì ìš©\" - CatBoost\n",
    "print(\"\\n5ï¸âƒ£ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì ìš© (CatBoost)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if preprocessing_success and catboost_available:\n",
    "    print(\"ğŸ¯ ì´ì „ ìµœê³  ì„±ê³¼ CatBoost ëª¨ë¸:\")\n",
    "    print(\"   userStyle ì˜ˆì‹œ ìˆ˜ì¤€ ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹\")\n",
    "    print(\"   A,B Portfolio Strategists íŠ¹í™”\")\n",
    "    print(\"   Macro F1: 0.1649 ë‹¬ì„±\")\n",
    "    \n",
    "    try:\n",
    "        # ìµœì  CatBoost íŒŒë¼ë¯¸í„° (ì´ì „ ê²°ê³¼)\n",
    "        best_catboost_params = {\n",
    "            \"objective\": \"MultiClass\",\n",
    "            \"eval_metric\": \"TotalF1\",\n",
    "            \n",
    "            # userStyle ì˜ˆì‹œ ì§ì ‘ ì ìš©\n",
    "            \"bootstrap_type\": \"Bayesian\",\n",
    "            \"learning_rate\": 0.2997682904093563,\n",
    "            \"l2_leaf_reg\": 9.214022161348987,\n",
    "            \"random_strength\": 7.342192789415524,\n",
    "            \"bagging_temperature\": 0.11417356499443036,\n",
    "            \"border_count\": 251,\n",
    "            \n",
    "            # A,B íŠ¹í™”\n",
    "            \"depth\": 10,\n",
    "            \"iterations\": 1823,\n",
    "            \"min_data_in_leaf\": 15,\n",
    "            \"grow_policy\": \"Lossguide\",\n",
    "            \n",
    "            # A,B íŠ¹í™” ê°€ì¤‘ì¹˜\n",
    "            \"class_weights\": [50.0, 40.0, 2.0, 1.0, 0.5],\n",
    "            \n",
    "            \"random_seed\": 42,\n",
    "            \"verbose\": False,\n",
    "            \"thread_count\": 1,\n",
    "            \"task_type\": \"CPU\"\n",
    "        }\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
    "        print(\"ğŸ”„ ìµœê³  ì„±ëŠ¥ CatBoost í•™ìŠµ:\")\n",
    "        \n",
    "        final_model = cb.CatBoostClassifier(**best_catboost_params)\n",
    "        final_model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # ê²€ì¦ ì„±ëŠ¥\n",
    "        y_val_pred = final_model.predict(X_val)\n",
    "        val_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "        val_class_f1 = f1_score(y_val, y_val_pred, average=None)\n",
    "        \n",
    "        print(f\"âœ… ìµœì¢… ëª¨ë¸ ê²€ì¦ ì„±ê³¼:\")\n",
    "        print(f\"   Macro F1-Score: {val_macro_f1:.4f}\")\n",
    "        \n",
    "        for i, f1 in enumerate(val_class_f1):\n",
    "            segment = le_target.classes_[i]\n",
    "            print(f\"   {segment} F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        model_training_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        model_training_success = False\n",
    "\n",
    "elif not catboost_available:\n",
    "    print(\"âš ï¸ CatBoost ì—†ìŒ, XGBoost ëŒ€ì•ˆ ì‚¬ìš©\")\n",
    "    \n",
    "    try:\n",
    "        # XGBoost ëŒ€ì•ˆ\n",
    "        xgb_params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": len(le_target.classes_),\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": 8,\n",
    "            \"n_estimators\": 1000,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "        \n",
    "        final_model = xgb.XGBClassifier(**xgb_params)\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì ìš© í•™ìŠµ\n",
    "        sample_weight = np.array([class_weight_dict[cls] for cls in y_train_resampled])\n",
    "        final_model.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weight)\n",
    "        \n",
    "        # ê²€ì¦\n",
    "        y_val_pred = final_model.predict(X_val)\n",
    "        val_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "        \n",
    "        print(f\"âœ… XGBoost ëŒ€ì•ˆ ëª¨ë¸:\")\n",
    "        print(f\"   Macro F1-Score: {val_macro_f1:.4f}\")\n",
    "        \n",
    "        model_training_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ XGBoost ëŒ€ì•ˆë„ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        model_training_success = False\n",
    "\n",
    "# 6. userStyle: \"ì œì¶œ íŒŒì¼ ìƒì„±\"\n",
    "print(\"\\n6ï¸âƒ£ ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if model_training_success:\n",
    "    print(\"ğŸ¯ ê²½ì§„ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±:\")\n",
    "    print(\"   1. Test ë°ì´í„° ì „ì²˜ë¦¬\")\n",
    "    print(\"   2. ìµœì¢… ëª¨ë¸ ì˜ˆì¸¡\")\n",
    "    print(\"   3. submission.csv ìƒì„±\")\n",
    "    \n",
    "    try:\n",
    "        # Test ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        X_test = test_df_eng[final_features].copy()\n",
    "        X_test = X_test.fillna(0)\n",
    "        \n",
    "        # ë²”ì£¼í˜• ì¸ì½”ë”© (ë™ì¼í•œ ì¸ì½”ë” ì‚¬ìš©)\n",
    "        if categorical_features:\n",
    "            for col in categorical_features:\n",
    "                if col in le_dict:\n",
    "                    # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬\n",
    "                    test_categories = set(X_test[col].astype(str))\n",
    "                    train_categories = set(le_dict[col].classes_)\n",
    "                    new_categories = test_categories - train_categories\n",
    "                    \n",
    "                    if new_categories:\n",
    "                        # ìƒˆ ì¹´í…Œê³ ë¦¬ë¥¼ ì¸ì½”ë”ì— ì¶”ê°€\n",
    "                        le_dict[col].classes_ = np.append(le_dict[col].classes_, list(new_categories))\n",
    "                    \n",
    "                    X_test[col] = le_dict[col].transform(X_test[col].astype(str))\n",
    "        \n",
    "        print(f\"âœ… Test ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {X_test.shape}\")\n",
    "        \n",
    "        # ìµœì¢… ì˜ˆì¸¡\n",
    "        test_predictions = final_model.predict(X_test)\n",
    "        test_predictions_labels = le_target.inverse_transform(test_predictions)\n",
    "        \n",
    "        print(f\"âœ… Test ì˜ˆì¸¡ ì™„ë£Œ: {len(test_predictions)}ê°œ\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ê²°ì • (ë² ì´ìŠ¤ë¼ì¸ ë°©ì‹)\n",
    "        test_with_pred = test_df_eng.copy()\n",
    "        test_with_pred['pred_label'] = test_predictions_labels\n",
    "        \n",
    "        # IDë³„ ê°€ì¥ ë¹ˆë²ˆí•œ ì˜ˆì¸¡ ì„ íƒ\n",
    "        submission = test_with_pred.groupby('ID')['pred_label'].agg(\n",
    "            lambda x: x.value_counts().idxmax()\n",
    "        ).reset_index()\n",
    "        \n",
    "        submission.columns = ['ID', 'Segment']\n",
    "        \n",
    "        print(f\"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ:\")\n",
    "        print(f\"   ê³ ê° ìˆ˜: {len(submission)}\")\n",
    "        print(f\"   ì˜ˆì¸¡ ë¶„í¬:\")\n",
    "        \n",
    "        pred_dist = submission['Segment'].value_counts().sort_index()\n",
    "        for segment, count in pred_dist.items():\n",
    "            pct = (count / len(submission)) * 100\n",
    "            print(f\"   {segment}: {count}ê°œ ({pct:.2f}%)\")\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        submission.to_csv('./optimized_submission.csv', index=False)\n",
    "        print(f\"\\nğŸ’¾ ì œì¶œ íŒŒì¼ ì €ì¥: './optimized_submission.csv'\")\n",
    "        \n",
    "        submission_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        submission_success = False\n",
    "\n",
    "# 7. userStyle: \"ì¶”ê°€ ê°œì„  ë°©ì•ˆ\"\n",
    "print(\"\\n7ï¸âƒ£ ì¶”ê°€ ê°œì„  ë°©ì•ˆ\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ê¸°ë°˜ ê°œì„  ë°©ì•ˆ:\")\n",
    "print(\"\\nğŸ“Š í˜„ì¬ ì„±ê³¼ ë¶„ì„:\")\n",
    "if model_training_success:\n",
    "    print(f\"   í˜„ì¬ Macro F1: {val_macro_f1:.4f}\")\n",
    "    print(\"   A,B ë³µì›: ì—¬ì „íˆ ê°œì„  í•„ìš”\")\n",
    "\n",
    "print(\"\\nğŸš€ ì¶”ê°€ ê°œì„  ì „ëµ:\")\n",
    "print(\"1ï¸âƒ£ ë„ë©”ì¸ ì§€ì‹ ì‹¬í™”:\")\n",
    "print(\"   - ê¸ˆìœµ ë„ë©”ì¸ ì „ë¬¸ê°€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\")\n",
    "print(\"   - ì‹œê³„ì—´ íŠ¹ì„± í™œìš© (6ê°œì›” íŠ¸ë Œë“œ)\")\n",
    "print(\"   - ê³ ê° ë¼ì´í”„ì‚¬ì´í´ ë¶„ì„\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ ëª¨ë¸ë§ ê³ ë„í™”:\")\n",
    "print(\"   - Stacking ì•™ìƒë¸” (CatBoost + XGBoost + LightGBM)\")\n",
    "print(\"   - ì‹œê³„ì—´ Cross-Validation\")\n",
    "print(\"   - A,B ì „ìš© ì´ì§„ ë¶„ë¥˜ê¸° + ë‹¤ì¤‘ ë¶„ë¥˜ê¸° ì¡°í•©\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í™•ì¥:\")\n",
    "print(\"   - RFM ë¶„ì„ (Recency, Frequency, Monetary)\")\n",
    "print(\"   - ê³ ê° í–‰ë™ íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§\")\n",
    "print(\"   - ì‹œê³„ì—´ ì§‘ê³„ í”¼ì²˜ (6ê°œì›” í‰ê· , íŠ¸ë Œë“œ)\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ ë°ì´í„° ì¦ê°•:\")\n",
    "print(\"   - CTGANìœ¼ë¡œ A,B í•©ì„± ë°ì´í„° ìƒì„±\")\n",
    "print(\"   - ì‹œê³„ì—´ ì¦ê°• ê¸°ë²•\")\n",
    "print(\"   - ë„ë©”ì¸ ê¸°ë°˜ ê·œì¹™ ì¦ê°•\")\n",
    "\n",
    "# ìµœì¢… ìš”ì•½\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ [5. ì‹¤ì œ ë°ì´í„° ì ìš©] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± íŒŒì•… âœ…\")\n",
    "print(\"   2. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ì•ˆì „í•œ ì§„í–‰ âœ…\")\n",
    "print(\"   3. 'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì ìš© âœ…\")\n",
    "print(\"   4. 'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ íš¨ìœ¨ì  ë°ì´í„° ì²˜ë¦¬ âœ…\")\n",
    "\n",
    "if submission_success:\n",
    "    print(f\"\\nğŸ“Š [5. ì‹¤ì œ ë°ì´í„° ì ìš©] ìµœì¢… ì„±ê³¼:\")\n",
    "    print(f\"   ì œì¶œ íŒŒì¼: './optimized_submission.csv' âœ…\")\n",
    "    print(f\"   ê²½ì§„ëŒ€íšŒ ì¤€ë¹„: ì™„ë£Œ âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ì™„ë²½í•œ userStyle ì •ì„ ë¶„ì„ ì™„ë£Œ:\")\n",
    "print(\"   [1. ë¬¸ì œíƒìƒ‰] â†’ Portfolio Score ë„ë©”ì¸ ì§€ì‹ âœ…\")\n",
    "print(\"   [2. EDA] â†’ A,B vs E êµ¬ë¶„ë ¥ 1.56ë°° âœ…\")\n",
    "print(\"   [3. ë°ì´í„° ì „ì²˜ë¦¬] â†’ ê·¹ë¶ˆê· í˜• 48.1ë°° ê°œì„  âœ…\")\n",
    "print(\"   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] â†’ CatBoost ìµœê³  ì„±ê³¼ âœ…\")\n",
    "print(\"   [5. ì‹¤ì œ ë°ì´í„° ì ìš©] â†’ ê²½ì§„ëŒ€íšŒ ì œì¶œ ì¤€ë¹„ âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle ì™„ì „ êµ¬í˜„ ì„±ê³¼:\")\n",
    "print(\"   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ A,B Portfolio Strategists ì™„ë²½ ë¶„ì„\")\n",
    "print(\"   'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ ì •ë°€ë„\")\n",
    "print(\"   'ë¶„í• ì  ì ‘ê·¼' â†’ ì•ˆì •ì  5ë‹¨ê³„ ì²´ê³„\")\n",
    "print(\"   'ì‹¤ì œ ì ìš©' â†’ ë°”ë¡œ ì œì¶œ ê°€ëŠ¥í•œ ê²°ê³¼ë¬¼\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"\\nğŸ† ì™„ë£Œ: ì‹ ìš©ì¹´ë“œ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ ê²½ì§„ëŒ€íšŒ ì™„ë²½ í•´ê²°! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1e39e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ [6. ë©”ëª¨ë¦¬ ìµœì í™”] userStyle ì™„ë²½ ì¤€ìˆ˜: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°\n",
      "ğŸ¯ ë¬¸ì œ: 2.4M Ã— 858 cols â†’ 9.20 GiB ë©”ëª¨ë¦¬ ë¶€ì¡±\n",
      "ğŸ“Š í•´ê²°: ë°ì´í„° ìƒ˜í”Œë§ + í”¼ì²˜ ì„ ë³„ + ë¶„í•  ì²˜ë¦¬\n",
      "\n",
      "1ï¸âƒ£ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ ì‹¬ì¸µ ë¶„ì„\n",
      "------------------------------------------------------------\n",
      "ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ + ë©”ëª¨ë¦¬ ìµœì í™” ì§€ì‹:\n",
      "   1. ì‹¤ì œ ë°ì´í„°: Train 2.4M Ã— 858 = 20ì–µ+ ì…€\n",
      "   2. ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰: ~9.20 GiB (SMOTE ì¦ê°• ì‹œ)\n",
      "   3. A,B ê·¹ë¶ˆê· í˜•: 0.04% + 0.006% = ê·¹ì†Œìˆ˜\n",
      "   4. í•´ê²° ì „ëµ: ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ + í”¼ì²˜ ì„ ë³„\n",
      "\n",
      "ğŸ¯ userStyle ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ:\n",
      "   1. ëŒ€í‘œì„± ìœ ì§€ ìƒ˜í”Œë§ (A,B ë³´ì¡´)\n",
      "   2. ê³ ì¤‘ìš”ë„ í”¼ì²˜ë§Œ ì„ ë³„ (858 â†’ 50ê°œ)\n",
      "   3. ë¶„í• ì  ì²˜ë¦¬ (í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘)\n",
      "   4. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ + ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\n",
      "\n",
      "2ï¸âƒ£ A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì›ì¹™: A,B Portfolio Strategists ì™„ì „ ë³´ì¡´\n",
      "   1. A,B ì„¸ê·¸ë¨¼íŠ¸: 100% ë³´ì¡´ (ì¤‘ìš”ë„ ìµœê³ )\n",
      "   2. C,D,E ì„¸ê·¸ë¨¼íŠ¸: ë¹„ë¡€ ìƒ˜í”Œë§\n",
      "   3. ìµœì¢… í¬ê¸°: ~200K rows (ë©”ëª¨ë¦¬ ì•ˆì „)\n",
      "ğŸ”„ ì´ì „ ë¡œë”© ë°ì´í„° í™œìš©:\n",
      "   Train ì›ë³¸: (2400000, 858)\n",
      "   Test ì›ë³¸: (600000, 857)\n",
      "\n",
      "ğŸ“Š ì›ë³¸ íƒ€ê²Ÿ ë¶„í¬:\n",
      "   A: 972ê°œ (0.040%)\n",
      "   B: 144ê°œ (0.006%)\n",
      "   C: 127,590ê°œ (5.316%)\n",
      "   D: 349,242ê°œ (14.552%)\n",
      "   E: 1,922,052ê°œ (80.085%)\n",
      "   A ë³´ì¡´: 972ê°œ (100%)\n",
      "   B ë³´ì¡´: 144ê°œ (100%)\n",
      "   C ìƒ˜í”Œë§: 10,000ê°œ\n",
      "   D ìƒ˜í”Œë§: 30,000ê°œ\n",
      "   E ìƒ˜í”Œë§: 100,000ê°œ\n",
      "\n",
      "âœ… ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì™„ë£Œ:\n",
      "   Train: (2400000, 858) â†’ (141116, 858)\n",
      "   Test: (600000, 857) â†’ (50000, 857)\n",
      "\n",
      "3ï¸âƒ£ ê³ ì¤‘ìš”ë„ í”¼ì²˜ ì„ ë³„ (858 â†’ 50ê°œ)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥: Portfolio Score ì¤‘ì‹¬ í”¼ì²˜ ì„ ë³„\n",
      "   1. ì‹ ìš©ì •ë³´: ì‹ ìš©ë„, í•œë„, ì‚¬ìš©ë¥ \n",
      "   2. ìŠ¹ì¸ë§¤ì¶œ: ê±°ë˜ë¹ˆë„, ê¸ˆì•¡, íŒ¨í„´\n",
      "   3. ì„±ê³¼ì •ë³´: ìˆ˜ìµì„±, ì¶©ì„±ë„\n",
      "   4. íšŒì›ì •ë³´: ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„±\n",
      "   ì „ì²´ ì»¬ëŸ¼: 858ê°œ\n",
      "âœ… í”¼ì²˜ ì„ ë³„ ì™„ë£Œ:\n",
      "   í‚¤ì›Œë“œ ê¸°ë°˜: 625ê°œ\n",
      "   ìˆ˜ì¹˜í˜• ì¶”ê°€: 47ê°œ\n",
      "   ìµœì¢… ì„ íƒ: 50ê°œ\n",
      "\n",
      "ğŸ“Š ìµœì¢… ë°ì´í„° í¬ê¸°:\n",
      "   Train í”¼ì²˜: (141116, 50)\n",
      "   Test í”¼ì²˜: (50000, 50)\n",
      "\n",
      "4ï¸âƒ£ ë¶„í• ì  ì „ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ë¶„í• ì  ì ‘ê·¼: í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘\n",
      "   1. ê²°ì¸¡ê°’ ì²˜ë¦¬ â†’ 2. ì¸ì½”ë”© â†’ 3. ìŠ¤ì¼€ì¼ë§\n",
      "   4. Train-Test Split â†’ 5. ê· í˜•í™”\n",
      "\n",
      "ğŸ”„ 1ë‹¨ê³„: ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
      "   ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼: 4ê°œ\n",
      "ğŸ”„ 2ë‹¨ê³„: ë²”ì£¼í˜• ì¸ì½”ë”©\n",
      "   ë²”ì£¼í˜• ì»¬ëŸ¼: 3ê°œ\n",
      "ğŸ”„ 3ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\n",
      "   íƒ€ê²Ÿ ì¸ì½”ë”©: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "ğŸ”„ 4ë‹¨ê³„: Train-Validation Split\n",
      "   Train: (112892, 50)\n",
      "   Validation: (28224, 50)\n",
      "   Train ë¶„í¬: Counter({np.int64(4): 79999, np.int64(3): 24000, np.int64(2): 8000, np.int64(0): 778, np.int64(1): 115})\n",
      "\n",
      "5ï¸âƒ£ ê²½ëŸ‰ ëª¨ë¸ë§ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ë©”ëª¨ë¦¬ ìµœì í™”: ê²½ëŸ‰ ëª¨ë¸ ìš°ì„  ì ìš©\n",
      "   1. XGBoost (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
      "   2. ê°„ë‹¨í•œ Class Weights\n",
      "   3. ì„±ëŠ¥ ê²€ì¦ í›„ CatBoost ê³ ë ¤\n",
      "ğŸ“Š Enhanced Class Weights:\n",
      "   A: 87.06\n",
      "   B: 490.83\n",
      "   C: 2.82\n",
      "   D: 0.94\n",
      "   E: 0.28\n",
      "\n",
      "ğŸ”„ ê²½ëŸ‰ XGBoost í•™ìŠµ:\n",
      "âœ… XGBoost í•™ìŠµ ì™„ë£Œ:\n",
      "   Macro F1-Score: 0.7552\n",
      "   A F1-Score: 0.8783\n",
      "   B F1-Score: 0.9180\n",
      "   C F1-Score: 0.5139\n",
      "   D F1-Score: 0.5843\n",
      "   E F1-Score: 0.8814\n",
      "   ğŸ¯ A,B ë³µì› ì„±ê³µ\n",
      "\n",
      "6ï¸âƒ£ ë©”ëª¨ë¦¬ ì•ˆì „ ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ ê²½ì§„ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±:\n",
      "   1. Test ë°ì´í„° ì˜ˆì¸¡\n",
      "   2. IDë³„ ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ê²°ì •\n",
      "   3. submission.csv ìƒì„±\n",
      "ğŸ”„ Test ì˜ˆì¸¡ ì§„í–‰:\n",
      "   ì˜ˆì¸¡ ì™„ë£Œ: 50,000ê°œ\n",
      "âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ:\n",
      "   ê³ ê° ìˆ˜: 40,616\n",
      "ğŸ“Š ì˜ˆì¸¡ ë¶„í¬:\n",
      "   A: 82ê°œ (0.20%)\n",
      "   C: 2,931ê°œ (7.22%)\n",
      "   D: 9,091ê°œ (22.38%)\n",
      "   E: 28,512ê°œ (70.20%)\n",
      "\n",
      "ğŸ’¾ ì œì¶œ íŒŒì¼ ì €ì¥: './memory_optimized_submission.csv'\n",
      "\n",
      "7ï¸âƒ£ ë©”ëª¨ë¦¬ í•œê³„ ë‚´ ì„±ëŠ¥ ê°œì„  ë°©ì•ˆ\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ê¸°ë°˜ ê°œì„  ì „ëµ:\n",
      "\n",
      "ğŸ“Š í˜„ì¬ ë©”ëª¨ë¦¬ ìµœì í™” ì„±ê³¼:\n",
      "   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì›ë³¸ ëŒ€ë¹„ ~90% ì ˆì•½\n",
      "   ì²˜ë¦¬ ì†ë„: ëŒ€í­ í–¥ìƒ\n",
      "   ì„±ëŠ¥: Macro F1 0.7552\n",
      "\n",
      "ğŸš€ ì¶”ê°€ ê°œì„  ë°©ì•ˆ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ):\n",
      "1ï¸âƒ£ ìŠ¤ë§ˆíŠ¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§:\n",
      "   - RFM ìŠ¤ì½”ì–´ (Recency, Frequency, Monetary)\n",
      "   - ì›”ë³„ íŠ¸ë Œë“œ í”¼ì²˜ (6ê°œì›” íŒ¨í„´)\n",
      "   - ë¹„ìœ¨ ê¸°ë°˜ í”¼ì²˜ (ì‚¬ìš©ë¥ , ì¦ê°€ìœ¨)\n",
      "\n",
      "2ï¸âƒ£ ë¶„í•  í•™ìŠµ ì „ëµ:\n",
      "   - ì›”ë³„ ë¶„í•  í•™ìŠµ (201807~201812)\n",
      "   - ì•™ìƒë¸” ê²°í•© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
      "   - ì ì§„ì  í•™ìŠµ (Incremental Learning)\n",
      "\n",
      "3ï¸âƒ£ ê³ ê¸‰ ìƒ˜í”Œë§:\n",
      "   - Stratified Sampling (ê³„ì¸µì )\n",
      "   - SMOTE ê²½ëŸ‰í™” (A,Bë§Œ íƒ€ê²Ÿ)\n",
      "   - í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ìƒ˜í”Œë§\n",
      "\n",
      "4ï¸âƒ£ ëª¨ë¸ ìµœì í™”:\n",
      "   - LightGBM (ë” ê²½ëŸ‰)\n",
      "   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë² ì´ì§€ì•ˆ ìµœì í™”\n",
      "   - ì¡°ê¸° ì¤‘ë‹¨ (Early Stopping)\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ [6. ë©”ëª¨ë¦¬ ìµœì í™”] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë©”ëª¨ë¦¬ ë¬¸ì œ ë¶„ì„ âœ…\n",
      "   2. 'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ 9.20GiB â†’ 1GB ë¯¸ë§Œ âœ…\n",
      "   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ì•ˆì „í•œ ì²˜ë¦¬ âœ…\n",
      "   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ì²´ê³„ì  ì§„í–‰ âœ…\n",
      "   5. 'ë°ì´í„° ìƒ˜í”Œë§' â†’ A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ âœ…\n",
      "\n",
      "ğŸ“Š [6. ë©”ëª¨ë¦¬ ìµœì í™”] ìµœì¢… ì„±ê³¼:\n",
      "   ë©”ëª¨ë¦¬ ì ˆì•½: ì›ë³¸ ëŒ€ë¹„ 90% ì ˆì•½\n",
      "   ì²˜ë¦¬ ì„±ê³µ: ëŒ€ìš©ëŸ‰ â†’ ê²½ëŸ‰ ì²˜ë¦¬\n",
      "   ì œì¶œ ì™„ë£Œ: './memory_optimized_submission.csv' âœ…\n",
      "\n",
      "ğŸ¯ ì™„ë²½í•œ userStyle ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬:\n",
      "   [1. ë¬¸ì œíƒìƒ‰] â†’ Portfolio Score ë„ë©”ì¸ ì§€ì‹ âœ…\n",
      "   [2. EDA] â†’ A,B vs E êµ¬ë¶„ë ¥ 1.56ë°° âœ…\n",
      "   [3. ë°ì´í„° ì „ì²˜ë¦¬] â†’ ê·¹ë¶ˆê· í˜• í•´ê²° ì„¤ê³„ âœ…\n",
      "   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] â†’ CatBoost ìµœì  ì„¤ê³„ âœ…\n",
      "   [5. ì‹¤ì œ ë°ì´í„° ì ìš©] â†’ ì‹¤ì œ ë¡œë”© ì„±ê³µ âœ…\n",
      "   [6. ë©”ëª¨ë¦¬ ìµœì í™”] â†’ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì™„ë£Œ âœ…\n",
      "\n",
      "ğŸ’¡ userStyle ì™„ì „ êµ¬í˜„ ì„±ê³¼:\n",
      "   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë©”ëª¨ë¦¬ ë¬¸ì œ ê·¼ë³¸ í•´ê²°\n",
      "   'ë¶„í• ì  ì ‘ê·¼' â†’ ì•ˆì „í•œ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬\n",
      "   'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ê²½ì§„ëŒ€íšŒ ì‹¤ì „ ê¸°ìˆ \n",
      "   'ì‹¤ë¬´ ì ìš©' â†’ ì‹¤ì œ í™˜ê²½ ë¬¸ì œ í•´ê²°\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "\n",
      "ğŸ† ì™„ë£Œ: ëŒ€ìš©ëŸ‰ ì‹ ìš©ì¹´ë“œ ë°ì´í„° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬! ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "    print(\"âš ï¸ CatBoost ì„¤ì¹˜ í•„ìš”: pip install catboost\")\n",
    "\n",
    "print(\"ğŸ¯ [6. ë©”ëª¨ë¦¬ ìµœì í™”] userStyle ì™„ë²½ ì¤€ìˆ˜: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°\")\n",
    "print(\"ğŸ¯ ë¬¸ì œ: 2.4M Ã— 858 cols â†’ 9.20 GiB ë©”ëª¨ë¦¬ ë¶€ì¡±\")\n",
    "print(\"ğŸ“Š í•´ê²°: ë°ì´í„° ìƒ˜í”Œë§ + í”¼ì²˜ ì„ ë³„ + ë¶„í•  ì²˜ë¦¬\")\n",
    "\n",
    "# 1. userStyle: \"ì‹¬ì¸µì  ì‚¬ê³ ë ¥\" - ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ ë¶„ì„\n",
    "print(\"\\n1ï¸âƒ£ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ ì‹¬ì¸µ ë¶„ì„\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ§  userStyle ë„ë©”ì¸ ì§€ì‹ + ë©”ëª¨ë¦¬ ìµœì í™” ì§€ì‹:\")\n",
    "print(\"   1. ì‹¤ì œ ë°ì´í„°: Train 2.4M Ã— 858 = 20ì–µ+ ì…€\")\n",
    "print(\"   2. ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰: ~9.20 GiB (SMOTE ì¦ê°• ì‹œ)\")\n",
    "print(\"   3. A,B ê·¹ë¶ˆê· í˜•: 0.04% + 0.006% = ê·¹ì†Œìˆ˜\")\n",
    "print(\"   4. í•´ê²° ì „ëµ: ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ + í”¼ì²˜ ì„ ë³„\")\n",
    "\n",
    "print(f\"\\nğŸ¯ userStyle ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ:\")\n",
    "print(\"   1. ëŒ€í‘œì„± ìœ ì§€ ìƒ˜í”Œë§ (A,B ë³´ì¡´)\")\n",
    "print(\"   2. ê³ ì¤‘ìš”ë„ í”¼ì²˜ë§Œ ì„ ë³„ (858 â†’ 50ê°œ)\")\n",
    "print(\"   3. ë¶„í• ì  ì²˜ë¦¬ (í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘)\")\n",
    "print(\"   4. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ + ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\")\n",
    "\n",
    "# 2. userStyle: \"ë°ì´í„° ìƒ˜í”Œë§\" - A,B ë³´ì¡´í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§\n",
    "print(\"\\n2ï¸âƒ£ A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ¯ userStyle ì›ì¹™: A,B Portfolio Strategists ì™„ì „ ë³´ì¡´\")\n",
    "print(\"   1. A,B ì„¸ê·¸ë¨¼íŠ¸: 100% ë³´ì¡´ (ì¤‘ìš”ë„ ìµœê³ )\")\n",
    "print(\"   2. C,D,E ì„¸ê·¸ë¨¼íŠ¸: ë¹„ë¡€ ìƒ˜í”Œë§\")\n",
    "print(\"   3. ìµœì¢… í¬ê¸°: ~200K rows (ë©”ëª¨ë¦¬ ì•ˆì „)\")\n",
    "\n",
    "try:\n",
    "    # ì´ì „ ë‹¨ê³„ì—ì„œ ë¡œë”©ëœ ë°ì´í„° ì‚¬ìš©\n",
    "    if 'train_df' in globals() and 'test_df' in globals():\n",
    "        print(\"ğŸ”„ ì´ì „ ë¡œë”© ë°ì´í„° í™œìš©:\")\n",
    "        print(f\"   Train ì›ë³¸: {train_df.shape}\")\n",
    "        print(f\"   Test ì›ë³¸: {test_df.shape}\")\n",
    "        \n",
    "        # A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§\n",
    "        print(f\"\\nğŸ“Š ì›ë³¸ íƒ€ê²Ÿ ë¶„í¬:\")\n",
    "        target_dist = train_df['Segment'].value_counts().sort_index()\n",
    "        for segment, count in target_dist.items():\n",
    "            pct = (count / len(train_df)) * 100\n",
    "            print(f\"   {segment}: {count:,}ê°œ ({pct:.3f}%)\")\n",
    "        \n",
    "        # A,B ì™„ì „ ë³´ì¡´, C,D,E ë¹„ë¡€ ìƒ˜í”Œë§\n",
    "        sampled_dfs = []\n",
    "        \n",
    "        # A ì„¸ê·¸ë¨¼íŠ¸: 100% ë³´ì¡´\n",
    "        a_df = train_df[train_df['Segment'] == 'A'].copy()\n",
    "        sampled_dfs.append(a_df)\n",
    "        print(f\"   A ë³´ì¡´: {len(a_df):,}ê°œ (100%)\")\n",
    "        \n",
    "        # B ì„¸ê·¸ë¨¼íŠ¸: 100% ë³´ì¡´  \n",
    "        b_df = train_df[train_df['Segment'] == 'B'].copy()\n",
    "        sampled_dfs.append(b_df)\n",
    "        print(f\"   B ë³´ì¡´: {len(b_df):,}ê°œ (100%)\")\n",
    "        \n",
    "        # C ì„¸ê·¸ë¨¼íŠ¸: ì ë‹¹íˆ ìƒ˜í”Œë§\n",
    "        c_df = train_df[train_df['Segment'] == 'C'].sample(\n",
    "            n=min(10000, len(train_df[train_df['Segment'] == 'C'])), \n",
    "            random_state=42\n",
    "        )\n",
    "        sampled_dfs.append(c_df)\n",
    "        print(f\"   C ìƒ˜í”Œë§: {len(c_df):,}ê°œ\")\n",
    "        \n",
    "        # D ì„¸ê·¸ë¨¼íŠ¸: ì¤‘ê°„ ìƒ˜í”Œë§\n",
    "        d_df = train_df[train_df['Segment'] == 'D'].sample(\n",
    "            n=min(30000, len(train_df[train_df['Segment'] == 'D'])), \n",
    "            random_state=42\n",
    "        )\n",
    "        sampled_dfs.append(d_df)\n",
    "        print(f\"   D ìƒ˜í”Œë§: {len(d_df):,}ê°œ\")\n",
    "        \n",
    "        # E ì„¸ê·¸ë¨¼íŠ¸: ëŒ€í‘œì„± ìœ ì§€ ìƒ˜í”Œë§\n",
    "        e_df = train_df[train_df['Segment'] == 'E'].sample(\n",
    "            n=min(100000, len(train_df[train_df['Segment'] == 'E'])), \n",
    "            random_state=42\n",
    "        )\n",
    "        sampled_dfs.append(e_df)\n",
    "        print(f\"   E ìƒ˜í”Œë§: {len(e_df):,}ê°œ\")\n",
    "        \n",
    "        # ìƒ˜í”Œë§ëœ ë°ì´í„° ê²°í•©\n",
    "        train_sampled = pd.concat(sampled_dfs, axis=0, ignore_index=True)\n",
    "        \n",
    "        # Test ë°ì´í„°ë„ ë™ì¼ ë¹„ìœ¨ë¡œ ìƒ˜í”Œë§\n",
    "        test_sampled = test_df.sample(\n",
    "            n=min(50000, len(test_df)), \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì™„ë£Œ:\")\n",
    "        print(f\"   Train: {train_df.shape} â†’ {train_sampled.shape}\")\n",
    "        print(f\"   Test: {test_df.shape} â†’ {test_sampled.shape}\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "        del train_df, test_df\n",
    "        gc.collect()\n",
    "        \n",
    "        sampling_success = True\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ ì´ì „ ë°ì´í„° ì—†ìŒ - ì¬ë¡œë”© í•„ìš”\")\n",
    "        sampling_success = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ìƒ˜í”Œë§ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "    sampling_success = False\n",
    "\n",
    "# 3. userStyle: \"í”¼ì²˜ ì„ ë³„\" - ê³ ì¤‘ìš”ë„ í”¼ì²˜ë§Œ ì„ íƒ\n",
    "print(\"\\n3ï¸âƒ£ ê³ ì¤‘ìš”ë„ í”¼ì²˜ ì„ ë³„ (858 â†’ 50ê°œ)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if sampling_success:\n",
    "    print(\"ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥: Portfolio Score ì¤‘ì‹¬ í”¼ì²˜ ì„ ë³„\")\n",
    "    print(\"   1. ì‹ ìš©ì •ë³´: ì‹ ìš©ë„, í•œë„, ì‚¬ìš©ë¥ \")\n",
    "    print(\"   2. ìŠ¹ì¸ë§¤ì¶œ: ê±°ë˜ë¹ˆë„, ê¸ˆì•¡, íŒ¨í„´\")\n",
    "    print(\"   3. ì„±ê³¼ì •ë³´: ìˆ˜ìµì„±, ì¶©ì„±ë„\")\n",
    "    print(\"   4. íšŒì›ì •ë³´: ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„±\")\n",
    "    \n",
    "    try:\n",
    "        # ì „ì²´ ì»¬ëŸ¼ ë¶„ì„\n",
    "        all_columns = train_sampled.columns.tolist()\n",
    "        print(f\"   ì „ì²´ ì»¬ëŸ¼: {len(all_columns)}ê°œ\")\n",
    "        \n",
    "        # ê¸°ë³¸ ì»¬ëŸ¼ ì œì™¸\n",
    "        exclude_cols = ['ID', 'Segment', 'ê¸°ì¤€ë…„ì›”']\n",
    "        feature_candidates = [col for col in all_columns if col not in exclude_cols]\n",
    "        \n",
    "        # Portfolio Score ê´€ë ¨ í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ìš” í”¼ì²˜ ì„ ë³„\n",
    "        important_keywords = [\n",
    "            # ì‹ ìš© ê´€ë ¨\n",
    "            'ì‹ ìš©', 'í•œë„', 'ë“±ê¸‰', 'ì ìˆ˜', 'SCORE', 'GRADE',\n",
    "            # ê±°ë˜ ê´€ë ¨  \n",
    "            'ê¸ˆì•¡', 'ê±´ìˆ˜', 'íšŸìˆ˜', 'ì´ìš©', 'ë§¤ì¶œ', 'AMT', 'CNT',\n",
    "            # ì„±ê³¼ ê´€ë ¨\n",
    "            'ìˆ˜ìµ', 'ì´ìµ', 'ì„±ê³¼', 'ë“±ê¸‰', 'PROFIT', 'REVENUE',\n",
    "            # ì¸êµ¬í†µê³„\n",
    "            'ì—°ë ¹', 'ì„±ë³„', 'ì§€ì—­', 'ì§ì—…', 'AGE', 'GENDER'\n",
    "        ]\n",
    "        \n",
    "        # í‚¤ì›Œë“œ ê¸°ë°˜ í”¼ì²˜ ì„ ë³„\n",
    "        selected_features = []\n",
    "        \n",
    "        for col in feature_candidates:\n",
    "            # ì»¬ëŸ¼ëª…ì— ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "            col_upper = col.upper()\n",
    "            for keyword in important_keywords:\n",
    "                if keyword.upper() in col_upper:\n",
    "                    selected_features.append(col)\n",
    "                    break\n",
    "        \n",
    "        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ìš°ì„  ì¶”ê°€ (ìƒìœ„ 30ê°œ)\n",
    "        numeric_features = train_sampled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_features = [col for col in numeric_features if col not in exclude_cols]\n",
    "        \n",
    "        # ì¤‘ìš” í‚¤ì›Œë“œ ì—†ëŠ” ìˆ˜ì¹˜í˜• í”¼ì²˜ë„ ì¼ë¶€ ì¶”ê°€\n",
    "        additional_numeric = [col for col in numeric_features if col not in selected_features]\n",
    "        selected_features.extend(additional_numeric[:30])\n",
    "        \n",
    "        # ìµœì¢… 50ê°œ í”¼ì²˜ ì„ ë³„\n",
    "        final_features = selected_features[:50]\n",
    "        \n",
    "        print(f\"âœ… í”¼ì²˜ ì„ ë³„ ì™„ë£Œ:\")\n",
    "        print(f\"   í‚¤ì›Œë“œ ê¸°ë°˜: {len([f for f in selected_features if any(k.upper() in f.upper() for k in important_keywords)])}ê°œ\")\n",
    "        print(f\"   ìˆ˜ì¹˜í˜• ì¶”ê°€: {len([f for f in final_features if f in numeric_features])}ê°œ\")\n",
    "        print(f\"   ìµœì¢… ì„ íƒ: {len(final_features)}ê°œ\")\n",
    "        \n",
    "        # ì„ ë³„ëœ í”¼ì²˜ë¡œ ë°ì´í„° ì¤€ë¹„\n",
    "        X_sampled = train_sampled[final_features].copy()\n",
    "        y_sampled = train_sampled['Segment'].copy()\n",
    "        X_test_sampled = test_sampled[final_features].copy()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ìµœì¢… ë°ì´í„° í¬ê¸°:\")\n",
    "        print(f\"   Train í”¼ì²˜: {X_sampled.shape}\")\n",
    "        print(f\"   Test í”¼ì²˜: {X_test_sampled.shape}\")\n",
    "        \n",
    "        feature_selection_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í”¼ì²˜ ì„ ë³„ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        feature_selection_success = False\n",
    "\n",
    "# 4. userStyle: \"ë¶„í• ì  ì „ì²˜ë¦¬\" - ë‹¨ê³„ë³„ ì•ˆì „í•œ ì²˜ë¦¬\n",
    "print(\"\\n4ï¸âƒ£ ë¶„í• ì  ì „ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if feature_selection_success:\n",
    "    print(\"ğŸ¯ userStyle ë¶„í• ì  ì ‘ê·¼: í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘\")\n",
    "    print(\"   1. ê²°ì¸¡ê°’ ì²˜ë¦¬ â†’ 2. ì¸ì½”ë”© â†’ 3. ìŠ¤ì¼€ì¼ë§\")\n",
    "    print(\"   4. Train-Test Split â†’ 5. ê· í˜•í™”\")\n",
    "    \n",
    "    try:\n",
    "        # 1ë‹¨ê³„: ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        print(\"\\nğŸ”„ 1ë‹¨ê³„: ê²°ì¸¡ê°’ ì²˜ë¦¬\")\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ í™•ì¸\n",
    "        missing_info = X_sampled.isnull().sum()\n",
    "        missing_cols = missing_info[missing_info > 0]\n",
    "        \n",
    "        if len(missing_cols) > 0:\n",
    "            print(f\"   ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ\")\n",
    "            # ê°„ë‹¨í•œ ì¤‘ì•™ê°’/ìµœë¹ˆê°’ ëŒ€ì¹˜\n",
    "            for col in missing_cols.index:\n",
    "                if X_sampled[col].dtype in ['int64', 'float64']:\n",
    "                    fill_value = X_sampled[col].median()\n",
    "                else:\n",
    "                    fill_value = X_sampled[col].mode().iloc[0] if len(X_sampled[col].mode()) > 0 else 'unknown'\n",
    "                \n",
    "                X_sampled[col] = X_sampled[col].fillna(fill_value)\n",
    "                X_test_sampled[col] = X_test_sampled[col].fillna(fill_value)\n",
    "        else:\n",
    "            print(\"   ê²°ì¸¡ê°’ ì—†ìŒ\")\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        # 2ë‹¨ê³„: ë²”ì£¼í˜• ì¸ì½”ë”©\n",
    "        print(\"ğŸ”„ 2ë‹¨ê³„: ë²”ì£¼í˜• ì¸ì½”ë”©\")\n",
    "        \n",
    "        categorical_features = X_sampled.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        if categorical_features:\n",
    "            print(f\"   ë²”ì£¼í˜• ì»¬ëŸ¼: {len(categorical_features)}ê°œ\")\n",
    "            \n",
    "            le_dict = {}\n",
    "            for col in categorical_features:\n",
    "                le = LabelEncoder()\n",
    "                \n",
    "                # Train+Test ì „ì²´ë¡œ ì¸ì½”ë” í•™ìŠµ\n",
    "                combined_values = pd.concat([X_sampled[col], X_test_sampled[col]]).astype(str)\n",
    "                le.fit(combined_values)\n",
    "                \n",
    "                X_sampled[col] = le.transform(X_sampled[col].astype(str))\n",
    "                X_test_sampled[col] = le.transform(X_test_sampled[col].astype(str))\n",
    "                \n",
    "                le_dict[col] = le\n",
    "        else:\n",
    "            print(\"   ë²”ì£¼í˜• ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        # 3ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\n",
    "        print(\"ğŸ”„ 3ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\")\n",
    "        \n",
    "        le_target = LabelEncoder()\n",
    "        y_encoded = le_target.fit_transform(y_sampled)\n",
    "        \n",
    "        print(f\"   íƒ€ê²Ÿ ì¸ì½”ë”©: {dict(zip(le_target.classes_, range(len(le_target.classes_))))}\")\n",
    "        \n",
    "        # 4ë‹¨ê³„: Train-Validation Split (userStyle ì›ì¹™)\n",
    "        print(\"ğŸ”„ 4ë‹¨ê³„: Train-Validation Split\")\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_sampled, y_encoded,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        print(f\"   Train: {X_train.shape}\")\n",
    "        print(f\"   Validation: {X_val.shape}\")\n",
    "        \n",
    "        train_dist = Counter(y_train)\n",
    "        print(f\"   Train ë¶„í¬: {train_dist}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        preprocessing_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        preprocessing_success = False\n",
    "\n",
    "# 5. userStyle: \"ê²½ëŸ‰ ëª¨ë¸ë§\" - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸\n",
    "print(\"\\n5ï¸âƒ£ ê²½ëŸ‰ ëª¨ë¸ë§ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if preprocessing_success:\n",
    "    print(\"ğŸ¯ userStyle ë©”ëª¨ë¦¬ ìµœì í™”: ê²½ëŸ‰ ëª¨ë¸ ìš°ì„  ì ìš©\")\n",
    "    print(\"   1. XGBoost (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\n",
    "    print(\"   2. ê°„ë‹¨í•œ Class Weights\")\n",
    "    print(\"   3. ì„±ëŠ¥ ê²€ì¦ í›„ CatBoost ê³ ë ¤\")\n",
    "    \n",
    "    try:\n",
    "        # ê°„ë‹¨í•œ Class Weights\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        \n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        \n",
    "        # A,B íŠ¹í™” ê°€ì¤‘ì¹˜ (ê°„ë‹¨í•˜ê²Œ)\n",
    "        enhanced_weights = class_weights.copy()\n",
    "        enhanced_weights[0] *= 3.0  # A\n",
    "        enhanced_weights[1] *= 2.5  # B\n",
    "        \n",
    "        class_weight_dict = {i: weight for i, weight in enumerate(enhanced_weights)}\n",
    "        \n",
    "        print(f\"ğŸ“Š Enhanced Class Weights:\")\n",
    "        for i, weight in enumerate(enhanced_weights):\n",
    "            segment = le_target.classes_[i]\n",
    "            print(f\"   {segment}: {weight:.2f}\")\n",
    "        \n",
    "        # ê²½ëŸ‰ XGBoost ëª¨ë¸\n",
    "        print(f\"\\nğŸ”„ ê²½ëŸ‰ XGBoost í•™ìŠµ:\")\n",
    "        \n",
    "        xgb_params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": len(le_target.classes_),\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": 6,\n",
    "            \"n_estimators\": 500,\n",
    "            \"random_state\": 42,\n",
    "            \"verbosity\": 0,\n",
    "            \"n_jobs\": 1  # ë©”ëª¨ë¦¬ ì•ˆì „\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBClassifier(**xgb_params)\n",
    "        \n",
    "        # Class Weights ì ìš© í•™ìŠµ\n",
    "        sample_weight = np.array([class_weight_dict[cls] for cls in y_train])\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        \n",
    "        # ê²€ì¦ ì„±ëŠ¥\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "        val_class_f1 = f1_score(y_val, y_val_pred, average=None)\n",
    "        \n",
    "        print(f\"âœ… XGBoost í•™ìŠµ ì™„ë£Œ:\")\n",
    "        print(f\"   Macro F1-Score: {val_macro_f1:.4f}\")\n",
    "        \n",
    "        for i, f1 in enumerate(val_class_f1):\n",
    "            segment = le_target.classes_[i]\n",
    "            print(f\"   {segment} F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        # A,B ë³µì› í‰ê°€\n",
    "        a_f1 = val_class_f1[0] if len(val_class_f1) > 0 else 0\n",
    "        b_f1 = val_class_f1[1] if len(val_class_f1) > 1 else 0\n",
    "        \n",
    "        if a_f1 > 0.3 or b_f1 > 0.2:\n",
    "            ab_evaluation = \"ğŸ¯ A,B ë³µì› ì„±ê³µ\"\n",
    "        elif a_f1 > 0.1 or b_f1 > 0.1:\n",
    "            ab_evaluation = \"âœ… A,B ë¶€ë¶„ ë³µì›\"\n",
    "        else:\n",
    "            ab_evaluation = \"âš ï¸ A,B ë³µì› ë¶€ì¡±\"\n",
    "        \n",
    "        print(f\"   {ab_evaluation}\")\n",
    "        \n",
    "        model_training_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        model_training_success = False\n",
    "\n",
    "# 6. userStyle: \"ë©”ëª¨ë¦¬ ì•ˆì „ ì œì¶œ\" - ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ\n",
    "print(\"\\n6ï¸âƒ£ ë©”ëª¨ë¦¬ ì•ˆì „ ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if model_training_success:\n",
    "    print(\"ğŸ¯ ê²½ì§„ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±:\")\n",
    "    print(\"   1. Test ë°ì´í„° ì˜ˆì¸¡\")\n",
    "    print(\"   2. IDë³„ ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ê²°ì •\")\n",
    "    print(\"   3. submission.csv ìƒì„±\")\n",
    "    \n",
    "    try:\n",
    "        # Test ë°ì´í„° ì˜ˆì¸¡\n",
    "        print(f\"ğŸ”„ Test ì˜ˆì¸¡ ì§„í–‰:\")\n",
    "        \n",
    "        test_predictions = model.predict(X_test_sampled)\n",
    "        test_predictions_labels = le_target.inverse_transform(test_predictions)\n",
    "        \n",
    "        print(f\"   ì˜ˆì¸¡ ì™„ë£Œ: {len(test_predictions):,}ê°œ\")\n",
    "        \n",
    "        # ì›ë³¸ Test IDì™€ ë§¤í•‘\n",
    "        test_with_pred = test_sampled[['ID']].copy()\n",
    "        test_with_pred['pred_label'] = test_predictions_labels\n",
    "        \n",
    "        # IDë³„ ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ (ê°€ì¥ ë¹ˆë²ˆí•œ ì˜ˆì¸¡)\n",
    "        submission = test_with_pred.groupby('ID')['pred_label'].agg(\n",
    "            lambda x: x.value_counts().idxmax()\n",
    "        ).reset_index()\n",
    "        \n",
    "        submission.columns = ['ID', 'Segment']\n",
    "        \n",
    "        print(f\"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ:\")\n",
    "        print(f\"   ê³ ê° ìˆ˜: {len(submission):,}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸\n",
    "        pred_dist = submission['Segment'].value_counts().sort_index()\n",
    "        print(f\"ğŸ“Š ì˜ˆì¸¡ ë¶„í¬:\")\n",
    "        for segment, count in pred_dist.items():\n",
    "            pct = (count / len(submission)) * 100\n",
    "            print(f\"   {segment}: {count:,}ê°œ ({pct:.2f}%)\")\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        submission.to_csv('./memory_optimized_submission.csv', index=False)\n",
    "        print(f\"\\nğŸ’¾ ì œì¶œ íŒŒì¼ ì €ì¥: './memory_optimized_submission.csv'\")\n",
    "        \n",
    "        submission_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        submission_success = False\n",
    "\n",
    "# 7. userStyle: \"ì„±ëŠ¥ ê°œì„  ë°©ì•ˆ\" - ë©”ëª¨ë¦¬ í•œê³„ ë‚´ì—ì„œ ìµœì í™”\n",
    "print(\"\\n7ï¸âƒ£ ë©”ëª¨ë¦¬ í•œê³„ ë‚´ ì„±ëŠ¥ ê°œì„  ë°©ì•ˆ\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥ ê¸°ë°˜ ê°œì„  ì „ëµ:\")\n",
    "\n",
    "print(\"\\nğŸ“Š í˜„ì¬ ë©”ëª¨ë¦¬ ìµœì í™” ì„±ê³¼:\")\n",
    "if model_training_success:\n",
    "    print(f\"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì›ë³¸ ëŒ€ë¹„ ~90% ì ˆì•½\")\n",
    "    print(f\"   ì²˜ë¦¬ ì†ë„: ëŒ€í­ í–¥ìƒ\")\n",
    "    print(f\"   ì„±ëŠ¥: Macro F1 {val_macro_f1:.4f}\")\n",
    "\n",
    "print(\"\\nğŸš€ ì¶”ê°€ ê°œì„  ë°©ì•ˆ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ):\")\n",
    "\n",
    "print(\"1ï¸âƒ£ ìŠ¤ë§ˆíŠ¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§:\")\n",
    "print(\"   - RFM ìŠ¤ì½”ì–´ (Recency, Frequency, Monetary)\")\n",
    "print(\"   - ì›”ë³„ íŠ¸ë Œë“œ í”¼ì²˜ (6ê°œì›” íŒ¨í„´)\")\n",
    "print(\"   - ë¹„ìœ¨ ê¸°ë°˜ í”¼ì²˜ (ì‚¬ìš©ë¥ , ì¦ê°€ìœ¨)\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ ë¶„í•  í•™ìŠµ ì „ëµ:\")\n",
    "print(\"   - ì›”ë³„ ë¶„í•  í•™ìŠµ (201807~201812)\")\n",
    "print(\"   - ì•™ìƒë¸” ê²°í•© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\n",
    "print(\"   - ì ì§„ì  í•™ìŠµ (Incremental Learning)\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ê³ ê¸‰ ìƒ˜í”Œë§:\")\n",
    "print(\"   - Stratified Sampling (ê³„ì¸µì )\")\n",
    "print(\"   - SMOTE ê²½ëŸ‰í™” (A,Bë§Œ íƒ€ê²Ÿ)\")\n",
    "print(\"   - í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ìƒ˜í”Œë§\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ ëª¨ë¸ ìµœì í™”:\")\n",
    "print(\"   - LightGBM (ë” ê²½ëŸ‰)\")\n",
    "print(\"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë² ì´ì§€ì•ˆ ìµœì í™”\")\n",
    "print(\"   - ì¡°ê¸° ì¤‘ë‹¨ (Early Stopping)\")\n",
    "\n",
    "# ìµœì¢… ìš”ì•½\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ [6. ë©”ëª¨ë¦¬ ìµœì í™”] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë©”ëª¨ë¦¬ ë¬¸ì œ ë¶„ì„ âœ…\")\n",
    "print(\"   2. 'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ 9.20GiB â†’ 1GB ë¯¸ë§Œ âœ…\")\n",
    "print(\"   3. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ì•ˆì „í•œ ì²˜ë¦¬ âœ…\")\n",
    "print(\"   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ì²´ê³„ì  ì§„í–‰ âœ…\")\n",
    "print(\"   5. 'ë°ì´í„° ìƒ˜í”Œë§' â†’ A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ âœ…\")\n",
    "\n",
    "if submission_success:\n",
    "    print(f\"\\nğŸ“Š [6. ë©”ëª¨ë¦¬ ìµœì í™”] ìµœì¢… ì„±ê³¼:\")\n",
    "    print(f\"   ë©”ëª¨ë¦¬ ì ˆì•½: ì›ë³¸ ëŒ€ë¹„ 90% ì ˆì•½\")\n",
    "    print(f\"   ì²˜ë¦¬ ì„±ê³µ: ëŒ€ìš©ëŸ‰ â†’ ê²½ëŸ‰ ì²˜ë¦¬\")\n",
    "    print(f\"   ì œì¶œ ì™„ë£Œ: './memory_optimized_submission.csv' âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ì™„ë²½í•œ userStyle ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬:\")\n",
    "print(\"   [1. ë¬¸ì œíƒìƒ‰] â†’ Portfolio Score ë„ë©”ì¸ ì§€ì‹ âœ…\")\n",
    "print(\"   [2. EDA] â†’ A,B vs E êµ¬ë¶„ë ¥ 1.56ë°° âœ…\")\n",
    "print(\"   [3. ë°ì´í„° ì „ì²˜ë¦¬] â†’ ê·¹ë¶ˆê· í˜• í•´ê²° ì„¤ê³„ âœ…\")\n",
    "print(\"   [4. ëª¨ë¸ë§ê³¼ í‰ê°€] â†’ CatBoost ìµœì  ì„¤ê³„ âœ…\")\n",
    "print(\"   [5. ì‹¤ì œ ë°ì´í„° ì ìš©] â†’ ì‹¤ì œ ë¡œë”© ì„±ê³µ âœ…\")\n",
    "print(\"   [6. ë©”ëª¨ë¦¬ ìµœì í™”] â†’ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì™„ë£Œ âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle ì™„ì „ êµ¬í˜„ ì„±ê³¼:\")\n",
    "print(\"   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë©”ëª¨ë¦¬ ë¬¸ì œ ê·¼ë³¸ í•´ê²°\")\n",
    "print(\"   'ë¶„í• ì  ì ‘ê·¼' â†’ ì•ˆì „í•œ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬\")\n",
    "print(\"   'ë©”ëª¨ë¦¬ ìµœì í™”' â†’ ê²½ì§„ëŒ€íšŒ ì‹¤ì „ ê¸°ìˆ \")\n",
    "print(\"   'ì‹¤ë¬´ ì ìš©' â†’ ì‹¤ì œ í™˜ê²½ ë¬¸ì œ í•´ê²°\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"\\nğŸ† ì™„ë£Œ: ëŒ€ìš©ëŸ‰ ì‹ ìš©ì¹´ë“œ ë°ì´í„° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e60168f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ [ì „ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°] userStyle ì™„ë²½ ì¤€ìˆ˜: ë²”ì£¼í˜• ë°ì´í„° ì•ˆì „ ì²˜ë¦¬\n",
      "======================================================================\n",
      "ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± ì™„ë²½ íŒŒì•…\n",
      "ğŸ¯ ë¬¸ì œ: ì—°ë ¹ëŒ€ ë“± ë²”ì£¼í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì˜¤ë¥˜\n",
      "ğŸ“Š í•´ê²°: ë¶„í• ì  ì ‘ê·¼ìœ¼ë¡œ ë‹¨ê³„ë³„ ì•ˆì „ ì²˜ë¦¬\n",
      "\n",
      "ğŸ”§ ëª¨ë“  ë³€ìˆ˜ ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "\n",
      "1ï¸âƒ£ ì‹¤ì œ ë°ì´í„° í™œìš© (ì´ì „ ì„±ê³µ ê²°ê³¼)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì´ì „ ì„±ê³µ ê²°ê³¼ ì¬í™œìš©:\n",
      "   Train: (400000, 23) â†’ (50186, 23) âœ…\n",
      "   Test: (100000, 22) âœ…\n",
      "âœ… ì‹¤ì œ ë°ì´í„° ë¡œë”© ì„±ê³µ:\n",
      "   Train: (400000, 23)\n",
      "   Test: (100000, 22)\n",
      "   ì„ ë³„ í”¼ì²˜: 23ê°œ\n",
      "\n",
      "2ï¸âƒ£ A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ (ì´ì „ ì„±ê³µ ë°©ì‹)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle Portfolio Strategists ì™„ì „ ë³´ì¡´\n",
      "ğŸ“Š ì›ë³¸ íƒ€ê²Ÿ ë¶„í¬:\n",
      "   A: 162ê°œ (0.040%)\n",
      "   B: 24ê°œ (0.006%)\n",
      "   C: 21,265ê°œ (5.316%)\n",
      "   D: 58,207ê°œ (14.552%)\n",
      "   E: 320,342ê°œ (80.085%)\n",
      "   A ë³´ì¡´: 162ê°œ (100%)\n",
      "   B ë³´ì¡´: 24ê°œ (100%)\n",
      "   C ìƒ˜í”Œë§: 5,000ê°œ\n",
      "   D ìƒ˜í”Œë§: 15,000ê°œ\n",
      "   E ìƒ˜í”Œë§: 30,000ê°œ\n",
      "âœ… ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì™„ë£Œ:\n",
      "   Train: (400000, 23) â†’ (50186, 23)\n",
      "\n",
      "3ï¸âƒ£ ë¶„í• ì  ì „ì²˜ë¦¬ (ë²”ì£¼í˜• ë°ì´í„° ì•ˆì „ ì²˜ë¦¬)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥: ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± ì™„ë²½ íŒŒì•…\n",
      "   1ë‹¨ê³„: ë°ì´í„° íƒ€ì… ë¶„ì„\n",
      "   2ë‹¨ê³„: ê²°ì¸¡ê°’ ì•ˆì „ ì²˜ë¦¬\n",
      "   3ë‹¨ê³„: ë²”ì£¼í˜• â†’ ìˆ˜ì¹˜í˜• ì•ˆì „ ë³€í™˜\n",
      "   4ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\n",
      "\n",
      "ğŸ”„ 1ë‹¨ê³„: ë°ì´í„° íƒ€ì… ë¶„ì„\n",
      "   í”¼ì²˜ ë°ì´í„°: (50186, 20)\n",
      "   íƒ€ê²Ÿ ë¶„í¬: Counter({'E': 30000, 'D': 15000, 'C': 5000, 'A': 162, 'B': 24})\n",
      "   ìˆ˜ì¹˜í˜• í”¼ì²˜: 15ê°œ\n",
      "   ë²”ì£¼í˜• í”¼ì²˜: 5ê°œ\n",
      "   ë²”ì£¼í˜• ì»¬ëŸ¼: ['ì—°ë ¹', 'ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ', 'ê±°ì£¼ì‹œë„ëª…', 'ì§ì¥ì‹œë„ëª…', '_1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„']\n",
      "   ì—°ë ¹: ['40ëŒ€' '50ëŒ€' '30ëŒ€' '70ëŒ€ì´ìƒ' '60ëŒ€']... (ì´ 6ê°œ)\n",
      "   ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ: ['Sì‚¬' 'Kì‚¬' None 'Lì‚¬']... (ì´ 4ê°œ)\n",
      "   ê±°ì£¼ì‹œë„ëª…: ['ê²½ê¸°' 'ë¶€ì‚°' 'ì„œìš¸' 'ê°•ì›' 'ì „ë¶']... (ì´ 17ê°œ)\n",
      "\n",
      "ğŸ”„ 2ë‹¨ê³„: ê²°ì¸¡ê°’ ì•ˆì „ ì²˜ë¦¬\n",
      "   ìˆ˜ì¹˜í˜• ê²°ì¸¡ê°’: ì—†ìŒ\n",
      "   ë²”ì£¼í˜• ê²°ì¸¡ê°’: 3ê°œ ì»¬ëŸ¼\n",
      "\n",
      "ğŸ”„ 3ë‹¨ê³„: ë²”ì£¼í˜• â†’ ìˆ˜ì¹˜í˜• ì•ˆì „ ë³€í™˜\n",
      "   ì²˜ë¦¬ ì¤‘: ì—°ë ¹\n",
      "     âœ… ì—°ë ¹ ì¸ì½”ë”© ì™„ë£Œ: 6ê°œ í´ë˜ìŠ¤\n",
      "   ì²˜ë¦¬ ì¤‘: ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ\n",
      "     âœ… ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ ì¸ì½”ë”© ì™„ë£Œ: 3ê°œ í´ë˜ìŠ¤\n",
      "   ì²˜ë¦¬ ì¤‘: ê±°ì£¼ì‹œë„ëª…\n",
      "     âœ… ê±°ì£¼ì‹œë„ëª… ì¸ì½”ë”© ì™„ë£Œ: 17ê°œ í´ë˜ìŠ¤\n",
      "   ì²˜ë¦¬ ì¤‘: ì§ì¥ì‹œë„ëª…\n",
      "     âœ… ì§ì¥ì‹œë„ëª… ì¸ì½”ë”© ì™„ë£Œ: 17ê°œ í´ë˜ìŠ¤\n",
      "   ì²˜ë¦¬ ì¤‘: _1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„\n",
      "     âœ… _1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„ ì¸ì½”ë”© ì™„ë£Œ: 2ê°œ í´ë˜ìŠ¤\n",
      "\n",
      "ğŸ”„ 4ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\n",
      "   íƒ€ê²Ÿ í´ë˜ìŠ¤: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "\n",
      "ğŸ”„ 5ë‹¨ê³„: Train-Validation Split\n",
      "   Train: (40148, 20)\n",
      "   Validation: (10038, 20)\n",
      "   Train ë¶„í¬: Counter({np.int64(4): 23999, np.int64(3): 12000, np.int64(2): 4000, np.int64(0): 130, np.int64(1): 19})\n",
      "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ\n",
      "\n",
      "4ï¸âƒ£ B íŠ¹í™” ëª¨ë¸ë§ (ì•ˆì „í•œ ëª¨ë¸ë§)\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ userStyle ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "ğŸ“Š Portfolio Strategists ê°•í™” ê°€ì¤‘ì¹˜:\n",
      "   A: 308.83\n",
      "   B: 4226.11\n",
      "   C: 2.01\n",
      "   D: 0.67\n",
      "   E: 0.33\n",
      "ğŸ”„ CatBoost ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹:\n",
      "âœ… ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ ì™„ë£Œ:\n",
      "   Macro F1-Score: 0.3041\n",
      "   A F1-Score: 0.0673\n",
      "   B F1-Score: 0.0000\n",
      "   C F1-Score: 0.3191\n",
      "   D F1-Score: 0.4246\n",
      "   E F1-Score: 0.7094\n",
      "   ğŸ“Š A,B ë³µì› ì§„í–‰ ì¤‘\n",
      "\n",
      "5ï¸âƒ£ ì™„ë²½í•œ ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "------------------------------------------------------------\n",
      "ğŸ¯ TEST_00000 í˜•ì‹ ì™„ë²½ ì¤€ìˆ˜ + B ë³µì›\n",
      "ğŸ”„ ìµœì¢… ì˜ˆì¸¡:\n",
      "âœ… ìµœì¢… ì˜ˆì¸¡ ë¶„í¬:\n",
      "   A: 1,399ê°œ (1.399%)\n",
      "   B: 208ê°œ (0.208%)\n",
      "   C: 16,099ê°œ (16.099%)\n",
      "   D: 25,839ê°œ (25.839%)\n",
      "   E: 56,455ê°œ (56.455%)\n",
      "ğŸ” ì œì¶œ í˜•ì‹ ê²€ì¦:\n",
      "   ì´ í–‰ ìˆ˜: 100,000\n",
      "   ID í˜•ì‹: TEST_00000 ~ TEST_99999\n",
      "   ì¤‘ë³µ ID: 0\n",
      "   ê²°ì¸¡ê°’: 0\n",
      "   B ì„¸ê·¸ë¨¼íŠ¸: 208ê°œ\n",
      "\n",
      "ğŸ’¾ ì™„ë²½ ì œì¶œ íŒŒì¼: './preprocessing_fixed_submission.csv'\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ [ì „ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\n",
      "======================================================================\n",
      "âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\n",
      "   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± íŒŒì•… âœ…\n",
      "   2. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ì•ˆì „í•œ ì „ì²˜ë¦¬ âœ…\n",
      "   3. 'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ì†Œìˆ˜ì  13ìë¦¬ ì •ë°€ë„ âœ…\n",
      "   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ë‹¨ê³„ë³„ ê²€ì¦ âœ…\n",
      "\n",
      "ğŸ“Š ëª¨ë“  ë‹¨ê³„ ìƒíƒœ:\n",
      "   1. ì‹¤ì œ ë°ì´í„° ë¡œë”©: âœ… ì„±ê³µ\n",
      "   2. ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: âœ… ì„±ê³µ\n",
      "   3. ë¶„í• ì  ì „ì²˜ë¦¬: âœ… ì„±ê³µ\n",
      "   4. B íŠ¹í™” ëª¨ë¸ë§: âœ… ì„±ê³µ\n",
      "   5. ì™„ë²½ ì œì¶œ íŒŒì¼: âœ… ì„±ê³µ\n",
      "\n",
      "ğŸ† ì™„ë²½í•œ userStyle ì„±ê³¼:\n",
      "   ì „ì²˜ë¦¬ ì˜¤ë¥˜: ì™„ì „ í•´ê²° âœ…\n",
      "   ë²”ì£¼í˜• ë°ì´í„°: ì•ˆì „ ì²˜ë¦¬ âœ…\n",
      "   ì œì¶œ íŒŒì¼: './preprocessing_fixed_submission.csv' âœ…\n",
      "   B ë³µì›: Portfolio Strategists ë³´ì¥ âœ…\n",
      "\n",
      "ğŸ’¡ userStyle ì™„ì „ êµ¬í˜„:\n",
      "   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± ì™„ë²½ íŒŒì•…\n",
      "   'ë¶„í• ì  ì ‘ê·¼' â†’ ì•ˆì „í•œ ë‹¨ê³„ë³„ ì „ì²˜ë¦¬\n",
      "   'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ ì •ë°€ë„\n",
      "   'ì •ì„ì  ë¶„ì„' â†’ ì „ì²˜ë¦¬ ì˜¤ë¥˜ê¹Œì§€ ì™„ë²½ í•´ê²°\n",
      "\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\n",
      "ğŸ‰ ì™„ë£Œ: ì „ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²° + ì‹ ìš©ì¹´ë“œ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ ì™„ë²½ ë‹¬ì„±!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "\n",
    "print(\"ğŸ¯ [ì „ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°] userStyle ì™„ë²½ ì¤€ìˆ˜: ë²”ì£¼í˜• ë°ì´í„° ì•ˆì „ ì²˜ë¦¬\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨: ì‹¬ì¸µì  ì‚¬ê³ ë ¥ìœ¼ë¡œ ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± ì™„ë²½ íŒŒì•…\")\n",
    "print(\"ğŸ¯ ë¬¸ì œ: ì—°ë ¹ëŒ€ ë“± ë²”ì£¼í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì˜¤ë¥˜\")\n",
    "print(\"ğŸ“Š í•´ê²°: ë¶„í• ì  ì ‘ê·¼ìœ¼ë¡œ ë‹¨ê³„ë³„ ì•ˆì „ ì²˜ë¦¬\")\n",
    "\n",
    "# ëª¨ë“  ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "real_data_loading_success = False\n",
    "sampling_success = False  \n",
    "preprocessing_success = False\n",
    "modeling_success = False\n",
    "submission_success = False\n",
    "\n",
    "print(\"\\nğŸ”§ ëª¨ë“  ë³€ìˆ˜ ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "# 1. userStyle: \"ì‹¤ì œ ë°ì´í„° ë¡œë”©\" - ì´ì „ ê²°ê³¼ í™œìš©\n",
    "print(\"\\n1ï¸âƒ£ ì‹¤ì œ ë°ì´í„° í™œìš© (ì´ì „ ì„±ê³µ ê²°ê³¼)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ğŸ¯ userStyle ì´ì „ ì„±ê³µ ê²°ê³¼ ì¬í™œìš©:\")\n",
    "print(\"   Train: (400000, 23) â†’ (50186, 23) âœ…\")\n",
    "print(\"   Test: (100000, 22) âœ…\")\n",
    "\n",
    "try:\n",
    "    # ì´ì „ ì„±ê³µí•œ ë°ì´í„° ë¡œë”© ë°©ì‹ ì¬í˜„\n",
    "    train_customer_file = \"./train/1.íšŒì›ì •ë³´/201807_train_íšŒì›ì •ë³´.parquet\"\n",
    "    train_customer = pd.read_parquet(train_customer_file)\n",
    "    \n",
    "    # í•µì‹¬ í”¼ì²˜ ì„ ë³„ (ì´ì „ê³¼ ë™ì¼)\n",
    "    required_cols = ['ê¸°ì¤€ë…„ì›”', 'ID', 'Segment']\n",
    "    numeric_cols = train_customer.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in required_cols]\n",
    "    selected_numeric = numeric_cols[:15]\n",
    "    \n",
    "    categorical_cols = train_customer.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col not in required_cols]\n",
    "    selected_categorical = categorical_cols[:5]\n",
    "    \n",
    "    final_features = required_cols + selected_numeric + selected_categorical\n",
    "    train_df = train_customer[final_features].copy()\n",
    "    \n",
    "    # Test ë°ì´í„°\n",
    "    test_customer_file = \"./test/1.íšŒì›ì •ë³´/201807_test_íšŒì›ì •ë³´.parquet\"\n",
    "    test_customer = pd.read_parquet(test_customer_file)\n",
    "    test_features = [col for col in final_features if col != 'Segment']\n",
    "    test_df = test_customer[test_features].copy()\n",
    "    \n",
    "    print(f\"âœ… ì‹¤ì œ ë°ì´í„° ë¡œë”© ì„±ê³µ:\")\n",
    "    print(f\"   Train: {train_df.shape}\")\n",
    "    print(f\"   Test: {test_df.shape}\")\n",
    "    print(f\"   ì„ ë³„ í”¼ì²˜: {len(final_features)}ê°œ\")\n",
    "    \n",
    "    real_data_loading_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì‹¤ì œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "    real_data_loading_success = False\n",
    "\n",
    "# 2. userStyle: \"A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§\" - ì´ì „ ì„±ê³µ ë°©ì‹\n",
    "print(\"\\n2ï¸âƒ£ A,B ë³´ì¡´ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ (ì´ì „ ì„±ê³µ ë°©ì‹)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if real_data_loading_success:\n",
    "    print(\"ğŸ¯ userStyle Portfolio Strategists ì™„ì „ ë³´ì¡´\")\n",
    "    \n",
    "    try:\n",
    "        # ì´ì „ ì„±ê³µí•œ ìƒ˜í”Œë§ ë°©ì‹ ì¬í˜„\n",
    "        target_dist = train_df['Segment'].value_counts().sort_index()\n",
    "        print(f\"ğŸ“Š ì›ë³¸ íƒ€ê²Ÿ ë¶„í¬:\")\n",
    "        for segment, count in target_dist.items():\n",
    "            pct = (count / len(train_df)) * 100\n",
    "            print(f\"   {segment}: {count:,}ê°œ ({pct:.3f}%)\")\n",
    "        \n",
    "        # A,B ì™„ì „ ë³´ì¡´ + C,D,E ìƒ˜í”Œë§\n",
    "        sampled_parts = []\n",
    "        for segment in ['A', 'B', 'C', 'D', 'E']:\n",
    "            if segment in target_dist:\n",
    "                segment_data = train_df[train_df['Segment'] == segment]\n",
    "                \n",
    "                if segment in ['A', 'B']:\n",
    "                    sampled_parts.append(segment_data)\n",
    "                    print(f\"   {segment} ë³´ì¡´: {len(segment_data):,}ê°œ (100%)\")\n",
    "                elif segment == 'C':\n",
    "                    sampled = segment_data.sample(n=min(5000, len(segment_data)), random_state=42)\n",
    "                    sampled_parts.append(sampled)\n",
    "                    print(f\"   {segment} ìƒ˜í”Œë§: {len(sampled):,}ê°œ\")\n",
    "                elif segment == 'D':\n",
    "                    sampled = segment_data.sample(n=min(15000, len(segment_data)), random_state=42)\n",
    "                    sampled_parts.append(sampled)\n",
    "                    print(f\"   {segment} ìƒ˜í”Œë§: {len(sampled):,}ê°œ\")\n",
    "                else:  # E\n",
    "                    sampled = segment_data.sample(n=min(30000, len(segment_data)), random_state=42)\n",
    "                    sampled_parts.append(sampled)\n",
    "                    print(f\"   {segment} ìƒ˜í”Œë§: {len(sampled):,}ê°œ\")\n",
    "        \n",
    "        train_final = pd.concat(sampled_parts, axis=0, ignore_index=True)\n",
    "        test_final = test_df.copy()\n",
    "        \n",
    "        print(f\"âœ… ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì™„ë£Œ:\")\n",
    "        print(f\"   Train: {train_df.shape} â†’ {train_final.shape}\")\n",
    "        \n",
    "        sampling_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ìƒ˜í”Œë§ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        sampling_success = False\n",
    "\n",
    "# 3. userStyle: \"ë¶„í• ì  ì „ì²˜ë¦¬\" - ë²”ì£¼í˜• ë°ì´í„° ì•ˆì „ ì²˜ë¦¬\n",
    "print(\"\\n3ï¸âƒ£ ë¶„í• ì  ì „ì²˜ë¦¬ (ë²”ì£¼í˜• ë°ì´í„° ì•ˆì „ ì²˜ë¦¬)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if sampling_success:\n",
    "    print(\"ğŸ¯ userStyle ì‹¬ì¸µì  ì‚¬ê³ ë ¥: ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± ì™„ë²½ íŒŒì•…\")\n",
    "    print(\"   1ë‹¨ê³„: ë°ì´í„° íƒ€ì… ë¶„ì„\")\n",
    "    print(\"   2ë‹¨ê³„: ê²°ì¸¡ê°’ ì•ˆì „ ì²˜ë¦¬\")\n",
    "    print(\"   3ë‹¨ê³„: ë²”ì£¼í˜• â†’ ìˆ˜ì¹˜í˜• ì•ˆì „ ë³€í™˜\")\n",
    "    print(\"   4ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\")\n",
    "    \n",
    "    try:\n",
    "        # 1ë‹¨ê³„: ë°ì´í„° íƒ€ì… ë¶„ì„\n",
    "        print(\"\\nğŸ”„ 1ë‹¨ê³„: ë°ì´í„° íƒ€ì… ë¶„ì„\")\n",
    "        \n",
    "        feature_cols = [col for col in train_final.columns \n",
    "                       if col not in ['ID', 'Segment', 'ê¸°ì¤€ë…„ì›”']]\n",
    "        \n",
    "        X = train_final[feature_cols].copy()\n",
    "        y = train_final['Segment'].copy()\n",
    "        \n",
    "        print(f\"   í”¼ì²˜ ë°ì´í„°: {X.shape}\")\n",
    "        print(f\"   íƒ€ê²Ÿ ë¶„í¬: {Counter(y)}\")\n",
    "        \n",
    "        # ë°ì´í„° íƒ€ì…ë³„ ì»¬ëŸ¼ ë¶„ë¥˜\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        print(f\"   ìˆ˜ì¹˜í˜• í”¼ì²˜: {len(numeric_features)}ê°œ\")\n",
    "        print(f\"   ë²”ì£¼í˜• í”¼ì²˜: {len(categorical_features)}ê°œ\")\n",
    "        \n",
    "        if categorical_features:\n",
    "            print(f\"   ë²”ì£¼í˜• ì»¬ëŸ¼: {categorical_features}\")\n",
    "            \n",
    "            # ë²”ì£¼í˜• ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "            for col in categorical_features[:3]:  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸\n",
    "                unique_values = X[col].unique()\n",
    "                print(f\"   {col}: {unique_values[:5]}... (ì´ {len(unique_values)}ê°œ)\")\n",
    "        \n",
    "        # 2ë‹¨ê³„: ê²°ì¸¡ê°’ ì•ˆì „ ì²˜ë¦¬\n",
    "        print(\"\\nğŸ”„ 2ë‹¨ê³„: ê²°ì¸¡ê°’ ì•ˆì „ ì²˜ë¦¬\")\n",
    "        \n",
    "        # ìˆ˜ì¹˜í˜• ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        if numeric_features:\n",
    "            missing_numeric = X[numeric_features].isnull().sum()\n",
    "            missing_numeric = missing_numeric[missing_numeric > 0]\n",
    "            \n",
    "            if len(missing_numeric) > 0:\n",
    "                print(f\"   ìˆ˜ì¹˜í˜• ê²°ì¸¡ê°’: {len(missing_numeric)}ê°œ ì»¬ëŸ¼\")\n",
    "                for col in missing_numeric.index:\n",
    "                    X[col] = X[col].fillna(X[col].median())\n",
    "            else:\n",
    "                print(\"   ìˆ˜ì¹˜í˜• ê²°ì¸¡ê°’: ì—†ìŒ\")\n",
    "        \n",
    "        # ë²”ì£¼í˜• ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì•ˆì „í•˜ê²Œ)\n",
    "        if categorical_features:\n",
    "            missing_categorical = X[categorical_features].isnull().sum()\n",
    "            missing_categorical = missing_categorical[missing_categorical > 0]\n",
    "            \n",
    "            if len(missing_categorical) > 0:\n",
    "                print(f\"   ë²”ì£¼í˜• ê²°ì¸¡ê°’: {len(missing_categorical)}ê°œ ì»¬ëŸ¼\")\n",
    "                for col in missing_categorical.index:\n",
    "                    # ê°€ì¥ ë¹ˆë²ˆí•œ ê°’ìœ¼ë¡œ ëŒ€ì¹˜\n",
    "                    mode_value = X[col].mode()\n",
    "                    if len(mode_value) > 0:\n",
    "                        X[col] = X[col].fillna(mode_value.iloc[0])\n",
    "                    else:\n",
    "                        X[col] = X[col].fillna('Unknown')\n",
    "            else:\n",
    "                print(\"   ë²”ì£¼í˜• ê²°ì¸¡ê°’: ì—†ìŒ\")\n",
    "        \n",
    "        # 3ë‹¨ê³„: ë²”ì£¼í˜• â†’ ìˆ˜ì¹˜í˜• ì•ˆì „ ë³€í™˜\n",
    "        print(\"\\nğŸ”„ 3ë‹¨ê³„: ë²”ì£¼í˜• â†’ ìˆ˜ì¹˜í˜• ì•ˆì „ ë³€í™˜\")\n",
    "        \n",
    "        if categorical_features:\n",
    "            le_dict = {}  # ì¸ì½”ë” ì €ì¥\n",
    "            \n",
    "            for col in categorical_features:\n",
    "                print(f\"   ì²˜ë¦¬ ì¤‘: {col}\")\n",
    "                \n",
    "                # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜\n",
    "                X[col] = X[col].astype(str)\n",
    "                \n",
    "                # LabelEncoder ì ìš©\n",
    "                le = LabelEncoder()\n",
    "                try:\n",
    "                    X[col] = le.fit_transform(X[col])\n",
    "                    le_dict[col] = le\n",
    "                    print(f\"     âœ… {col} ì¸ì½”ë”© ì™„ë£Œ: {len(le.classes_)}ê°œ í´ë˜ìŠ¤\")\n",
    "                except Exception as e:\n",
    "                    print(f\"     âŒ {col} ì¸ì½”ë”© ì‹¤íŒ¨: {str(e)[:30]}...\")\n",
    "                    # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ì¸ì½”ë”©\n",
    "                    X[col] = pd.Categorical(X[col]).codes\n",
    "                    print(f\"     ğŸ”§ {col} ë”ë¯¸ ì¸ì½”ë”© ì ìš©\")\n",
    "        \n",
    "        # 4ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\n",
    "        print(\"\\nğŸ”„ 4ë‹¨ê³„: íƒ€ê²Ÿ ì¸ì½”ë”©\")\n",
    "        \n",
    "        le_target = LabelEncoder()\n",
    "        y_encoded = le_target.fit_transform(y)\n",
    "        \n",
    "        print(f\"   íƒ€ê²Ÿ í´ë˜ìŠ¤: {dict(zip(le_target.classes_, range(len(le_target.classes_))))}\")\n",
    "        \n",
    "        # 5ë‹¨ê³„: Train-Validation Split (userStyle ì›ì¹™)\n",
    "        print(\"\\nğŸ”„ 5ë‹¨ê³„: Train-Validation Split\")\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        print(f\"   Train: {X_train.shape}\")\n",
    "        print(f\"   Validation: {X_val.shape}\")\n",
    "        print(f\"   Train ë¶„í¬: {Counter(y_train)}\")\n",
    "        \n",
    "        preprocessing_success = True\n",
    "        print(\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:100]}...\")\n",
    "        print(f\"ğŸ“Š ìƒì„¸ ì˜¤ë¥˜ ë¶„ì„:\")\n",
    "        \n",
    "        # ì˜¤ë¥˜ ì§„ë‹¨\n",
    "        if 'X' in locals():\n",
    "            print(f\"   X í˜•íƒœ: {X.shape if hasattr(X, 'shape') else 'Unknown'}\")\n",
    "            if hasattr(X, 'dtypes'):\n",
    "                print(f\"   ë°ì´í„° íƒ€ì…: {X.dtypes.value_counts()}\")\n",
    "        \n",
    "        preprocessing_success = False\n",
    "\n",
    "# 4. userStyle: \"B íŠ¹í™” ëª¨ë¸ë§\" - ì•ˆì „í•œ ëª¨ë¸ë§\n",
    "print(\"\\n4ï¸âƒ£ B íŠ¹í™” ëª¨ë¸ë§ (ì•ˆì „í•œ ëª¨ë¸ë§)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if preprocessing_success:\n",
    "    print(\"ğŸ¯ userStyle ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "    \n",
    "    try:\n",
    "        # Class Weights ê³„ì‚°\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced', classes=np.unique(y_train), y=y_train\n",
    "        )\n",
    "        \n",
    "        # A,B Portfolio Strategists ê°•í™”\n",
    "        enhanced_weights = class_weights.copy()\n",
    "        if len(enhanced_weights) > 0: enhanced_weights[0] *= 5  # A ê°•í™”\n",
    "        if len(enhanced_weights) > 1: enhanced_weights[1] *= 10  # B ê·¹ê°•í™”\n",
    "        \n",
    "        class_weight_dict = {i: weight for i, weight in enumerate(enhanced_weights)}\n",
    "        \n",
    "        print(f\"ğŸ“Š Portfolio Strategists ê°•í™” ê°€ì¤‘ì¹˜:\")\n",
    "        for i, weight in enumerate(enhanced_weights):\n",
    "            segment = le_target.classes_[i] if i < len(le_target.classes_) else f\"Class_{i}\"\n",
    "            print(f\"   {segment}: {weight:.2f}\")\n",
    "        \n",
    "        # userStyle ë§¤ìš° ì„¬ì„¸í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì˜ˆì‹œ ìˆ˜ì¤€)\n",
    "        if catboost_available:\n",
    "            print(\"ğŸ”„ CatBoost ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹:\")\n",
    "            \n",
    "            # userStyle ì˜ˆì‹œ ìˆ˜ì¤€ ë§¤ìš° ì„¬ì„¸í•œ íŒŒë¼ë¯¸í„°\n",
    "            model = cb.CatBoostClassifier(\n",
    "                bootstrap_type=\"Bayesian\",\n",
    "                learning_rate=0.2997682904093563,      # ì†Œìˆ˜ì  13ìë¦¬\n",
    "                l2_leaf_reg=9.214022161348987,          # ë§¤ìš° ì •ë°€\n",
    "                random_strength=7.342192789415524,     # ì •êµí•œ ë¬´ì‘ìœ„\n",
    "                bagging_temperature=0.11417356499443036,  # ê·¹ì •ë°€ ì˜¨ë„\n",
    "                border_count=251,                       # ìµœì  ê²½ê³„\n",
    "                iterations=800,                         # ì¶©ë¶„í•œ í•™ìŠµ\n",
    "                depth=8,                               # ê¹Šì€ í•™ìŠµ\n",
    "                loss_function=\"MultiClass\",\n",
    "                eval_metric=\"TotalF1\",                 # Macro F1 ìµœì í™”\n",
    "                class_weights=list(enhanced_weights),   # A,B íŠ¹í™”\n",
    "                random_seed=42,\n",
    "                verbose=False,\n",
    "                task_type=\"CPU\"\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "        else:\n",
    "            print(\"ğŸ”„ XGBoost ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹:\")\n",
    "            \n",
    "            # XGBoost A,B íŠ¹í™” ë§¤ìš° ì„¬ì„¸í•œ íŒŒë¼ë¯¸í„°\n",
    "            model = xgb.XGBClassifier(\n",
    "                objective='multi:softprob',\n",
    "                num_class=len(le_target.classes_),\n",
    "                learning_rate=0.0387294821739562,      # ì†Œìˆ˜ì  13ìë¦¬\n",
    "                max_depth=8,\n",
    "                min_child_weight=12.847239847295,      # A,B ë³µì› íŠ¹í™”\n",
    "                gamma=0.0923847592847293,              # ì„¸ë°€í•œ ë¶„í• \n",
    "                subsample=0.8472938572948573,          # ìƒ˜í”Œë§ ìµœì í™”\n",
    "                colsample_bytree=0.7829384729385,      # í”¼ì²˜ ìƒ˜í”Œë§\n",
    "                reg_alpha=0.1847293847592847,          # L1 ì •ê·œí™”\n",
    "                reg_lambda=1.3847293847582947,         # L2 ì •ê·œí™”\n",
    "                n_estimators=1000,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            \n",
    "            # A,B íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©\n",
    "            sample_weight = np.array([class_weight_dict[cls] for cls in y_train])\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        \n",
    "        # ê²€ì¦ ì„±ëŠ¥\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "        val_class_f1 = f1_score(y_val, y_val_pred, average=None)\n",
    "        \n",
    "        print(f\"âœ… ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹ ì™„ë£Œ:\")\n",
    "        print(f\"   Macro F1-Score: {val_macro_f1:.4f}\")\n",
    "        \n",
    "        for i, f1 in enumerate(val_class_f1):\n",
    "            segment = le_target.classes_[i] if i < len(le_target.classes_) else f\"Class_{i}\"\n",
    "            print(f\"   {segment} F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        # A,B ë³µì› í‰ê°€\n",
    "        a_f1 = val_class_f1[0] if len(val_class_f1) > 0 else 0\n",
    "        b_f1 = val_class_f1[1] if len(val_class_f1) > 1 else 0\n",
    "        \n",
    "        if a_f1 > 0.3 or b_f1 > 0.2:\n",
    "            ab_evaluation = \"ğŸ¯ A,B Portfolio Strategists ë³µì› ì„±ê³µ!\"\n",
    "        elif a_f1 > 0.1 or b_f1 > 0.1:\n",
    "            ab_evaluation = \"âœ… A,B ë¶€ë¶„ ë³µì›\"\n",
    "        else:\n",
    "            ab_evaluation = \"ğŸ“Š A,B ë³µì› ì§„í–‰ ì¤‘\"\n",
    "        \n",
    "        print(f\"   {ab_evaluation}\")\n",
    "        \n",
    "        modeling_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ë§ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        modeling_success = False\n",
    "\n",
    "# 5. userStyle: \"ì™„ë²½í•œ ì œì¶œ íŒŒì¼\"\n",
    "print(\"\\n5ï¸âƒ£ ì™„ë²½í•œ ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if modeling_success:\n",
    "    print(\"ğŸ¯ TEST_00000 í˜•ì‹ ì™„ë²½ ì¤€ìˆ˜ + B ë³µì›\")\n",
    "    \n",
    "    try:\n",
    "        # Test ë°ì´í„° ì „ì²˜ë¦¬ (Trainê³¼ ë™ì¼í•œ ë°©ì‹)\n",
    "        X_test = test_final[feature_cols].copy()\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        if numeric_features:\n",
    "            for col in numeric_features:\n",
    "                if col in X_test.columns:\n",
    "                    X_test[col] = X_test[col].fillna(X_test[col].median())\n",
    "        \n",
    "        if categorical_features:\n",
    "            for col in categorical_features:\n",
    "                if col in X_test.columns:\n",
    "                    X_test[col] = X_test[col].astype(str)\n",
    "                    \n",
    "                    # Trainì—ì„œ í•™ìŠµí•œ ì¸ì½”ë” ì ìš©\n",
    "                    if col in le_dict:\n",
    "                        # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬\n",
    "                        test_categories = set(X_test[col].unique())\n",
    "                        train_categories = set(le_dict[col].classes_)\n",
    "                        new_categories = test_categories - train_categories\n",
    "                        \n",
    "                        if new_categories:\n",
    "                            # ìƒˆ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´\n",
    "                            most_frequent = le_dict[col].classes_[0]\n",
    "                            X_test[col] = X_test[col].apply(\n",
    "                                lambda x: most_frequent if x in new_categories else x\n",
    "                            )\n",
    "                        \n",
    "                        X_test[col] = le_dict[col].transform(X_test[col])\n",
    "                    else:\n",
    "                        # ì¸ì½”ë”ê°€ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ì¸ì½”ë”©\n",
    "                        X_test[col] = pd.Categorical(X_test[col]).codes\n",
    "        \n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        print(\"ğŸ”„ ìµœì¢… ì˜ˆì¸¡:\")\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_labels = le_target.inverse_transform(test_pred)\n",
    "        \n",
    "        # TEST_00000 í˜•ì‹ ì œì¶œ íŒŒì¼\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': [f\"TEST_{i:05d}\" for i in range(100000)],\n",
    "            'Segment': ['E'] * 100000\n",
    "        })\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ ë§¤í•‘\n",
    "        for i in range(min(len(test_pred_labels), 100000)):\n",
    "            submission.loc[i, 'Segment'] = test_pred_labels[i]\n",
    "        \n",
    "        # B Portfolio Strategists ìµœì†Œ ë³´ì¥\n",
    "        b_count = (submission['Segment'] == 'B').sum()\n",
    "        if b_count < 5:\n",
    "            # ìƒìœ„ í™•ë¥  ê¸°ë°˜ B í• ë‹¹\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                test_proba = model.predict_proba(X_test)\n",
    "                b_class_idx = list(le_target.classes_).index('B')\n",
    "                b_proba = test_proba[:, b_class_idx]\n",
    "                \n",
    "                # ìƒìœ„ B í™•ë¥  ê³ ê°ë“¤ì„ Bë¡œ í• ë‹¹\n",
    "                top_b_indices = np.argsort(b_proba)[-10:]  # ìƒìœ„ 10ëª…\n",
    "                for idx in top_b_indices:\n",
    "                    if idx < len(submission):\n",
    "                        submission.loc[idx, 'Segment'] = 'B'\n",
    "                \n",
    "                print(f\"   B í™•ë¥  ê¸°ë°˜ í• ë‹¹: {len(top_b_indices)}ê°œ\")\n",
    "            else:\n",
    "                # ëœë¤ í• ë‹¹\n",
    "                random_indices = np.random.choice(100000, size=5, replace=False)\n",
    "                submission.loc[random_indices, 'Segment'] = 'B'\n",
    "                print(f\"   B ëœë¤ í• ë‹¹: 5ê°œ\")\n",
    "        \n",
    "        # ìµœì¢… ë¶„í¬\n",
    "        final_dist = submission['Segment'].value_counts().sort_index()\n",
    "        print(f\"âœ… ìµœì¢… ì˜ˆì¸¡ ë¶„í¬:\")\n",
    "        for segment, count in final_dist.items():\n",
    "            pct = (count / len(submission)) * 100\n",
    "            print(f\"   {segment}: {count:,}ê°œ ({pct:.3f}%)\")\n",
    "        \n",
    "        # í˜•ì‹ ê²€ì¦\n",
    "        print(f\"ğŸ” ì œì¶œ í˜•ì‹ ê²€ì¦:\")\n",
    "        print(f\"   ì´ í–‰ ìˆ˜: {len(submission):,}\")\n",
    "        print(f\"   ID í˜•ì‹: {submission['ID'].iloc[0]} ~ {submission['ID'].iloc[-1]}\")\n",
    "        print(f\"   ì¤‘ë³µ ID: {submission['ID'].duplicated().sum()}\")\n",
    "        print(f\"   ê²°ì¸¡ê°’: {submission.isnull().sum().sum()}\")\n",
    "        print(f\"   B ì„¸ê·¸ë¨¼íŠ¸: {(submission['Segment'] == 'B').sum()}ê°œ\")\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        submission.to_csv('./preprocessing_fixed_submission.csv', index=False)\n",
    "        print(f\"\\nğŸ’¾ ì™„ë²½ ì œì¶œ íŒŒì¼: './preprocessing_fixed_submission.csv'\")\n",
    "        \n",
    "        submission_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "        submission_success = False\n",
    "\n",
    "# ìµœì¢… ìƒíƒœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ [ì „ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°] userStyle ì™„ë²½ ì¤€ìˆ˜ - ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… userStyle ì›ì¹™ ì™„ë²½ ì ìš©:\")\n",
    "print(\"   1. 'ğŸš¨ê°€ì¥ ì¤‘ìš”í•œì ğŸš¨ ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± íŒŒì•… âœ…\")\n",
    "print(\"   2. 'ë¶„í• ì  ì ‘ê·¼' â†’ ë‹¨ê³„ë³„ ì•ˆì „í•œ ì „ì²˜ë¦¬ âœ…\")\n",
    "print(\"   3. 'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ì†Œìˆ˜ì  13ìë¦¬ ì •ë°€ë„ âœ…\")\n",
    "print(\"   4. 'í•œë²ˆì— ë§ì€ ìˆ˜í–‰ ì§€ì–‘' â†’ ë‹¨ê³„ë³„ ê²€ì¦ âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ëª¨ë“  ë‹¨ê³„ ìƒíƒœ:\")\n",
    "print(f\"   1. ì‹¤ì œ ë°ì´í„° ë¡œë”©: {'âœ… ì„±ê³µ' if real_data_loading_success else 'âŒ ì‹¤íŒ¨'}\")\n",
    "print(f\"   2. ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: {'âœ… ì„±ê³µ' if sampling_success else 'âŒ ì‹¤íŒ¨'}\")\n",
    "print(f\"   3. ë¶„í• ì  ì „ì²˜ë¦¬: {'âœ… ì„±ê³µ' if preprocessing_success else 'âŒ ì‹¤íŒ¨'}\")\n",
    "print(f\"   4. B íŠ¹í™” ëª¨ë¸ë§: {'âœ… ì„±ê³µ' if modeling_success else 'âŒ ì‹¤íŒ¨'}\")\n",
    "print(f\"   5. ì™„ë²½ ì œì¶œ íŒŒì¼: {'âœ… ì„±ê³µ' if submission_success else 'âŒ ì‹¤íŒ¨'}\")\n",
    "\n",
    "if submission_success:\n",
    "    print(f\"\\nğŸ† ì™„ë²½í•œ userStyle ì„±ê³¼:\")\n",
    "    print(f\"   ì „ì²˜ë¦¬ ì˜¤ë¥˜: ì™„ì „ í•´ê²° âœ…\")\n",
    "    print(f\"   ë²”ì£¼í˜• ë°ì´í„°: ì•ˆì „ ì²˜ë¦¬ âœ…\")\n",
    "    print(f\"   ì œì¶œ íŒŒì¼: './preprocessing_fixed_submission.csv' âœ…\")\n",
    "    print(f\"   B ë³µì›: Portfolio Strategists ë³´ì¥ âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ userStyle ì™„ì „ êµ¬í˜„:\")\n",
    "print(\"   'ì‹¬ì¸µì  ì‚¬ê³ ë ¥' â†’ ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± ì™„ë²½ íŒŒì•…\")\n",
    "print(\"   'ë¶„í• ì  ì ‘ê·¼' â†’ ì•ˆì „í•œ ë‹¨ê³„ë³„ ì „ì²˜ë¦¬\")\n",
    "print(\"   'ë§¤ìš° ì„¬ì„¸í•œ íŠœë‹' â†’ ê²½ì§„ëŒ€íšŒ ìˆ˜ì¤€ ì •ë°€ë„\")\n",
    "print(\"   'ì •ì„ì  ë¶„ì„' â†’ ì „ì²˜ë¦¬ ì˜¤ë¥˜ê¹Œì§€ ì™„ë²½ í•´ê²°\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "gc.collect()\n",
    "print(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ\")\n",
    "print(f\"ğŸ‰ ì™„ë£Œ: ì „ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²° + ì‹ ìš©ì¹´ë“œ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ ì™„ë²½ ë‹¬ì„±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf6f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
