{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896dabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost==1.2.8\n",
    "!pip install optuna==4.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb87550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost\n",
    "import sklearn\n",
    "import optuna\n",
    "\n",
    "versions = {\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"catboost\": catboost.__version__,\n",
    "    \"sklearn\": sklearn.__version__,\n",
    "    \"optuna\": optuna.__version__,\n",
    "}\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf25f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt\n",
    "# íŒŒì¼ ì •ë¦¬ì—ëŠ” v2-8 TPUì„ ì‚¬ìš©.\n",
    "\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_folder = '/content/drive/MyDrive/base_file/'\n",
    "\n",
    "months = [\"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "categories = [\"íšŒì›ì •ë³´\", \"ì‹ ìš©ì •ë³´\", \"ìŠ¹ì¸ë§¤ì¶œì •ë³´\", \"ì²­êµ¬ì…ê¸ˆì •ë³´\", \"ì”ì•¡ì •ë³´\", \"ì±„ë„ì •ë³´\", \"ë§ˆì¼€íŒ…ì •ë³´\", \"ì„±ê³¼ì •ë³´\"]\n",
    "data_types = [\"train\", \"test\"]\n",
    "\n",
    "def merge_monthly_data(data_type, category):\n",
    "    merged_list = []\n",
    "\n",
    "    for month in months:\n",
    "        file_name = f\"{drive_folder}2018{month}_{data_type}_{category}.parquet\"\n",
    "        try:\n",
    "            df = pd.read_parquet(file_name, engine=\"pyarrow\")\n",
    "            merged_list.append(df)\n",
    "            print(f\"âœ… {file_name} ë³€í™˜ ì™„ë£Œ\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_name}\")\n",
    "\n",
    "    if merged_list:\n",
    "        merged_df = pd.concat(merged_list, ignore_index=True)\n",
    "        output_file = f\"{drive_folder}{data_type}_{category}.csv\"\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"âœ… {output_file} ì €ì¥ ì™„ë£Œ (Shape: {merged_df.shape})\")\n",
    "    else:\n",
    "        print(f\"âŒ {data_type}_{category} ë°ì´í„° ì—†ìŒ\")\n",
    "\n",
    "for data_type in data_types:\n",
    "    for category in categories:\n",
    "        merge_monthly_data(data_type, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/base_file/\"\n",
    "\n",
    "file_names = [\n",
    "    \"train_íšŒì›ì •ë³´.csv\",\n",
    "    \"train_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"train_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"train_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"train_ì±„ë„ì •ë³´.csv\",\n",
    "    \"train_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"train_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(base_path + file_names[0])\n",
    "\n",
    "for idx, file in enumerate(file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì¤‘: {file} ({idx}/{len(file_names)})\")\n",
    "    temp_df = pd.read_csv(base_path + file)\n",
    "\n",
    "    df = df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ í¬ê¸°: {df.shape}\")\n",
    "\n",
    "output_file = base_path + \"base_train.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "print(f\"ğŸ§¾ ìµœì¢… ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/base_file/\"\n",
    "\n",
    "file_names = [\n",
    "    \"test_íšŒì›ì •ë³´.csv\",\n",
    "    \"test_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"test_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"test_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"test_ì±„ë„ì •ë³´.csv\",\n",
    "    \"test_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"test_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(base_path + file_names[0])\n",
    "\n",
    "for idx, file in enumerate(file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì¤‘: {file} ({idx}/{len(file_names)})\")\n",
    "    temp_df = pd.read_csv(base_path + file)\n",
    "\n",
    "    df = df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ í¬ê¸°: {df.shape}\")\n",
    "\n",
    "output_file = base_path + \"base_test.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "print(f\"ğŸ§¾ ìµœì¢… ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/base_file/\"\n",
    "\n",
    "file_names = [\n",
    "    \"train_íšŒì›ì •ë³´.csv\",\n",
    "    \"train_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"train_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"train_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"train_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"train_ì±„ë„ì •ë³´.csv\",\n",
    "    \"train_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"train_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(base_path + file_names[0])\n",
    "\n",
    "original_shape = df.shape\n",
    "\n",
    "for idx, file in enumerate(file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì§„í–‰ ì¤‘: {file} (íŒŒì¼ {idx} / {len(file_names)})\")\n",
    "\n",
    "    temp_df = pd.read_csv(base_path + file)\n",
    "    df = df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")\n",
    "\n",
    "    constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    if constant_cols:\n",
    "        print(f\"ğŸ“Œ ì œê±°ëœ ëª¨ë“  ê°’ì´ ë™ì¼í•œ ì¹¼ëŸ¼: {constant_cols}\")\n",
    "        df = df.drop(columns=constant_cols)\n",
    "    else:\n",
    "        print(\"ğŸ“Œ ëª¨ë“  ê°’ì´ ë™ì¼í•œ ì¹¼ëŸ¼ ì—†ìŒ\")\n",
    "\n",
    "    col_groups = {}\n",
    "    for col in df.columns:\n",
    "        for key in col_groups:\n",
    "            if df[col].equals(df[key]):\n",
    "                col_groups[key].append(col)\n",
    "                break\n",
    "        else:\n",
    "            col_groups[col] = [col]\n",
    "\n",
    "    duplicate_cols = [col for group in col_groups.values() for col in group[1:]]\n",
    "    if duplicate_cols:\n",
    "        print(f\"ğŸ“Œ ì œê±°ëœ ì¤‘ë³µ ì¹¼ëŸ¼: {duplicate_cols}\")\n",
    "        df = df.drop(columns=duplicate_cols)\n",
    "    else:\n",
    "        print(\"ğŸ“Œ ì¤‘ë³µ ì¹¼ëŸ¼ ì—†ìŒ\")\n",
    "\n",
    "    if 'ID' in df.columns and df.columns.str.contains('ID').sum() > 1:\n",
    "        df = df.loc[:, ~df.columns.str.contains('ID', case=False)].join(df[['ID']])\n",
    "\n",
    "    if 'ê¸°ì¤€ë…„ì›”' in df.columns and df.columns.str.contains('ê¸°ì¤€ë…„ì›”').sum() > 1:\n",
    "        df = df.loc[:, ~df.columns.str.contains('ê¸°ì¤€ë…„ì›”', case=False)].join(df[['ê¸°ì¤€ë…„ì›”']])\n",
    "\n",
    "    print(f\"ğŸ”¹ {file} ì²˜ë¦¬ ì™„ë£Œ. í˜„ì¬ ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰, {df.shape[1]}ì—´\")\n",
    "\n",
    "new_shape = df.shape\n",
    "\n",
    "output_file = base_path + \"base_clean_train.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì›ë˜ ë°ì´í„° í¬ê¸°: {original_shape[0]}í–‰, {original_shape[1]}ì—´\")\n",
    "print(f\"âœ… ë³‘í•© í›„ ìµœì¢… ë°ì´í„° í¬ê¸°: {new_shape[0]}í–‰, {new_shape[1]}ì—´\")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/base_file/\"\n",
    "\n",
    "test_file_names = [\n",
    "    \"test_íšŒì›ì •ë³´.csv\",\n",
    "    \"test_ì‹ ìš©ì •ë³´.csv\",\n",
    "    \"test_ìŠ¹ì¸ë§¤ì¶œì •ë³´.csv\",\n",
    "    \"test_ì²­êµ¬ì…ê¸ˆì •ë³´.csv\",\n",
    "    \"test_ì”ì•¡ì •ë³´.csv\",\n",
    "    \"test_ì±„ë„ì •ë³´.csv\",\n",
    "    \"test_ë§ˆì¼€íŒ…ì •ë³´.csv\",\n",
    "    \"test_ì„±ê³¼ì •ë³´.csv\"\n",
    "]\n",
    "\n",
    "test_df = pd.read_csv(base_path + test_file_names[0])\n",
    "\n",
    "test_original_shape = test_df.shape\n",
    "for idx, file in enumerate(test_file_names[1:], start=2):\n",
    "    print(f\"\\nğŸ”¹ ë³‘í•© ì§„í–‰ ì¤‘: {file} (íŒŒì¼ {idx} / {len(test_file_names)})\")\n",
    "    temp_df = pd.read_csv(base_path + file)\n",
    "    test_df = test_df.merge(temp_df, how=\"left\", on=[\"ID\", \"ê¸°ì¤€ë…„ì›”\"])\n",
    "    print(f\"âœ… ë³‘í•© í›„ ë°ì´í„° í¬ê¸°: {test_df.shape[0]}í–‰, {test_df.shape[1]}ì—´\")\n",
    "\n",
    "train_df = pd.read_csv(base_path + \"base_clean_train.csv\", nrows=1)\n",
    "train_columns = train_df.columns\n",
    "test_columns_to_keep = [col for col in test_df.columns if col in train_columns]\n",
    "\n",
    "test_df = test_df[test_columns_to_keep]\n",
    "test_final_shape = test_df.shape\n",
    "\n",
    "test_output_file = base_path + \"base_clean_test.csv\"\n",
    "test_df.to_csv(test_output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì›ë˜ test ë°ì´í„° í¬ê¸°: {test_original_shape[0]}í–‰, {test_original_shape[1]}ì—´\")\n",
    "print(f\"âœ… ë³‘í•© í›„ ìµœì¢… test ë°ì´í„° í¬ê¸°: {test_final_shape[0]}í–‰, {test_final_shape[1]}ì—´\")\n",
    "print(f\"\\nâœ… ìµœì¢… test ë°ì´í„° ì €ì¥ ì™„ë£Œ: {test_output_file}\")\n",
    "\n",
    "train_col_set = set(train_columns)\n",
    "test_col_set = set(test_df.columns)\n",
    "\n",
    "if train_col_set == test_col_set:\n",
    "    print(\"\\nâœ… trainê³¼ testì˜ ì»¬ëŸ¼ì´ ì™„ì „íˆ ì¼ì¹˜í•©ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    train_only_cols = train_col_set - test_col_set\n",
    "    test_only_cols = test_col_set - train_col_set\n",
    "\n",
    "    print(f\"\\nâš ï¸ trainê³¼ testì˜ ì»¬ëŸ¼ì´ ë‹¤ë¦…ë‹ˆë‹¤!\")\n",
    "    print(f\"ğŸ”¹ trainì—ë§Œ ìˆëŠ” ì»¬ëŸ¼ ({len(train_only_cols)}ê°œ): {train_only_cols}\")\n",
    "    print(f\"ğŸ”¹ testì—ë§Œ ìˆëŠ” ì»¬ëŸ¼ ({len(test_only_cols)}ê°œ): {test_only_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d84c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE modelì—ëŠ” A100 GPUì„ ì‚¬ìš©.\n",
    "\n",
    "!pip install catboost==1.2.8\n",
    "!pip install optuna==4.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aac614",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36603333",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/base_file/base_clean_train.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/base_file/base_clean_test.csv')\n",
    "\n",
    "ab_ids = train[train['Segment'].isin(['A', 'B'])]['ID'].unique()\n",
    "\n",
    "train = train[~train['ID'].isin(ab_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbea0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train['Segment'] = label_encoder.fit_transform(train['Segment'])\n",
    "\n",
    "X = train.drop(columns=['Segment', 'ID'])\n",
    "y = train['Segment']\n",
    "X_test = test.drop(columns=['ID'])\n",
    "\n",
    "cat_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "best_params = {\n",
    "    \"bootstrap_type\": \"Bayesian\",\n",
    "    \"learning_rate\": 0.2997682904093563,\n",
    "    \"l2_leaf_reg\": 9.214022161348987,\n",
    "    \"random_strength\": 7.342192789415524,\n",
    "    \"bagging_temperature\": 0.11417356499443036,\n",
    "    \"border_count\": 251,\n",
    "    \"iterations\": 1500,\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"eval_metric\": \"TotalF1\",\n",
    "    \"task_type\": \"GPU\",\n",
    "    \"verbose\": 100,\n",
    "    \"random_seed\": 42,\n",
    "    \"depth\": 8,\n",
    "    \"class_weights\": [2, 1, 1]\n",
    "}\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_test_probs = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"ğŸš€ Fold {fold+1} training...\")\n",
    "\n",
    "    X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_valid_fold, y_valid_fold = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "\n",
    "    model = CatBoostClassifier(**best_params)\n",
    "    model.fit(X_train_fold, y_train_fold, cat_features=cat_features)\n",
    "\n",
    "    fold_probs = model.predict_proba(X_test)\n",
    "    all_test_probs += fold_probs\n",
    "\n",
    "avg_test_probs = all_test_probs / kf.get_n_splits()\n",
    "prob_df = pd.DataFrame(avg_test_probs, columns=range(n_classes))\n",
    "prob_df['ID'] = test['ID'].values\n",
    "\n",
    "mean_probs = prob_df.groupby('ID').mean().reset_index()\n",
    "mean_probs['Segment'] = mean_probs.drop(columns='ID').values.argmax(axis=1)\n",
    "\n",
    "segment_mapping = {0: 'C', 1: 'D', 2: 'E'}\n",
    "mean_probs['Segment'] = mean_probs['Segment'].map(segment_mapping)\n",
    "\n",
    "submission = pd.DataFrame({'ID': mean_probs['ID'], 'Segment': mean_probs['Segment']})\n",
    "submission.to_csv('/content/drive/MyDrive/base_file/base_catboost_kfold.csv', index=False)\n",
    "\n",
    "print(\"âœ… CatBoost + 10-Fold CV ì˜ˆì¸¡ ì™„ë£Œ ë° ì €ì¥ ğŸ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIP modelì—ëŠ” ì¬í˜„ì„± ìœ ì§€ë¥¼ ìœ„í•´ L4 GPU ì‚¬ìš©.\n",
    "# íŒŒì¼ ìƒì„± ë¶€ë¶„ RAM ë¶€ì¡± ë¬¸ì œë¡œ v2-8 TPU ì‚¬ìš©. L4 GPU ì „í™˜ì§€ì  ë”°ë¡œ í‘œì‹œ ì˜ˆì •.\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/base_file/base_clean_train.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/base_file/base_clean_test.csv')\n",
    "\n",
    "train_A = train[train['Segment'] == 'A']\n",
    "\n",
    "cols_to_check = [col for col in train.columns if col not in ['ID', 'Segment']]\n",
    "\n",
    "def is_fixed_column(df, col):\n",
    "    return df[col].nunique() == 1\n",
    "\n",
    "fixed_columns_A = {col: train_A[col].iloc[0] for col in cols_to_check if is_fixed_column(train_A, col)}\n",
    "max_column_values = fixed_columns_A.copy()\n",
    "\n",
    "fixed_cols = list(max_column_values.keys())\n",
    "\n",
    "print(f\"ğŸ“¦ ê³ ì •ëœ ì¹¼ëŸ¼ {len(fixed_cols)}ê°œ ì œê±°í•  ì˜ˆì •ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ids_train = train.copy()\n",
    "for col, value in max_column_values.items():\n",
    "    matching_ids_train = matching_ids_train[matching_ids_train[col] == value]\n",
    "\n",
    "matching_ids_train_grouped = matching_ids_train.groupby('ID').filter(lambda x: len(x) == 6)\n",
    "matching_ids_train_list = matching_ids_train_grouped['ID'].unique()\n",
    "\n",
    "matching_ids_test = test.copy()\n",
    "for col, value in max_column_values.items():\n",
    "    matching_ids_test = matching_ids_test[matching_ids_test[col] == value]\n",
    "\n",
    "matching_ids_test_grouped = matching_ids_test.groupby('ID').filter(lambda x: len(x) == 6)\n",
    "matching_ids_test_list = matching_ids_test_grouped['ID'].unique()\n",
    "\n",
    "train_filtered = train[train['ID'].isin(matching_ids_train_list)].drop(columns=fixed_cols)\n",
    "test_filtered = test[test['ID'].isin(matching_ids_test_list)].drop(columns=fixed_cols)\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… train ë°ì´í„° shape: {train_filtered.shape}\")\n",
    "print(f\"ğŸš€ ìµœì¢… test ë°ì´í„° shape: {test_filtered.shape}\")\n",
    "\n",
    "train_filtered.to_csv('/content/drive/MyDrive/base_file/train_vips_A.csv', index=False)\n",
    "test_filtered.to_csv('/content/drive/MyDrive/base_file/test_vips_A.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/base_file/base_clean_train.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/base_file/base_clean_test.csv')\n",
    "\n",
    "train_B = train[train['Segment'] == 'B']\n",
    "\n",
    "cols_to_check = [col for col in train.columns if col not in ['ID', 'Segment']]\n",
    "\n",
    "def is_fixed_column(df, col):\n",
    "    return df[col].nunique() == 1\n",
    "\n",
    "fixed_columns_B = {col: train_B[col].iloc[0] for col in cols_to_check if is_fixed_column(train_B, col)}\n",
    "max_column_values = fixed_columns_B.copy()\n",
    "\n",
    "fixed_cols = list(max_column_values.keys())\n",
    "\n",
    "print(f\"ğŸ“¦ ê³ ì •ëœ ì¹¼ëŸ¼ {len(fixed_cols)}ê°œ ì œê±°í•  ì˜ˆì •ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e213cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ids_train = train.copy()\n",
    "for col, value in max_column_values.items():\n",
    "    matching_ids_train = matching_ids_train[matching_ids_train[col] == value]\n",
    "\n",
    "matching_ids_train_grouped = matching_ids_train.groupby('ID').filter(lambda x: len(x) == 6)\n",
    "matching_ids_train_list = matching_ids_train_grouped['ID'].unique()\n",
    "\n",
    "matching_ids_test = test.copy()\n",
    "for col, value in max_column_values.items():\n",
    "    matching_ids_test = matching_ids_test[matching_ids_test[col] == value]\n",
    "\n",
    "matching_ids_test_grouped = matching_ids_test.groupby('ID').filter(lambda x: len(x) == 6)\n",
    "matching_ids_test_list = matching_ids_test_grouped['ID'].unique()\n",
    "\n",
    "train_filtered = train[train['ID'].isin(matching_ids_train_list)].drop(columns=fixed_cols)\n",
    "test_filtered = test[test['ID'].isin(matching_ids_test_list)].drop(columns=fixed_cols)\n",
    "\n",
    "print(f\"ğŸš€ ìµœì¢… train ë°ì´í„° shape: {train_filtered.shape}\")\n",
    "print(f\"ğŸš€ ìµœì¢… test ë°ì´í„° shape: {test_filtered.shape}\")\n",
    "\n",
    "train_filtered.to_csv('/content/drive/MyDrive/base_file/train_vips_B.csv', index=False)\n",
    "test_filtered.to_csv('/content/drive/MyDrive/base_file/test_vips_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034590ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•´ë‹¹ ì½”ë“œë¶€í„° v2-8 TPU ëŒ€ì‹  L4 GPU ì‚¬ìš©.\n",
    "!pip install catboost==1.2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/base_file/train_vips_A.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/base_file/test_vips_A.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['Segment'] = label_encoder.fit_transform(train['Segment'])\n",
    "\n",
    "X = train.drop(columns=['Segment', 'ID'])\n",
    "y = train['Segment']\n",
    "X_test = test.drop(columns=['ID'])\n",
    "\n",
    "cat_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "params = {\n",
    "    'iterations': 2000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'verbose': 100,\n",
    "    'random_seed': 42,\n",
    "    'task_type': 'GPU',\n",
    "    'class_weights': [20, 50, 2, 1, 1],\n",
    "}\n",
    "\n",
    "n_classes = 5\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¨ì¼ Model Run ì‹œì‘\")\n",
    "\n",
    "all_test_probs = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"ğŸ“‚ Fold {fold + 1}\")\n",
    "\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_valid_fold, y_valid_fold),\n",
    "        cat_features=cat_features,\n",
    "        early_stopping_rounds=100,\n",
    "        use_best_model=True\n",
    "    )\n",
    "\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "    all_test_probs += test_probs\n",
    "\n",
    "avg_test_probs = all_test_probs / kf.get_n_splits()\n",
    "prob_df = pd.DataFrame(avg_test_probs, columns=[0, 1, 2, 3, 4])\n",
    "prob_df['ID'] = test['ID'].values\n",
    "\n",
    "mean_probs = prob_df.groupby('ID').mean().reset_index()\n",
    "mean_probs['Segment'] = mean_probs[[0, 1, 2, 3, 4]].idxmax(axis=1)\n",
    "\n",
    "segment_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}\n",
    "mean_probs['Segment'] = mean_probs['Segment'].map(segment_mapping)\n",
    "\n",
    "a_ids = mean_probs.loc[mean_probs['Segment'] == 'A', 'ID'].tolist()\n",
    "\n",
    "print(f\"\\nâœ… Aë¡œ ë¶„ë¥˜ëœ ID ìˆ˜ = {len(a_ids)}ê°œ\")\n",
    "print(f\"ğŸ” A ID: {a_ids[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e089ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/base_file/train_vips_B.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/base_file/test_vips_B.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['Segment'] = label_encoder.fit_transform(train['Segment'])\n",
    "\n",
    "X = train.drop(columns=['Segment', 'ID'])\n",
    "y = train['Segment']\n",
    "X_test = test.drop(columns=['ID'])\n",
    "\n",
    "cat_features = [col for col in X.columns if X[col].dtype == 'object']\n",
    "for col in cat_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'verbose': 100,\n",
    "    'random_seed': 42,\n",
    "    'task_type': 'GPU',\n",
    "    'class_weights': [10, 10, 1, 1, 1],\n",
    "}\n",
    "\n",
    "n_classes = 5\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¨ì¼ Model Run ì‹œì‘\")\n",
    "\n",
    "all_test_probs = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"ğŸ“‚ Fold {fold + 1}\")\n",
    "\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_valid_fold, y_valid_fold),\n",
    "        cat_features=cat_features,\n",
    "        early_stopping_rounds=100,\n",
    "        use_best_model=True\n",
    "    )\n",
    "\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "    all_test_probs += test_probs\n",
    "\n",
    "avg_test_probs = all_test_probs / kf.get_n_splits()\n",
    "prob_df = pd.DataFrame(avg_test_probs, columns=[0, 1, 2, 3, 4])\n",
    "prob_df['ID'] = test['ID'].values\n",
    "\n",
    "mean_probs = prob_df.groupby('ID').mean().reset_index()\n",
    "mean_probs['Segment'] = mean_probs[[0, 1, 2, 3, 4]].idxmax(axis=1)\n",
    "\n",
    "segment_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}\n",
    "mean_probs['Segment'] = mean_probs['Segment'].map(segment_mapping)\n",
    "\n",
    "b_ids = mean_probs.loc[mean_probs['Segment'] == 'B', 'ID'].tolist()\n",
    "\n",
    "print(f\"\\nâœ… Bë¡œ ë¶„ë¥˜ëœ ID ìˆ˜ = {len(b_ids)}ê°œ\")\n",
    "print(f\"ğŸ” B ID: {b_ids[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b67c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv('/content/drive/MyDrive/base_file/base_catboost_kfold.csv')\n",
    "\n",
    "base_df.loc[base_df['ID'].isin(a_ids), 'Segment'] = 'A'\n",
    "base_df.loc[base_df['ID'].isin(b_ids), 'Segment'] = 'B'\n",
    "\n",
    "base_df.to_csv('/content/drive/MyDrive/base_file/final_catboost.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Segmentê°€ 'A'ë¡œ ìˆ˜ì •ëœ {len(a_ids)}ê°œ ID ë°˜ì˜ ì™„ë£Œ\")\n",
    "print(f\"âœ… Segmentê°€ 'B'ë¡œ ìˆ˜ì •ëœ {len(b_ids)}ê°œ ID ë°˜ì˜ ì™„ë£Œ\")\n",
    "print(\"ğŸ¯ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: final_catboost.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
